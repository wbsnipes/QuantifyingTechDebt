@inproceedings{cunningham1992wycash,
    author = {Cunningham, Ward},
    booktitle = {ACM SIGPLAN OOPS Messenger},
    citeulike-article-id = {13242715},
    keywords = {td},
    number = {2},
    organization = {ACM},
    pages = {29--30},
    posted-at = {2014-06-25 21:20:06},
    priority = {2},
    title = {{The WyCash portfolio management system}},
    volume = {4},
    year = {1992}
}

@unpublished{Gamifying,
    citeulike-article-id = {13242666},
    citeulike-linkout-0 = {http://www.infosys.com/infosys-labs/publications/Documents/gamification/gamifying-software-development-process.pdf},
    keywords = {gamification},
    posted-at = {2014-06-25 19:33:33},
    priority = {2},
    title = {{Gamifying the Software Development Process}},
    url = {http://www.infosys.com/infosys-labs/publications/Documents/gamification/gamifying-software-development-process.pdf}
}

@inproceedings{Nugroho2011Empirical,
    abstract = {{Cunningham introduced the metaphor of technical debt as guidance for software developers that must trade engineering quality against short-term goals. We revisit the technical debt metaphor, and translate it into terms that can help IT executives better understand their IT investments. An approach is proposed to quantify debts (cost to fix technical quality issues) and interest (extra cost spent on maintenance due to technical quality issues). Our approach is based on an empirical assessment method of software quality developed at the Software Improvement Group (SIG). The core part of the technical debt calculation is constructed on the basis of empirical data of 44 systems that are currently being monitored by SIG. In a case study, we apply the approach to a real system, and discuss how the results provide useful insights on important questions related to IT investment such as the return on investment (ROI) in software quality improvement.}},
    address = {New York, NY, USA},
    author = {Nugroho, Ariadi and Visser, Joost and Kuipers, Tobias},
    booktitle = {Proceedings of the 2Nd Workshop on Managing Technical Debt},
    citeulike-article-id = {13242346},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1985364},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1985362.1985364},
    doi = {10.1145/1985362.1985364},
    isbn = {978-1-4503-0586-0},
    keywords = {td},
    location = {Waikiki, Honolulu, HI, USA},
    pages = {1--8},
    posted-at = {2014-06-25 14:00:57},
    priority = {5},
    publisher = {ACM},
    series = {MTD '11},
    title = {{An Empirical Model of Technical Debt and Interest}},
    url = {http://dx.doi.org/10.1145/1985362.1985364},
    year = {2011}
}

@inproceedings{seaman2012using,
    author = {Seaman, Carolyn and Guo, Yuepu and Izurieta, Clemente and Cai, Yuanfang and Zazworka, Nico and Shull, Forrest and Vetr{\`{o}}, Antonio},
    booktitle = {2012 Third International Workshop on Managing Technical Debt (MTD)},
    citeulike-article-id = {13242332},
    keywords = {td},
    pages = {45--48},
    posted-at = {2014-06-25 13:46:06},
    priority = {2},
    title = {{Using technical debt data in decision making: Potential decision approaches}},
    year = {2012}
}

@inproceedings{Guo2011Portfolio,
    abstract = {{Technical debt describes the effect of immature software artifacts on software maintenance - the potential of extra effort required in future as if paying interest for the incurred debt. The uncertainty of interest payment further complicates the problem of what debt should be incurred or repaid and when. To help software managers make informed decisions, a portfolio approach is proposed in this paper. The approach leverages the portfolio management theory in the finance domain to determine the optimal collection of technical debt items that should be incurred or held. We expect this approach could provide a new perspective for technical debt management.}},
    address = {New York, NY, USA},
    author = {Guo, Yuepu and Seaman, Carolyn},
    booktitle = {Proceedings of the 2Nd Workshop on Managing Technical Debt},
    citeulike-article-id = {13242320},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1985370},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1985362.1985370},
    doi = {10.1145/1985362.1985370},
    isbn = {978-1-4503-0586-0},
    keywords = {td},
    location = {Waikiki, Honolulu, HI, USA},
    pages = {31--34},
    posted-at = {2014-06-25 13:40:12},
    priority = {2},
    publisher = {ACM},
    series = {MTD '11},
    title = {{A Portfolio Approach to Technical Debt Management}},
    url = {http://dx.doi.org/10.1145/1985362.1985370},
    year = {2011}
}

@article{Smells,
    citeulike-article-id = {13241619},
    citeulike-linkout-0 = {http://www.industriallogic.com/wp-content/uploads/2005/09/smellstorefactorings.pdf},
    keywords = {refactoring, td},
    posted-at = {2014-06-24 21:53:00},
    priority = {0},
    title = {{Smells to Refactorings
 Quick Reference Guide}},
    url = {http://www.industriallogic.com/wp-content/uploads/2005/09/smellstorefactorings.pdf}
}

@book{Fowler1999Refactoring,
    abstract = {{Your class library works, but could it be better? \_Refactoring: Improving the
Design of Existing Code\_ shows how refactoring can make object-oriented code
simpler and easier to maintain. Today, refactoring requires considerable
design know-how, but once tools become available, all programmers should be
able to improve their code using refactoring techniques.

Besides an introduction to what refactoring is, this handbook provides a
catalogue of dozens of tips for improving code. The best thing about
\_Refactoring\_ is its remarkably clear presentation, along with excellent nuts-
and-bolts advice, from object expert Martin Fowler. The author is also an
authority on software patterns and UML, and this experience helps make this a
better book, one that should be immediately accessible to any intermediate or
advanced object-oriented developer. (Just like patterns, each refactoring tip
is presented with a simple name, a "motivation," and examples using Java and
UML.)

Early chapters stress the importance of testing in successful refactoring.
(When you improve code, you have to test to verify that it still works.) After
the discussion on how to detect the "smells" of bad code, readers get to the
heart of the book, its catalogue of more than 70 "refactorings"--tips for
better and simpler class design. Each tip is illustrated with "before" and
"after" code, along with an explanation. Later chapters provide a quick look
at refactoring research.

Like software patterns, refactoring may be an idea whose time has come. This
groundbreaking title will surely help bring refactoring to the programming
mainstream. With its clear advice on a hot new topic, \_Refactoring\_ is sure to
be essential reading for anyone who writes or maintains object- oriented
software. --\_Richard Dragan\_

**Topics Covered:** Refactoring, improving software code, redesign, design
tips, patterns, unit testing, refactoring research and tools.}},
    author = {Fowler, Martin and Beck, Kent and Brant, John and Opdyke, William and Roberts, Don},
    citeulike-article-id = {251681},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0201485672},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0201485672},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0201485672},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0201485672},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0201485672/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0201485672},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0201485672},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0201485672},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0201485672\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0201485672},
    day = {08},
    edition = {1},
    howpublished = {Hardcover},
    isbn = {0201485672},
    keywords = {refactoring, td},
    month = jul,
    posted-at = {2014-06-24 21:43:41},
    priority = {4},
    publisher = {Addison-Wesley Professional},
    title = {{Refactoring: Improving the Design of Existing Code}},
    url = {http://www.worldcat.org/isbn/0201485672},
    year = {1999}
}

@inproceedings{Curtis2012Estimating,
    author = {Curtis, B. and Sappidi, J. and Szynkarski, A.},
    booktitle = {Managing Technical Debt (MTD), 2012 Third International Workshop on},
    citeulike-article-id = {13241430},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/mtd.2012.6226000},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6226000},
    doi = {10.1109/mtd.2012.6226000},
    institution = {CAST, Fort Worth, TX, USA},
    isbn = {978-1-4673-1748-1},
    keywords = {td},
    month = jun,
    pages = {49--53},
    posted-at = {2014-06-24 14:47:52},
    priority = {4},
    publisher = {IEEE},
    title = {{Estimating the size, cost, and types of Technical Debt}},
    url = {http://dx.doi.org/10.1109/mtd.2012.6226000},
    year = {2012}
}

@unpublished{RozenfeldGaming,
    author = {Rozenfeld, Monica},
    citeulike-article-id = {13170741},
    citeulike-linkout-0 = {http://theinstitute.ieee.org/technology-focus/technology-topic/gaming-in-the-workplace},
    journal = {the Institute},
    keywords = {gamification},
    posted-at = {2014-05-14 19:17:43},
    priority = {0},
    title = {{Gaming in the Workplace - IEEE - The Institute}},
    url = {http://theinstitute.ieee.org/technology-focus/technology-topic/gaming-in-the-workplace}
}

@inproceedings{DeLine2012Debugger,
    abstract = {{At ICSE 2010, the Code Bubbles team from Brown University and the Code Canvas team from Microsoft Research presented similar ideas for new user experiences for an integrated development environment. Since then, the two teams formed a collaboration, along with the Microsoft Visual Studio team, to release Debugger Canvas, an industrial version of the Code Bubbles paradigm. With Debugger Canvas, a programmer debugs her code as a collection of code bubbles, annotated with call paths and variable values, on a two-dimensional pan-and-zoom surface. In this experience report, we describe new user interface ideas, describe the rationale behind our design choices, evaluate the performance overhead of the new design, and provide user feedback based on lab participants, post-release usage data, and a user survey and interviews. We conclude that the code bubbles paradigm does scale to existing customer code bases, is best implemented as a mode in the existing user experience rather than a replacement, and is most useful when the user has a long or complex call paths, a large or unfamiliar code base, or complex control patterns, like factories or dynamic linking.}},
    address = {Piscataway, NJ, USA},
    author = {DeLine, Robert and Bragdon, Andrew and Rowan, Kael and Jacobsen, Jens and Reiss, Steven P.},
    booktitle = {Proceedings of the 34th International Conference on Software Engineering},
    citeulike-article-id = {13157599},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=2337362},
    isbn = {978-1-4673-1067-3},
    keywords = {navigation},
    location = {Zurich, Switzerland},
    pages = {1064--1073},
    posted-at = {2014-05-02 21:04:37},
    priority = {3},
    publisher = {IEEE Press},
    series = {ICSE '12},
    title = {{Debugger Canvas: Industrial Experience with the Code Bubbles Paradigm}},
    url = {http://portal.acm.org/citation.cfm?id=2337362},
    year = {2012}
}

@article{Dzidek2008Realistic,
    abstract = {{The Unified Modeling Language (UML) is the de facto standard for object-oriented software analysis and design modeling. However, few empirical studies exist that investigate the costs and evaluate the benefits of using UML in realistic contexts. Such studies are needed so that the software industry can make informed decisions regarding the extent to which they should adopt UML in their development practices. This is the first controlled experiment that investigates the costs of maintaining and the benefits of using UML documentation during the maintenance and evolution of a real, non-trivial system, using professional developers as subjects, working with a state-of-the-art UML tool during an extended period of time. The subjects in the control group had no UML documentation. In this experiment, the subjects in the UML group had on average a practically and statistically significant 54\% increase in the functional correctness of changes (p=0.03), and an insignificant 7\% overall improvement in design quality (p=0.22) - though a much larger improvement was observed on the first change task (56\%) - at the expense of an insignificant 14\% increase in development time caused by the overhead of updating the UML documentation (p=0.35).}},
    address = {Los Alamitos, CA, USA},
    author = {Dzidek, W. J. and Arisholm, E. and Briand, L. C.},
    booktitle = {Software Engineering, IEEE Transactions on},
    citeulike-article-id = {2909429},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1383295},
    citeulike-linkout-1 = {http://doi.ieeecomputersociety.org/10.1109/TSE.2008.15},
    citeulike-linkout-2 = {http://dx.doi.org/10.1109/tse.2008.15},
    citeulike-linkout-3 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4459340},
    doi = {10.1109/tse.2008.15},
    institution = {Dept. of Software Eng., Simula Res. Lab., Lysaker},
    issn = {0098-5589},
    journal = {Software Engineering, IEEE Transactions on},
    keywords = {comprehension, maintainability, uml},
    month = may,
    number = {3},
    pages = {407--432},
    posted-at = {2014-04-14 20:14:45},
    priority = {2},
    publisher = {IEEE},
    title = {{A Realistic Empirical Evaluation of the Costs and Benefits of UML in Software Maintenance}},
    url = {http://dx.doi.org/10.1109/tse.2008.15},
    volume = {34},
    year = {2008}
}

@article{Arisholm2006Impact,
    abstract = {{The Unified Modeling Language (UML) is becoming the de facto standard for software analysis and design modeling. However, there is still significant resistance to model-driven development in many software organizations because it is perceived to be expensive and not necessarily cost-effective. Hence, it is important to investigate the benefits obtained from modeling. As a first step in this direction, this paper reports on controlled experiments, spanning two locations, that investigate the impact of UML documentation on software maintenance. Results show that, for complex tasks and past a certain learning curve, the availability of UML documentation may result in significant improvements in the functional correctness of changes as well as the quality of their design. However, there does not seem to be any saving of time. For simpler tasks, the time needed to update the UML documentation may be substantial compared with the potential benefits, thus motivating the need for UML tools with better support for software maintenance.}},
    author = {Arisholm, E. and Briand, L. C. and Hove, S. E. and Labiche, Y.},
    citeulike-article-id = {757968},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/tse.2006.59},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1650213},
    doi = {10.1109/tse.2006.59},
    institution = {Dept. of Software Eng., Simula Res. Lab.},
    issn = {0098-5589},
    journal = {Software Engineering, IEEE Transactions on},
    keywords = {comprehension, maintainability},
    month = jun,
    number = {6},
    pages = {365--381},
    posted-at = {2014-04-14 19:54:02},
    priority = {4},
    publisher = {IEEE},
    title = {{The impact of UML documentation on software maintenance: an experimental evaluation}},
    url = {http://dx.doi.org/10.1109/tse.2006.59},
    volume = {32},
    year = {2006}
}

@article{Littman1987Mental,
    address = {New York, NY, USA},
    author = {Littman, David C. and Pinto, Jeannine and Letovsky, Stanley and Soloway, Elliot},
    citeulike-article-id = {4890225},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=39170},
    citeulike-linkout-1 = {http://dx.doi.org/10.1016/0164-1212(87)90033-1},
    doi = {10.1016/0164-1212(87)90033-1},
    issn = {01641212},
    journal = {Journal of Systems and Software},
    keywords = {comprehension, maintainability, productivity},
    month = dec,
    number = {4},
    pages = {341--355},
    posted-at = {2014-04-14 19:39:30},
    priority = {2},
    publisher = {Elsevier Science Inc.},
    title = {{Mental models and software maintenance}},
    url = {http://dx.doi.org/10.1016/0164-1212(87)90033-1},
    volume = {7},
    year = {1987}
}

@inproceedings{LaToza2006Maintaining,
    abstract = {{To understand developers' typical tools, activities, and practices and their satisfaction with each, we conducted two surveys and eleven interviews. We found that many problems arose because developers were forced to invest great effort recovering implicit knowledge by exploring code and interrupting teammates and this knowledge was only saved in their memory. Contrary to expectations that email and IM prevent expensive task switches caused by face-to-face interruptions, we found that face-to-face communication enjoys many advantages. Contrary to expectations that documentation makes understanding design rationale easy, we found that current design documents are inadequate. Contrary to expectations that code duplication involves the copy and paste of code snippets, developers reported several types of duplication. We use data to characterize these and other problems and draw implications for the design of tools for their solution.}},
    address = {New York, NY, USA},
    author = {LaToza, Thomas D. and Venolia, Gina and DeLine, Robert},
    booktitle = {Proceedings of the 28th International Conference on Software Engineering},
    citeulike-article-id = {768523},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1134285.1134355},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1134285.1134355},
    doi = {10.1145/1134285.1134355},
    isbn = {1-59593-375-1},
    keywords = {devnetworks, knowledgemgmt, productivity},
    location = {Shanghai, China},
    pages = {492--501},
    posted-at = {2014-04-14 19:21:41},
    priority = {2},
    publisher = {ACM},
    series = {ICSE '06},
    title = {{Maintaining Mental Models: A Study of Developer Work Habits}},
    url = {http://dx.doi.org/10.1145/1134285.1134355},
    year = {2006}
}

@article{Transitive,
    citeulike-article-id = {12889107},
    citeulike-linkout-0 = {http://www.cs.nmsu.edu/\~{}ipivkina/TransClosure/},
    keywords = {td},
    posted-at = {2014-01-02 14:15:33},
    priority = {2},
    title = {{Transitive closure}},
    url = {http://www.cs.nmsu.edu/\~{}ipivkina/TransClosure/}
}

@inproceedings{Sangal2005Using,
    abstract = {{An approach to managing the architecture of large software systems is presented. Dependencies are extracted from the code by a conventional static analysis, and shown in a tabular form known as the 'Dependency Structure Matrix' (DSM). A variety of algorithms are available to help organize the matrix in a form that reflects the architecture and highlights patterns and problematic dependencies. A hierarchical structure obtained in part by such algorithms, and in part by input from the user, then becomes the basis for 'design rules' that capture the architect's intent about which dependencies are acceptable. The design rules are applied repeatedly as the system evolves, to identify violations, and keep the code and its architecture in conformance with one another. The analysis has been implemented in a tool called LDM which has been applied in several commercial projects; in this paper, a case study application to Haystack, an information retrieval system, is described.}},
    address = {New York, NY, USA},
    author = {Sangal, Neeraj and Jordan, Ev and Sinha, Vineet and Jackson, Daniel},
    booktitle = {OOPSLA '05: Proceedings of the 20th annual ACM SIGPLAN conference on Object-oriented programming, systems, languages, and applications},
    citeulike-article-id = {703138},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1094824},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1094811.1094824},
    doi = {10.1145/1094811.1094824},
    isbn = {1-59593-031-0},
    keywords = {maintainability, productivity, td},
    location = {San Diego, CA, USA},
    month = oct,
    number = {10},
    pages = {167--176},
    posted-at = {2013-12-31 19:21:21},
    priority = {2},
    publisher = {ACM},
    title = {{Using dependency models to manage complex software architecture}},
    url = {http://dx.doi.org/10.1145/1094811.1094824},
    volume = {40},
    year = {2005}
}

@article{MacCormack2012Exploring,
    abstract = {{A variety of academic studies argue that a relationship exists between the structure of an organization and the design of the products that this organization produces. Specifically, products tend to  ” mirror” the architectures of the organizations in which they are developed. This dynamic occurs because the organization's governance structures, problem solving routines and communication patterns constrain the space in which it searches for new solutions. Such a relationship is important, given that product architecture has been shown to be an important predictor of product performance, product variety, process flexibility and even the path of industry evolution. We explore this relationship in the software industry. Our research takes advantage of a natural experiment, in that we observe products that fulfill the same function being developed by very different organizational forms. At one extreme are commercial software firms, in which the organizational participants are tightly-coupled, with respect to their goals, structure and behavior. At the other, are open source software communities, in which the participants are much more loosely-coupled by comparison. The mirroring hypothesis predicts that these different organizational forms will produce products with distinctly different architectures. Specifically, loosely-coupled organizations will develop more modular designs than tightly-coupled organizations. We test this hypothesis, using a sample of matched-pair products. We find strong evidence to support the mirroring hypothesis. In all of the pairs we examine, the product developed by the loosely-coupled organization is significantly more modular than the product from the tightly-coupled organization. We measure modularity by capturing the level of coupling between a product's components. The magnitude of the differences is substantial—up to a factor of six, in terms of the potential for a design change in one component to propagate to others. Our results have significant managerial implications, in highlighting the impact of organizational design decisions on the technical structure of the artifacts that these organizations subsequently develop. \^{a}º We explore the relationship between product designs and organizational designs. \^{a}º We compare open source software with software developed by commercial firms. \^{a}º We measure modularity by capturing the level of coupling between components. \^{a}º We find that loosely coupled organizations tend to develop more modular products. \^{a}º The differences in modularity are substantial—up to a factor of six in our sample.}},
    author = {MacCormack, Alan and Baldwin, Carliss and Rusnak, John},
    citeulike-article-id = {10772706},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.respol.2012.04.011},
    comment = {(private-note)Defines Propagation Cost as the mean visibility of files in a system where visibility is related to the density of the matrix of function calls that occur between files aggregate over all levels of calls.},
    doi = {10.1016/j.respol.2012.04.011},
    issn = {00487333},
    journal = {Research Policy},
    keywords = {maintainability, productivity, td},
    month = oct,
    number = {8},
    pages = {1309--1324},
    posted-at = {2013-12-31 19:17:59},
    priority = {0},
    title = {{Exploring the duality between product and organizational architectures: A test of the  ” mirroring” hypothesis}},
    url = {http://dx.doi.org/10.1016/j.respol.2012.04.011},
    volume = {41},
    year = {2012}
}

@inproceedings{Brown2011Analysis,
    author = {Brown, N. and Nord, R. L. and Ozkaya, I. and Pais, M.},
    booktitle = {Software Architecture (WICSA), 2011 9th Working IEEE/IFIP Conference on},
    citeulike-article-id = {12884146},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/wicsa.2011.22},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5959724},
    doi = {10.1109/wicsa.2011.22},
    institution = {Software Eng. Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA},
    isbn = {978-1-61284-399-5},
    keywords = {td},
    month = jun,
    pages = {103--112},
    posted-at = {2013-12-30 18:49:16},
    priority = {4},
    publisher = {IEEE},
    title = {{Analysis and Management of Architectural Dependencies in Iterative Release Planning}},
    url = {http://dx.doi.org/10.1109/wicsa.2011.22},
    year = {2011}
}

@inproceedings{Ko2008Debugging,
    abstract = {{When software developers want to understand the reason for a program's behavior, they must translate their questions about the behavior into a series of questions about code, speculating about the causes in the process. The Whyline is a new kind of debugging tool that avoids such speculation by instead enabling developers to select a question about program output from a set of why did and why didn't questions derived from the program's code and execution. The tool then finds one or more possible explanations for the output in question, using a combination of static and dynamic slicing, precise call graphs, and new algorithms for determining potential sources of values and explanations for why a line of code was not reached. Evaluations of the tool on one task showed that novice programmers with the Whyline were twice as fast as expert programmers without it. The tool has the potential to simplify debugging in many software development contexts.}},
    address = {New York, NY, USA},
    author = {Ko, Andrew J. and Myers, Brad A.},
    booktitle = {Proceedings of the 30th International Conference on Software Engineering},
    citeulike-article-id = {2818401},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1368088.1368130},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1368088.1368130},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4814141},
    doi = {10.1145/1368088.1368130},
    institution = {Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA},
    isbn = {978-1-60558-079-1},
    issn = {0270-5257},
    keywords = {debugging},
    location = {Leipzig, Germany},
    month = may,
    pages = {301--310},
    posted-at = {2013-10-22 14:27:54},
    priority = {4},
    publisher = {ACM},
    series = {ICSE '08},
    title = {{Debugging Reinvented: Asking and Answering Why and Why Not Questions About Program Behavior}},
    url = {http://dx.doi.org/10.1145/1368088.1368130},
    year = {2008}
}

@inproceedings{Snipes2014Experiences,
    author = {Snipes, W. B. and Nair, Anil R. and Murphy-Hill, E.},
    booktitle = {International Conference on Software Engineering (in submission)},
    citeulike-article-id = {12723507},
    keywords = {gamification, motivation},
    posted-at = {2013-10-16 14:45:53},
    priority = {2},
    publisher = {IEEE Press.},
    title = {{Experiences Gamifying Developer Practices}},
    year = {2014}
}

@book{2013Oxford,
    citeulike-article-id = {12717157},
    citeulike-linkout-0 = {http://oxforddictionaries.com/us/definition/american\_english/gamification},
    keywords = {gamification},
    posted-at = {2013-10-11 19:52:31},
    priority = {2},
    publisher = {Oxford University Press},
    title = {{Oxford Dictionaries Online - American English (US)}},
    url = {http://oxforddictionaries.com/us/definition/american\_english/gamification},
    year = {2013}
}

@inproceedings{Shepherd2012Sando,
    abstract = {{Developers heavily rely on Local Code Search (LCS)---the execution of a text-based search on a single code base---to find starting points in software maintenance tasks. While LCS approaches commonly used by developers are based on lexical matching and often result in failed searches or irrelevant results, developers have not yet migrated to the various research approaches that have made significant advancements in LCS. We hypothesize that two of the major reasons for this lack of migration are as follows. First, developers do not know which approach is the best, due to a lack of comparative field studies and the discrepancies in the underlying LCS process that these research approaches address. Second, developers lack access to a stable implementation of most of the research approaches. To address these issues, we studied a number of LCS approaches, distilled the general component structure underlying these approaches and, based on this structure, developed a LCS tool and framework, called Sando. Currently used by developers at ABB, Inc. and elsewhere, Sando also supports the flexible extension of its components to rapidly disseminate research advancements, and allows for user-based evaluation of competing approaches.}},
    address = {New York, NY, USA},
    author = {Shepherd, David and Damevski, Kostadin and Ropski, Bartosz and Fritz, Thomas},
    booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
    citeulike-article-id = {12688769},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=2393612},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/2393596.2393612},
    doi = {10.1145/2393596.2393612},
    isbn = {978-1-4503-1614-9},
    keywords = {productivity, thesis-ms},
    location = {Cary, North Carolina},
    posted-at = {2013-10-03 21:32:51},
    priority = {4},
    publisher = {ACM},
    series = {FSE},
    title = {{Sando: an extensible local code search framework}},
    url = {http://dx.doi.org/10.1145/2393596.2393612},
    year = {2012}
}

@article{Hall2009WEKA,
    abstract = {{More than twelve years have elapsed since the first public release of WEKA. In that time, the software has been rewritten entirely from scratch, evolved substantially and now accompanies a text on data mining [35]. These days, WEKA enjoys widespread acceptance in both academia and business, has an active community, and has been downloaded more than 1.4 million times since being placed on Source-Forge in April 2000. This paper provides an introduction to the WEKA workbench, reviews the history of the project, and, in light of the recent 3.6 stable release, briefly discusses what has been added since the last stable version (Weka 3.4) released in 2003. 1.}},
    author = {Hall, Mark and Frank, Eibe and Holmes, Geoffrey and Pfahringer, Bernhard and Reutemann, Peter and Witten, Ian H.},
    citeulike-article-id = {8219039},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.163.817},
    journal = {SIGKDD Explorations},
    keywords = {thesis-ms},
    number = {1},
    posted-at = {2013-10-03 18:33:40},
    priority = {0},
    title = {{The WEKA data mining software: an update}},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.163.817},
    volume = {11},
    year = {2009}
}

@book{Witten2005Data,
    abstract = {{As with any burgeoning technology that enjoys commercial attention, the use of data mining is surrounded by a great deal of hype. Exaggerated reports tell of secrets that can be uncovered by setting algorithms loose on oceans of data. But there is no magic in machine learning, no hidden power, no alchemy. Instead there is an identifiable body of practical techniques that can extract useful information from raw data. This book describes these techniques and shows how they work. <br><br>The book is a major revision of the first edition that appeared in 1999. While the basic core remains the same, it has been updated to reflect the changes that have taken place over five years, and now has nearly double the references. The highlights for the new edition include thirty new technique sections; an enhanced Weka machine learning workbench, which now features an interactive interface; comprehensive information on neural networks; a new section on Bayesian networks; plus much more.<br><br>+ Authors, Ian Witten and Eibe Frank, recipients of the 2005 ACM SIGKDD Service Award.<br>+ Algorithmic methods at the heart of successful data miningincluding tried and true techniques as well as leading edge methods; <br>+ Performance improvement techniques that work by transforming the input or output; <br>+ Downloadable Weka, a collection of machine learning algorithms for data mining tasks, including tools for data pre-processing, classification, regression, clustering, association rules, and visualizationin a new, interactive interface.}},
    author = {Witten, Ian H. and Frank, Eibe},
    citeulike-article-id = {340715},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0120884070},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0120884070},
    citeulike-linkout-10 = {http://www.worldcat.org/oclc/58451668},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0120884070},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0120884070},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0120884070/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0120884070},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0120884070},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0120884070},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0120884070\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0120884070},
    day = {22},
    edition = {Second},
    howpublished = {Paperback},
    isbn = {0120884070},
    keywords = {thesis-ms},
    month = jun,
    posted-at = {2013-10-03 18:32:52},
    priority = {0},
    publisher = {Morgan Kaufmann},
    series = {Morgan Kaufmann Series in Data Management Systems},
    title = {{Data Mining: Practical Machine Learning Tools and Techniques}},
    url = {http://www.worldcat.org/isbn/0120884070},
    year = {2005}
}

@inproceedings{Vakilian2011Need,
    abstract = {{Even though modern Integrated Development Environments (IDEs) support many refactorings, studies suggest that automated refactorings are used infrequently, and few developers use anything beyond Rename and Extract refactorings. Little is known about why automated refactorings are seldom used. We present a list of challenging questions whose answers are crucial for understanding the usability issues of refactoring tools. This paper argues that the existing data sources - Eclipse UDC, Eclipse refactoring histories, version control histories, etc. - are inadequate for answering these questions. Finally, we introduce our tools to collect richer usage data that will enable us to answer some of the open research questions about the usability of refactoring tools. Findings from our data will foster the design of the next generation of refactoring tools.}},
    address = {New York, NY, USA},
    author = {Vakilian, Mohsen and Chen, Nicholas and Negara, Stas and Rajkumar, Balaji A. and Moghaddam, Roshanak Z. and Johnson, Ralph E.},
    booktitle = {Proceedings of the 3rd ACM SIGPLAN workshop on Evaluation and usability of programming languages and tools},
    citeulike-article-id = {12673029},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=2089164},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/2089155.2089164},
    doi = {10.1145/2089155.2089164},
    isbn = {978-1-4503-1024-6},
    keywords = {hackystat, ide, productivity, refactoring},
    location = {Portland, Oregon, USA},
    pages = {31--38},
    posted-at = {2013-09-30 18:36:21},
    priority = {5},
    publisher = {ACM},
    series = {PLATEAU '11},
    title = {{The need for richer refactoring usage data}},
    url = {http://dx.doi.org/10.1145/2089155.2089164},
    year = {2011}
}


@article{Typical,
    citeulike-article-id = {12610435},
    citeulike-linkout-0 = {http://www.marketingzone.com/how/direct-mail-marketing/typical-results-direct-mail},
    keywords = {gamification},
    posted-at = {2013-09-08 19:37:42},
    priority = {2},
    title = {{Typical Results for Direct Mail | MarketingZone}},
    url = {http://www.marketingzone.com/how/direct-mail-marketing/typical-results-direct-mail}
}

@article{prechelt2008types,
    author = {Prechelt, Lutz and St{\"{a}}rk, Ulrich and Salinger, Stephan},
    citeulike-article-id = {12609438},
    comment = {(private-note)Describes episodes in team oriented programming where two people might be considering a software element or decision.  Interesting because the categories aren't just related to pair programming.},
    journal = {Freie Universit{\"{a}}t Berlin, Institut f {\"{u}}r Informatik, Berlin, Germany, Technical Report B-08-17},
    keywords = {gamification, productivity},
    posted-at = {2013-09-07 15:08:33},
    priority = {0},
    title = {{Types of cooperation episodes in side-by-side programming}},
    year = {2008}
}

@inproceedings{Dubois2013Understanding,
    abstract = {{In this paper we outline the idea to adopt gamification techniques to engage, train, monitor, and motivate all the players involved in the development of complex software artifacts, from the inception to the deployment and maintenance. The paper introduces the concept of gamification and proposes a research approach to understand how its principles may be successfully applied to the process of software development. Applying gamification to software engineering is not as straightforward as it may appear since it has to be casted to the peculiarities of this domain. Existing literature in the area has already recognized the possible use of such technology in the context of software development, however how to design and use gamification in this context is still an open question. This leads to several research challenges which are organized in a fascinating research agenda that is part of the contribution of this paper. Finally, to support the proposed ideas we present a preliminary experiment that shows the effect of gamification on the performance of students involved in a software engineering project.}},
    address = {New York, NY, USA},
    author = {Dubois, Daniel J. and Tamburrelli, Giordano},
    booktitle = {Proceedings of the 9th Joint Meeting on Foundations of Software Engineering},
    citeulike-article-id = {12606816},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=2494589},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/2491411.2494589},
    doi = {10.1145/2491411.2494589},
    isbn = {978-1-4503-2237-9},
    keywords = {gamification},
    location = {Saint Petersburg, Russia},
    pages = {659--662},
    posted-at = {2013-09-04 21:10:50},
    priority = {0},
    publisher = {ACM},
    series = {ESEC/FSE},
    title = {{Understanding gamification mechanisms for software development}},
    url = {http://dx.doi.org/10.1145/2491411.2494589},
    year = {2013}
}

@book{longitudinalbook,
    address = {711 Third Avenue New York, NY 10017},
    author = {Newsom, Jason T. and Jones, Richard N. and Hofer, Scott M.},
    booktitle = {Longitudinal Data Analysis},
    citeulike-article-id = {12606757},
    isbn = {9780415874151},
    keywords = {gameification, thesis-ms},
    posted-at = {2013-09-04 19:53:57},
    priority = {2},
    publisher = {Routledge},
    title = {{Longitudinal Data Analysis}},
    year = {2012}
}

@article{Kou2010Operational,
    abstract = {{Test-driven development (TDD) is a style of development named for its most visible characteristic: the design and implementation of test cases prior to the implementation of the code required to make them pass. Many claims have been made for TDD: that it can improve implementation as well as design quality, that it can improve productivity, that it results in 100\% coverage, and so forth. However, research to validate these claims has yielded mixed and sometimes contradictory results. We believe that at least part of the reason for these results stems from differing interpretations of the TDD development style, along with an inability to determine whether programmers actually follow whatever definition of TDD is in use.
                            Zorro is a system designed to automatically determine whether a developer is complying with an operational definition of Test-Driven Development (TDD) practices. Automated recognition of TDD can benefit the software development community in a variety of ways, from inquiry into the  ” true nature” of TDD, to pedagogical aids to support the practice of test-driven development, to support for more rigorous empirical studies on the effectiveness of TDD in both laboratory and real world settings.
                            This paper describes the Zorro system, its operational definition of TDD, the analyses made possible by Zorro, two empirical evaluations of the system, and an attempted case study. Our research shows that it is possible to define an operational definition of TDD that is amenable to automated recognition, and illustrates the architectural and design issues that must be addressed in order to do so. Zorro has implications not only for the practice of TDD, but also for software engineering  ” micro-process” definition and recognition through its parent framework, Software Development Stream Analysis.}},
    author = {Kou, Hongbing and Johnson, PhilipM and Erdogmus, Hakan},
    booktitle = {Automated Software Engineering},
    citeulike-article-id = {6180831},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/s10515-009-0058-8},
    citeulike-linkout-1 = {http://www.springerlink.com/content/4362667326046148},
    citeulike-linkout-2 = {http://link.springer.com/article/10.1007/s10515-009-0058-8},
    doi = {10.1007/s10515-009-0058-8},
    journal = {Automated Software Engineering},
    keywords = {hackystat, tdd},
    number = {1},
    pages = {57--85},
    posted-at = {2013-09-02 14:22:59},
    priority = {4},
    publisher = {Springer US},
    title = {{Operational definition and automated inference of test-driven development with Zorro}},
    url = {http://dx.doi.org/10.1007/s10515-009-0058-8},
    volume = {17},
    year = {2010}
}

@article{csdl2-06-13,
    abstract = {{Zorro is a system designed to automatically determine whether a developer is complying with an operational definition of Test-Driven Development (TDD) practices. Automated recognition of TDD can benefit the software development community in a variety of ways, from inquiry into the ``true nature'' of TDD, to pedagogical aids to support the practice of test-driven development, to support for more rigorous empirical studies on the effectiveness of TDD in both laboratory and real world settings. This paper introduces the Zorro system, its operational definition of TDD, the analyses made possible by Zorro, and our ongoing efforts to validate the system.}},
    author = {Johnson, Philip M. and Kou, Hongbing},
    citeulike-article-id = {6387982},
    citeulike-linkout-0 = {http://csdl.ics.hawaii.edu/techreports/06-13/06-13.pdf},
    comment = {(private-note)Johnson and Kou defined Zorro, a system for detecting whether developers use Test Driven Development techniques based on data from Hackystat.  The study conducted takes the approach of dividing development activities into episodes delimited by events such as configuration management code check-in, start of a unit test run, or start of a build.  Using the distinct events that developers follow within these episodes, Zorro determines whether the episode followed Test-Driven Development practices per their classic definition of test first - code - test pass, or a different scenario. When compared to a simultaneous observational screen video, Zorro achieved 89\% accuracy in classifying episodes into their proper TDD scenarios.  The study did not however, attempt to influence the student participants with instant feedback aimed at encouraging them to follow the classic definition of Test Driven Development.  },
    journal = {Proceedings of {Agile 2007}},
    keywords = {hackystat, productivity, tdd},
    month = aug,
    posted-at = {2013-09-02 13:26:35},
    priority = {2},
    title = {Automated Recognition of Test-Driven Development with {Z}orro},
    url = {http://csdl.ics.hawaii.edu/techreports/06-13/06-13.pdf},
    year = {2007}
}

@incollection{Moser2006Does,
    abstract = {{The improvement of the software development process through the development and utilization of high quality and reusable software components has been advocated for a long time. Agile Methods promote some interesting practices, in particular the practice of refactoring, which are supposed to improve understandability and maintainability of source code. In this research we analyze if refactoring promotes ad-hoc reuse of object-oriented classes by improving internal quality metrics. We conduct a case study in a close-to industrial, agile environment in order to analyze the impact of refactoring on internal quality metrics of source code. Our findings sustain the hypothesis that refactoring enhances quality and reusability of – otherwise hard to reuse – classes in an agile development environment. Given such promising results, additional experimentation is required to validate and generalize the results of this work.}},
    author = {Moser, Raimund and Sillitti, Alberto and Abrahamsson, Pekka and Succi, Giancarlo},
    booktitle = {Reuse of Off-the-Shelf Components},
    citeulike-article-id = {12602627},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/11763864\_21},
    citeulike-linkout-1 = {http://link.springer.com/chapter/10.1007/11763864\_21},
    doi = {10.1007/11763864\_21},
    editor = {Morisio, Maurizio},
    keywords = {prom, refactoring},
    pages = {287--297},
    posted-at = {2013-08-30 21:47:40},
    priority = {2},
    publisher = {Springer Berlin Heidelberg},
    series = {Lecture Notes in Computer Science},
    title = {{Does Refactoring Improve Reusability?}},
    url = {http://dx.doi.org/10.1007/11763864\_21},
    volume = {4039},
    year = {2006}
}

@inproceedings{Abrahamsson2007Effort,
    author = {Abrahamsson, P. and Moser, R. and Pedrycz, W. and Sillitti, A. and Succi, G.},
    booktitle = {Empirical Software Engineering and Measurement, ESEM},
    citeulike-article-id = {12602626},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/esem.2007.16},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4343762},
    doi = {10.1109/esem.2007.16},
    institution = {VTT Electron., Oulu},
    isbn = {978-0-7695-2886-1},
    issn = {1938-6451},
    keywords = {prediction, productivity, prom},
    pages = {344--353},
    posted-at = {2013-08-30 21:45:16},
    priority = {4},
    publisher = {IEEE},
    title = {{Effort Prediction in Iterative Software Development Processes -- Incremental Versus Global Prediction Models}},
    url = {http://dx.doi.org/10.1109/esem.2007.16},
    year = {2007}
}

@incollection{Moser2008Case,
    abstract = {{Refactoring is a hot and controversial issue. Supporters claim that it helps increasing the quality of the code, making it easier to understand, modify and maintain. Moreover, there are also claims that refactoring yields higher development productivity – however, there is only limited empirical evidence of such assumption. A case study has been conducted to assess the impact of refactoring in a close-to industrial environment. Results indicate that refactoring not only increases aspects of software quality, but also improves productivity. Our findings are applicable to small teams working in similar, highly volatile domains (ours is application development for mobile devices). However, additional research is needed to ensure that this is indeed true and to generalize it to other contexts.}},
    author = {Moser, Raimund and Abrahamsson, Pekka and Pedrycz, Witold and Sillitti, Alberto and Succi, Giancarlo},
    booktitle = {Balancing Agility and Formalism in Software Engineering},
    citeulike-article-id = {4326222},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/978-3-540-85279-7\_20},
    citeulike-linkout-1 = {http://www.springerlink.com/content/v4m8pt112041587j},
    citeulike-linkout-2 = {http://link.springer.com/chapter/10.1007/978-3-540-85279-7\_20},
    doi = {10.1007/978-3-540-85279-7\_20},
    editor = {Meyer, Bertrand and Nawrocki, JerzyR and Walter, Bartosz},
    journal = {Balancing Agility and Formalism in Software Engineering},
    keywords = {metrics, prom},
    pages = {252--266},
    posted-at = {2013-08-30 21:42:29},
    priority = {4},
    publisher = {Springer Berlin Heidelberg},
    series = {Lecture Notes in Computer Science},
    title = {{A Case Study on the Impact of Refactoring on Quality and Productivity in an Agile Team}},
    url = {http://dx.doi.org/10.1007/978-3-540-85279-7\_20},
    volume = {5082},
    year = {2008}
}

@incollection{Moser2008Model,
    abstract = {{The use of refactoring as a way to continuously improve the design and quality of software and prevent its aging is mostly limited to Agile Methodologies and to a lower amount to software reengineering. In these communities refactoring is supposed to improve in the long-term the structure of existing code in order to make it easier to modify and maintain. To sustain such claims and analyze the impact of refactoring on maintenance we need to know how much refactoring developers do. In few cases such information is directly available for example from CVS log messages. In this study we propose a model on how to mine software repositories in order to obtain information of refactoring effort throughout the evolution of a software system. Moreover, we have developed a prototype that implements our model and validate our approach with two small case studies.}},
    author = {Moser, Raimund and Pedrycz, Witold and Sillitti, Alberto and Succi, Giancarlo},
    booktitle = {Product-Focused Software Process Improvement},
    citeulike-article-id = {12602617},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/978-3-540-69566-0\_29},
    citeulike-linkout-1 = {http://link.springer.com/chapter/10.1007/978-3-540-69566-0\_29},
    doi = {10.1007/978-3-540-69566-0\_29},
    editor = {Jedlitschka, Andreas and Salo, Outi},
    keywords = {metrics, refactoring},
    pages = {360--370},
    posted-at = {2013-08-30 21:30:15},
    priority = {2},
    publisher = {Springer Berlin Heidelberg},
    series = {Lecture Notes in Computer Science},
    title = {{A Model to Identify Refactoring Effort during Maintenance by Mining Source Code Repositories}},
    url = {http://dx.doi.org/10.1007/978-3-540-69566-0\_29},
    volume = {5089},
    year = {2008}
}

@incollection{Coman2008Investigating,
    abstract = {{Pair-programming (PP) is one of the key practices of Agile Methods and there are various claims regarding its benefits. However, the empirical evidence to sustain these claims is insufficient, often coming from studies with students as participants. Moreover, the results are sometimes contradictory. Nevertheless, there are already mature agile teams that currently use PP, pairing on an  ” as needed” basis. We investigate the dynamics of the pairing process in a mature Agile team to understand when practitioners consider PP useful and to compare this with the claimed benefits of PP. In this paper we present the results of a 3 months study of PP in an Agile team of 16 developers.}},
    address = {Berlin, Heidelberg},
    author = {Coman, IrinaDiana and Sillitti, Alberto and Succi, Giancarlo},
    booktitle = {Agile Processes in Software Engineering and Extreme Programming},
    chapter = {13},
    citeulike-article-id = {9338275},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/978-3-540-68255-4\_13},
    citeulike-linkout-1 = {http://www.springerlink.com/content/vvp3q2m580014l36},
    citeulike-linkout-2 = {http://link.springer.com/chapter/10.1007/978-3-540-68255-4\_13},
    doi = {10.1007/978-3-540-68255-4\_13},
    editor = {Abrahamsson, Pekka and Et Al},
    isbn = {978-3-540-68254-7},
    keywords = {gamification, hackystat, metrics, prom},
    pages = {127--136},
    posted-at = {2013-08-30 21:28:56},
    priority = {2},
    publisher = {Springer},
    series = {Lecture Notes in Business Information Processing},
    title = {{Investigating the Usefulness of Pair-Programming in a Mature Agile Team}},
    url = {http://dx.doi.org/10.1007/978-3-540-68255-4\_13},
    volume = {9},
    year = {2008}
}

@inproceedings{Coman2009Casestudy,
    author = {Coman, I. D. and Sillitti, A. and Succi, G.},
    booktitle = {International Conference on Software Engineering, ICSE},
    citeulike-article-id = {12602596},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/icse.2009.5070511},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5070511},
    comment = {(private-note)Case study reports the experiences of the team deploying PROM to an industrial software development organization.  They discuss the challenges such as overcoming developer concerns about the data being shared and what data are shared with the tool. Since the system provides reports on time there were some accuracy issues with reports such as when Pair Programming is practiced or other scenarios where developers are using non-standard editors, virtual machines, or working remotely.  At each stage of deployment additional features were requested some dealing with developer data audits prior to uploading the data.  The paper doesn't discuss metrics that were reported or empirical findings of the deployment.},
    doi = {10.1109/icse.2009.5070511},
    institution = {Free Univ. of Bozen-Bolzano, Bozen},
    isbn = {978-1-4244-3453-4},
    issn = {0270-5257},
    keywords = {gamification, hackystat, metrics, productivity, prom},
    month = may,
    pages = {89--99},
    posted-at = {2013-08-30 20:49:12},
    priority = {5},
    publisher = {IEEE},
    title = {{A case-study on using an Automated In-process Software Engineering Measurement and Analysis system in an industrial environment}},
    url = {http://dx.doi.org/10.1109/icse.2009.5070511},
    year = {2009}
}

@article{DeckerStructural,
    author = {Decker, Michael},
    citeulike-article-id = {12593294},
    citeulike-linkout-0 = {http://etd.ohiolink.edu/ap:0:0:APPLICATION\_PROCESS=DOWNLOAD\_ETD\_SUB\_DOC\_ACCNUM:::F1501\_ID:akron1342458121,inline},
    keywords = {metrics, productivity},
    posted-at = {2013-08-23 21:09:01},
    priority = {0},
    title = {{Structural Analysis of Source Code Changes Through SrcML and Srcdiff.}},
    url = {http://etd.ohiolink.edu/ap:0:0:APPLICATION\_PROCESS=DOWNLOAD\_ETD\_SUB\_DOC\_ACCNUM:::F1501\_ID:akron1342458121,inline}
}

@inproceedings{DeLine2005Easing,
    abstract = {{Large software projects often require a programmer to make changes to unfamiliar source code. This paper describes a set of tools, called Team Tracks, designed to ease program comprehension by showing the source code navigation patterns of fellow development team members. One technique shows a list of related items, given that the user is viewing a given method or class. Another technique shows the favorite classes, by showing a class hierarchy view that hides less frequently visited classes, methods, and members. Two user studies, a laboratory study and a field study, were run to evaluate the effectiveness of these techniques. The results of the two studies demonstrate that sharing navigation data can improve program comprehension and is subjectively preferred by users.}},
    address = {Los Alamitos, CA, USA},
    author = {DeLine, R. and Czerwinski, M. and Robertson, G.},
    booktitle = {2005 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC'05)},
    citeulike-article-id = {1301388},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1092398},
    citeulike-linkout-1 = {http://doi.ieeecomputersociety.org/10.1109/VLHCC.2005.32},
    citeulike-linkout-2 = {http://dx.doi.org/10.1109/vlhcc.2005.32},
    citeulike-linkout-3 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1509509},
    doi = {10.1109/vlhcc.2005.32},
    isbn = {0-7695-2443-5},
    journal = {Visual Languages - Human Centric Computing},
    keywords = {knowledgemgmt},
    location = {Dallas, TX, USA},
    pages = {241--248},
    posted-at = {2013-07-19 20:59:15},
    priority = {2},
    publisher = {IEEE},
    title = {{Easing Program Comprehension by Sharing Navigation Data}},
    url = {http://dx.doi.org/10.1109/vlhcc.2005.32},
    volume = {0},
    year = {2005}
}

@article{Vessey1985Expertise,
    abstract = {{This paper reports the results of an exploratory study that investigated expert and novice debugging processes with the aim of contributing to a general theory of programming expertise. The method used was verbal protocol analysis. Data was collected from 16 programmers employed by the same organization. First, an expert-novice classification of subjects was derived from information based on subjects' problem solving processes: the criterion of expertise was the subjects' ability to chunk effectively the program they were required to debug. Then, significant differences in subjects' approaches to debugging were used to characterize programmers' debugging strategies. Comparisons of these strategies with the expert-novice classification showed programmer expertise based on chunking ability to be strongly related to debugging strategy. The following strategic propositions were identified for further testing. 1. (a) Experts use breadth-first approaches to debugging and, at the same time, adopt a system view of the problem area; (b) Experts are proficient at chunking programs and hence display smooth-flowing approaches to debugging. 2. (a) Novices use breadth-first approaches to debugging but are deficient in their ability to think in system terms; (b) Novices use depth-first approaches to debugging; (c) Novices are less proficient at chunking programs and hence display erratic approaches to debugging.}},
    author = {Vessey, Iris},
    citeulike-article-id = {12516063},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/s0020-7373(85)80054-7},
    doi = {10.1016/s0020-7373(85)80054-7},
    issn = {00207373},
    journal = {International Journal of Man-Machine Studies},
    keywords = {debugging, gameification, productivity},
    month = nov,
    number = {5},
    pages = {459--494},
    posted-at = {2013-07-19 20:32:49},
    priority = {2},
    title = {{Expertise in debugging computer programs: A process analysis}},
    url = {http://dx.doi.org/10.1016/s0020-7373(85)80054-7},
    volume = {23},
    year = {1985}
}

@inproceedings{Parnin2011Are,
    abstract = {{Debugging is notoriously difficult and extremely time consuming. Researchers have therefore invested a considerable amount of effort in developing automated techniques and tools for supporting various debugging tasks. Although potentially useful, most of these techniques have yet to demonstrate their practical effectiveness. One common limitation of existing approaches, for instance, is their reliance on a set of strong assumptions on how developers behave when debugging (e.g., the fact that examining a faulty statement in isolation is enough for a developer to understand and fix the corresponding bug). In more general terms, most existing techniques just focus on selecting subsets of potentially faulty statements and ranking them according to some criterion. By doing so, they ignore the fact that understanding the root cause of a failure typically involves complex activities, such as navigating program dependencies and rerunning the program with different inputs. The overall goal of this research is to investigate how developers use and benefit from automated debugging tools through a set of human studies. As a first step in this direction, we perform a preliminary study on a set of developers by providing them with an automated debugging tool and two tasks to be performed with and without the tool. Our results provide initial evidence that several assumptions made by automated debugging techniques do not hold in practice. Through an analysis of the results, we also provide insights on potential directions for future work in the area of automated debugging.}},
    address = {New York, NY, USA},
    author = {Parnin, Chris and Orso, Alessandro},
    booktitle = {Proceedings of the 2011 International Symposium on Software Testing and Analysis},
    citeulike-article-id = {9637661},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=2001445},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/2001420.2001445},
    comment = {(private-note)The authors investigated questions on whether program slicing and statement prioritization tools help developers locate bugs faster and fix them in less time.  They evaluate the usefulness of ranking statements by the automated tool.  The authors conclude that for simple bugs the program slicing tool is helpful at reducing fix time. For more complex bugs they did find evidence that slicing tools are effective.  They speculate that improved search tools may be key to assisting repair of more complex bugs.},
    doi = {10.1145/2001420.2001445},
    isbn = {978-1-4503-0562-4},
    keywords = {debugging, gameification},
    location = {Toronto, Ontario, Canada},
    pages = {199--209},
    posted-at = {2013-07-19 20:01:53},
    priority = {2},
    publisher = {ACM},
    series = {ISSTA '11},
    title = {{Are automated debugging techniques actually helping programmers?}},
    url = {http://dx.doi.org/10.1145/2001420.2001445},
    year = {2011}
}

@article{Ko2006Exploratory,
    abstract = {{Much of software developers' time is spent understanding unfamiliar code. To better understand how developers gain this understanding and how software development environments might be involved, a study was performed in which developers were given an unfamiliar program and asked to work on two debugging tasks and three enhancement tasks for 70 minutes. The study found that developers interleaved three activities. They began by searching for relevant code both manually and using search tools; however, they based their searches on limited and misrepresentative cues in the code, environment, and executing program, often leading to failed searches. When developers found relevant code, they followed its incoming and outgoing dependencies, often returning to it and navigating its other dependencies; while doing so, however, Eclipse's navigational tools caused significant overhead. Developers collected code and other information that they believed would be necessary to edit, duplicate, or otherwise refer to later by encoding it in the interactive state of Eclipse's package explorer, file tabs, and scroll bars. However, developers lost track of relevant code as these interfaces were used for other tasks, and developers were forced to find it again. These issues caused developers to spend, on average, 35 percent of their time performing the mechanics of navigation within and between source files. These observations suggest a new model of program understanding grounded in theories of information foraging and suggest ideas for tools that help developers seek, relate, and collect information in a more effective and explicit manner.}},
    address = {Piscataway, NJ, USA},
    author = {Ko, Andrew J. and Myers, Brad A. and Coblenz, Michael J. and Aung, Htet H.},
    booktitle = {Software Engineering, IEEE Transactions on},
    citeulike-article-id = {2057824},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1248780},
    citeulike-linkout-1 = {http://doi.ieeecomputersociety.org/10.1109/TSE.2006.116},
    citeulike-linkout-2 = {http://dx.doi.org/10.1109/tse.2006.116},
    citeulike-linkout-3 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4016573},
    comment = {(private-note)Describes the navigation steps and patterns followed by developer working on specific tasks.  Identifies opportunities to improve search tools, recommender, and visitation history tools.  Analysis of interruptions happens as well with effects and ways developers minimize the effects of the interruptions.},
    doi = {10.1109/tse.2006.116},
    institution = {Human-Comput. Interaction Inst., Carnegie Mellon Univ., Pittsburgh, PA},
    issn = {0098-5589},
    journal = {IEEE Trans. Softw. Eng.},
    keywords = {recommending, thesis-ms},
    month = dec,
    number = {12},
    pages = {971--987},
    posted-at = {2013-06-28 20:37:22},
    priority = {4},
    publisher = {IEEE Press},
    title = {{An Exploratory Study of How Developers Seek, Relate, and Collect Relevant Information during Software Maintenance Tasks}},
    url = {http://dx.doi.org/10.1109/tse.2006.116},
    volume = {32},
    year = {2006}
}

@inproceedings{Singer2012It,
    abstract = {{The adoption of software engineering practices cannot always be achieved by education or processes. However, social software has the potential for supporting deliberate behavior change. We present preliminary results of an experiment in which we encouraged computer science students to make more frequent commits to version control by using a social software application. We provided a web-based newsfeed of commits that also displayed a leaderboard. While we have yet to analyze the data, interviews we conducted with the participants allow for first qualitative insights.}},
    author = {Singer, Leif and Schneider, Kurt},
    booktitle = {Second International Workshop on Games and Software Engineering: Realizing User Engagement with Game Engineering Techniques (GAS)},
    citeulike-article-id = {12249369},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/gas.2012.6225927},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6225927},
    comment = {(private-note)Thesis includes workshop paper results.  Proposes a set of interventions or methods to promote adoption of practices for software engineering.  Demonstrates in a study the use of leader board and points for tweaking commit frequency and commit comments.  Results showed statistical significance in improvement for both categories.  Overall good source for innovative ways to promote software engineering practices.  Proposes a process based on different sources for implementing improvements, however, we could look towards process improvement literature for this as well.  Ties together improvement methods with motivation ideas or at least puts them in the same chart.  Interesting discussion of how developers in a social context agree to accept pulls for a repository based on their familiarity or trust of the commit developer. Another interesting set of results from interviews of developers who interact on a social game system for their open source projects.  The most valued aspects were communication/collaboration and endorsement of their work followed by helping others.  Competition was not valued by few people which is interesting when the sites are voluntary and contain competitive aspects.  },
    doi = {10.1109/gas.2012.6225927},
    institution = {Software Eng. Group, Leibniz Univ., Hannover, Germany},
    isbn = {978-1-4673-1768-9},
    keywords = {gamification},
    location = {Zurich, Switzerland},
    month = jun,
    pages = {5--8},
    posted-at = {2013-06-10 20:53:31},
    priority = {0},
    publisher = {IEEE},
    title = {{It was a bit of a race: Gamification of version control}},
    url = {http://dx.doi.org/10.1109/gas.2012.6225927},
    year = {2012}
}

@article{Deci1971Effects,
    author = {Deci, Edward L.},
    citeulike-article-id = {12390051},
    citeulike-linkout-0 = {http://www.utexas.edu/law/journals/tlr/sources/Issue\%2088.6/Lobel/fn103.Deci.pdf},
    journal = {{Journal 0} Personality and Social Psychology},
    posted-at = {2013-06-03 20:59:26},
    priority = {0},
    title = {{Effects of Externally Mediated Rewards on Intrinsic Motivation}},
    url = {http://www.utexas.edu/law/journals/tlr/sources/Issue\%2088.6/Lobel/fn103.Deci.pdf},
    year = {1971}
}

@electronic{Free,
    citeulike-article-id = {12389863},
    citeulike-linkout-0 = {http://www.economist.com/news/finance-and-economics/21578377-why-bosses-should-be-careful-when-using-performance-related-pay-making-pay-work},
    keywords = {gamification},
    posted-at = {2013-06-03 17:46:04},
    priority = {0},
    title = {{Free exchange: Making pay work | The Economist}},
    url = {http://www.economist.com/news/finance-and-economics/21578377-why-bosses-should-be-careful-when-using-performance-related-pay-making-pay-work}
}

@inproceedings{Ying2002Using,
    author = {Ying, Annie T. T.},
    citeulike-article-id = {12380801},
    journal = {In Tool Support for Aspect-Oriented Software Development Workshop. 2002.},
    keywords = {knowledgemgmt, recommending},
    posted-at = {2013-05-31 15:26:15},
    priority = {2},
    title = {{"Using version information for concern inference and code-assist."}},
    year = {2002}
}

@article{Mi1990Knowledgebased,
    author = {Mi, P. and Scacchi, W.},
    citeulike-article-id = {12380796},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/69.60792},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=60792},
    doi = {10.1109/69.60792},
    issn = {10414347},
    journal = {IEEE Transactions on Knowledge and Data Engineering},
    keywords = {knowledgemgmt, recommending},
    month = sep,
    number = {3},
    pages = {283--289},
    posted-at = {2013-05-31 15:23:05},
    priority = {2},
    title = {{A knowledge-based environment for modeling and simulating software engineering processes}},
    url = {http://dx.doi.org/10.1109/69.60792},
    volume = {2},
    year = {1990}
}

@electronic{Evolution,
    citeulike-article-id = {12345470},
    citeulike-linkout-0 = {http://almossawi.com/firefox/},
    keywords = {maintainability},
    posted-at = {2013-05-15 19:57:17},
    priority = {2},
    title = {{Evolution of the Firefox Codebase}},
    url = {http://almossawi.com/firefox/}
}

@electronic{How,
    citeulike-article-id = {12345468},
    citeulike-linkout-0 = {http://almossawi.com/firefox/prose/},
    keywords = {maintainability},
    location = {http://almossawi.com/firefox/prose/},
    posted-at = {2013-05-15 19:52:46},
    priority = {2},
    title = {{How maintainable is the Firefox codebase?}},
    url = {http://almossawi.com/firefox/prose/}
}

@inproceedings{Parnas1994Software,
    abstract = {{An abstract is not available.}},
    address = {Los Alamitos, CA, USA},
    author = {Parnas, David L.},
    booktitle = {Proceedings of the 16th international conference on Software engineering},
    citeulike-article-id = {2949417},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=257788},
    comment = {Discusses causes and factors to consider about software aging.  Not am empirical study but a survey of potential areas to investigate and ways to address software aging problem.  Interesting ideas are that modificaitons to software over time are a cause of decreased maintainability and reliability.  Another idea is that software engineering methods may improve maintenance practices so that the decay in maintainability is lessened or eliminated.},
    isbn = {0-8186-5855-X},
    keywords = {maintainability},
    location = {Sorrento, Italy},
    pages = {279--287},
    posted-at = {2013-05-09 14:51:56},
    priority = {0},
    publisher = {IEEE Computer Society Press},
    series = {ICSE '94},
    title = {{Software aging}},
    url = {http://portal.acm.org/citation.cfm?id=257788},
    year = {1994}
}

@inproceedings{Carter2010Are,
    abstract = {{It would be useful if software engineers/instructors could be aware that remote team members/students are having difficulty with their programming tasks. We have developed an approach that tries to automatically create this semantic awareness based on developers' interactions with the programming environment, which is extended to log these interactions and allow the developers to train or supervise the algorithm by explicitly indicating they are having difficulty. Based on the logs of six programmers, we have found that our approach has high accuracy.}},
    address = {New York, NY, USA},
    author = {Carter, Jason and Dewan, Prasun},
    booktitle = {Proceedings of the 2010 ACM conference on Computer supported cooperative work},
    citeulike-article-id = {12275132},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1718958},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1718918.1718958},
    doi = {10.1145/1718918.1718958},
    isbn = {978-1-60558-795-0},
    keywords = {recommending},
    location = {Savannah, Georgia, USA},
    pages = {211--214},
    posted-at = {2013-04-16 21:52:43},
    priority = {0},
    publisher = {ACM},
    series = {CSCW '10},
    title = {{Are you having difficulty?}},
    url = {http://dx.doi.org/10.1145/1718918.1718958},
    year = {2010}
}

@inproceedings{Parnin2006Enriching,
    abstract = {{Revision history provides a rich source of information to improve the understanding of changes made to programs, but it yields only limited insight into how these changes occurred. We explore an additional source of information - program viewing and editing history - where all historical artifacts associated with the program are included. In particular, we suggest augmenting revision histories with the interaction history of programmers. Using this additional information source enables the development of several interesting applications including an influence-recommendation system and a task-mining system. We present some results from a case study in which interaction histories from professional programmers were obtained and analyzed.}},
    address = {New York, NY, USA},
    author = {Parnin, Chris and G\"{o}rg, Carsten and Rugaber, Spencer},
    booktitle = {Proceedings of the 2006 international workshop on Mining software repositories},
    citeulike-article-id = {784046},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1138019},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1137983.1138019},
    doi = {10.1145/1137983.1138019},
    isbn = {1-59593-397-2},
    keywords = {recommending},
    location = {Shanghai, China},
    pages = {155--158},
    posted-at = {2013-04-16 21:50:07},
    priority = {0},
    publisher = {ACM},
    series = {MSR '06},
    title = {{Enriching revision history with interactions}},
    url = {http://dx.doi.org/10.1145/1137983.1138019},
    year = {2006}
}

@article{Armour2012Business,
    author = {Armour, Philip G.},
    citeulike-article-id = {12086633},
    comment = {(private-note)Describes operationalizing challenges of a measurement system in the context of the maturity of the organization. How applying measurement system in an in-mature organization may not provide meaningful metrics.  Interesting discussion of sources of variability and the concepts of measuring knowledge acquisition as a size measure or even as defects.  Could be a topic to apply knowledge management to software engineering and see how the flow of knowledge applies to the different artifacts.},
    journal = {Communications of the ACM},
    keywords = {knowledgemgmt, metrics},
    month = jun,
    number = {6},
    posted-at = {2013-02-27 17:32:32},
    priority = {0},
    title = {{The Business of Software A Measure of Control}},
    volume = {55},
    year = {2012}
}

@electronic{RauchEnterprise,
    author = {Rauch, Marta},
    citeulike-article-id = {12078371},
    citeulike-linkout-0 = {http://www.slideshare.net/MartaRauch/enterprise-gamification-12989629},
    keywords = {gameification},
    posted-at = {2013-02-26 17:03:12},
    priority = {2},
    title = {{Enterprise Gamification}},
    url = {http://www.slideshare.net/MartaRauch/enterprise-gamification-12989629}
}

@electronic{Gartner,
    citeulike-article-id = {12078369},
    citeulike-linkout-0 = {http://www.gartner.com/newsroom/id/1629214},
    isbn = {http://www.gartner.com/newsroom/id/1629214},
    keywords = {gamification},
    posted-at = {2013-02-26 17:02:06},
    priority = {2},
    title = {{Gartner Says By 2015, More Than 50 Percent of Organizations That Manage Innovation Processes Will Gamify Those Processes}},
    url = {http://www.gartner.com/newsroom/id/1629214}
}

@inproceedings{Binkley2007Source,
    abstract = {{The automated and semi-automated analysis of source code has remained a topic of intense research for more than thirty years. During this period, algorithms and techniques for source-code analysis have changed, sometimes dramatically. The abilities of the tools that implement them have also expanded to meet new and diverse challenges. This paper surveys current work on source-code analysis. It also provides a road map for future work over the next five-year period and speculates on the development of source-code analysis applications, techniques, and challenges over the next 10, 20, and 50 years.}},
    author = {Binkley, D.},
    booktitle = {Future of Software Engineering, 2007. FOSE \&\#039;07},
    citeulike-article-id = {12057403},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/fose.2007.27},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4221615},
    doi = {10.1109/fose.2007.27},
    institution = {Loyola Coll., Baltimore, MD},
    isbn = {0-7695-2829-5},
    keywords = {metrics},
    month = may,
    pages = {104--119},
    posted-at = {2013-02-22 13:26:07},
    priority = {3},
    publisher = {IEEE},
    title = {{Source Code Analysis: A Road Map}},
    url = {http://dx.doi.org/10.1109/fose.2007.27},
    year = {2007}
}

@article{Roberts2006Understanding,
    abstract = {{Understanding what motivates participation is a central theme in the research on open source software (OSS) development. Our study contributes by revealing how the different motivations of OSS developers are interrelated, how these motivations influence participation leading to performance, and how past performance influences subsequent motivations. Drawing on theories of intrinsic and extrinsic motivation, we develop a theoretical model relating the motivations, participation, and performance of OSS developers. We evaluate our model using survey and archival data collected from a longitudinal field study of software developers in the Apache projects. Our results reveal several important findings. First, we find that developers' motivations are not independent but rather are related in complex ways. Being paid to contribute to Apache projects is positively related to developers' status motivations but negatively related to their use-value motivations. Perhaps surprisingly, we find no evidence of diminished intrinsic motivation in the presence of extrinsic motivations; rather, status motivations enhance intrinsic motivations. Second, we find that different motivations have an impact on participation in different ways. Developers' paid participation and status motivations lead to above-average contribution levels, but use-value motivations lead to below-average contribution levels, and intrinsic motivations do not significantly impact average contribution levels. Third, we find that developers' contribution levels positively impact their performance rankings. Finally, our results suggest that past-performance rankings enhance developers' subsequent status motivations.}},
    address = {Institute for Operations Research and the Management Sciences (INFORMS), Linthicum, Maryland, USA},
    author = {Roberts, Jeffrey A. and Hann, Il-Horn and Slaughter, Sandra A.},
    citeulike-article-id = {10064228},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1246148.1246151},
    citeulike-linkout-1 = {http://dx.doi.org/10.1287/mnsc.1060.0554},
    citeulike-linkout-2 = {http://mansci.journal.informs.org/content/52/7/984.abstract},
    citeulike-linkout-3 = {http://mansci.journal.informs.org/content/52/7/984.full.pdf},
    day = {01},
    doi = {10.1287/mnsc.1060.0554},
    issn = {1526-5501},
    journal = {Management Science},
    keywords = {motivation},
    month = jul,
    number = {7},
    pages = {984--999},
    posted-at = {2013-02-20 17:06:02},
    priority = {2},
    publisher = {INFORMS},
    title = {{Understanding the Motivations, Participation, and Performance of Open Source Software Developers: A Longitudinal Study of the Apache Projects}},
    url = {http://dx.doi.org/10.1287/mnsc.1060.0554},
    volume = {52},
    year = {2006}
}

@incollection{Paiva2010Factors,
    abstract = {{To measure and improve the productivity of software developers is one of the greatest challenges faced by software development companies. Therefore, aiming to help these companies to identify possible causes that interfere in the productivity of their teams, we present in this paper a list of 32 factors, extracted from the literature, that influence the productivity of developers. To obtain the ranking of these factors, we have applied a questionnaire with developers. In this work, we present the results: the factors that have the greatest positive and negative influence on productivity, the factors with no influence and the most important factors and what influences them. To finish, we present a comparison of the results obtained from the literature.}},
    address = {Dordrecht},
    author = {Paiva, Edgy and Barbosa, Danielly and Lima, Roberto and Albuquerque, Adriano},
    booktitle = {Innovations in Computing Sciences and Software Engineering},
    chapter = {17},
    citeulike-article-id = {7402817},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/978-90-481-9112-3\_17},
    citeulike-linkout-1 = {http://www.springerlink.com/content/guh655w714744r09},
    citeulike-linkout-2 = {http://link.springer.com/chapter/10.1007/978-90-481-9112-3\_17},
    comment = {Provides a literature survey based analysis of the important positive, neutral, and negative factors influencing productivity.  Though commitment and motivation are top ranked positive influences, developer tools ranks \#7 under positive influence supporting the idea that better tools make developers more productive.},
    doi = {10.1007/978-90-481-9112-3\_17},
    editor = {Sobh, Tarek and Elleithy, Khaled},
    isbn = {978-90-481-9111-6},
    keywords = {motivation, productivity},
    pages = {99--104},
    posted-at = {2013-02-20 16:54:30},
    priority = {0},
    publisher = {Springer Netherlands},
    title = {{Factors that Influence the Productivity of Software Developers in a Developer View}},
    url = {http://dx.doi.org/10.1007/978-90-481-9112-3\_17},
    year = {2010}
}

@article{Nasraoui2008Web,
    abstract = {{In this paper, we present a complete framework and findings in mining Web usage patterns from Web log files of a real Web site that has all the challenging aspects of real-life Web usage mining, including evolving user profiles and external data describing an ontology of the Web content. Even though the Web site under study is part of a nonprofit organization that does not "sell" any products, it was crucial to understand "who" the users were, "what" they looked at, and "how their interests changed with time," all of which are important questions in Customer Relationship Management (CRM). Hence, we present an approach for discovering and tracking evolving user profiles. We also describe how the discovered user profiles can be enriched with explicit information need that is inferred from search queries extracted from Web log data. Profiles are also enriched with other domain-specific information facets that give a panoramic view of the discovered mass usage modes. An objective validation strategy is also used to assess the quality of the mined profiles, in particular their adaptability in the face of evolving user behavior.}},
    address = {Los Alamitos, CA, USA},
    author = {Nasraoui, O. and Soliman, M. and Saka, E. and Badia, A. and Germain, R.},
    booktitle = {IEEE Transactions on Knowledge and Data Engineering},
    citeulike-article-id = {2487252},
    citeulike-linkout-0 = {http://doi.ieeecomputersociety.org/10.1109/TKDE.2007.190667},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/tkde.2007.190667},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4358953},
    day = {26},
    doi = {10.1109/tkde.2007.190667},
    institution = {Univ. of Louisville, Louisville},
    issn = {1041-4347},
    journal = {IEEE Transactions on Knowledge and Data Engineering},
    keywords = {gameification},
    month = feb,
    number = {2},
    pages = {202--215},
    posted-at = {2013-02-19 22:43:13},
    priority = {0},
    publisher = {IEEE},
    title = {{A Web Usage Mining Framework for Mining Evolving User Profiles in Dynamic Web Sites}},
    url = {http://dx.doi.org/10.1109/tkde.2007.190667},
    volume = {20},
    year = {2008}
}

@article{Murphy2006How,
    abstract = {{The Eclipse integrated development environment continues to gain popularity among Java developers. Our usage monitoring approach allows tool builders to sample how developers are using their tools in the wild. The data gathered about tool use can be used to prevent feature bloat and to evolve the environments according to user needs. Information about how developers work in a development environment can also provide a baseline for assessing new software development tools. We hope this report provides a start in defining which in formation to collect and distribute on an on going basis to help improve Eclipse and other similar platforms and tools}},
    address = {Los Alamitos, CA, USA},
    author = {Murphy, G. C. and Kersten, M. and Findlater, L.},
    citeulike-article-id = {5403310},
    citeulike-linkout-0 = {http://doi.ieeecomputersociety.org/10.1109/MS.2006.105},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/ms.2006.105},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1657944},
    doi = {10.1109/ms.2006.105},
    institution = {Dept. of Comput. Sci., British Columbia Univ., Vancouver, BC},
    issn = {0740-7459},
    journal = {Software, IEEE},
    keywords = {gameification, ide},
    month = jul,
    number = {4},
    pages = {76--83},
    posted-at = {2013-02-19 22:41:14},
    priority = {0},
    publisher = {IEEE},
    title = {{How are Java software developers using the Elipse IDE?}},
    url = {http://dx.doi.org/10.1109/ms.2006.105},
    volume = {23},
    year = {2006}
}

@electronic{KellyReal,
    author = {Kelly, Tadhg},
    citeulike-article-id = {11839919},
    citeulike-linkout-0 = {http://techcrunch.com/2012/12/08/real-vs-fake-gamification-mechanics/},
    keywords = {gameification},
    posted-at = {2012-12-11 19:10:50},
    priority = {0},
    title = {{Real Gamification Mechanics Require Simplicity And, Yes, Game Designers Can Do It}},
    url = {http://techcrunch.com/2012/12/08/real-vs-fake-gamification-mechanics/}
}

@electronic{BurkeGamification,
    author = {Burke, Brian},
    citeulike-article-id = {11839887},
    citeulike-linkout-0 = {http://www.gartner.com/id=2226015},
    comment = {(private-note)Provides a view of how gamification may look in 7 years though I'd say not too crazy in the ideas presented.  Talks more about acceptance and specifics about acceptance in different applications.  Like the following:
In Gartner's "Hype Cycle for Emerging Energy Technologies, 2012," we placed gamification in the Peak of Inflated Expectations, with the expectation of reaching the Plateau of Productivity in five to 10 years. We expect that gamification will enter the Trough of Disillusionment within the next two years, driven primarily by the lack of understanding of game design and player engagement strategies, resulting in many failed applications. But we also believe that gamification, applied with correct game design principles, will have a significant impact in many domains, and in some fields, the use of game mechanics will have a transformational impact. 

Best Practice: While employee behavior design is an attractive idea, organizations must approach employee-facing gamification applications with caution. Employees must not feel manipulated or intimidated, but enabled to achieve their goals. 6 Organizations should seek to clearly define the organizational objectives of employee-facing applications, understand employee objectives and focus on where the two overlap. Applications should be people-centric and enable employees to be successful in achieving their objectives — where they are aligned with organizational objectives. },
    keywords = {gamification},
    posted-at = {2012-12-11 18:35:18},
    priority = {0},
    title = {{Gamification 2020: What Is the Future of Gamification?}},
    url = {http://www.gartner.com/id=2226015}
}

@electronic{Exploring,
    citeulike-article-id = {11831262},
    citeulike-linkout-0 = {http://sewiki.iai.uni-bonn.de/research/cultivate/tutorial\_exploring\_smells\_and\_metrics},
    keywords = {metrics, td},
    posted-at = {2012-12-06 22:02:19},
    priority = {0},
    title = {{Exploring Smells and Metrics}},
    url = {http://sewiki.iai.uni-bonn.de/research/cultivate/tutorial\_exploring\_smells\_and\_metrics}
}

@inproceedings{Koziolek2012MORPHOSIS,
    abstract = {{Managing the cost-effective evolution of industrial software systems is a challenging task because of their complexity and long lifetimes. Limited pro-active evolution planning and software architecture erosion often lead to huge maintenance costs in such systems. However, formerly researched approaches for evolution scenario analysis and architecture enforcement are only reluctantly applied by practitioners due to their perceived overhead and high costs. We have applied several recent sustainability evaluation and improvement approaches in a case study to the software architecture of a large industrial software system currently under development at ABB. We combined our selection of approaches in a lightweight method called MORPHOSIS, for which this paper presents experiences and lessons learned. We found that reasonable sustainability evaluation and improvement is possible already with limited efforts.}},
    author = {Koziolek, H. and Domis, D. and Goldschmidt, T. and Vorst, P. and Weiss, R. J.},
    booktitle = {Software Architecture (WICSA) and European Conference on Software Architecture (ECSA), 2012 Joint Working IEEE/IFIP Conference on},
    citeulike-article-id = {11831234},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/wicsa-ecsa.212.40},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6337730},
    doi = {10.1109/wicsa-ecsa.212.40},
    institution = {Ind. Software Syst. Program, ABB Corp. Res., Ladenburg, Germany},
    isbn = {978-1-4673-2809-8},
    keywords = {maintainability, metrics},
    month = aug,
    pages = {253--257},
    posted-at = {2012-12-06 21:46:47},
    priority = {4},
    publisher = {IEEE},
    title = {{MORPHOSIS: A Lightweight Method Facilitating Sustainable Software Architectures}},
    url = {http://dx.doi.org/10.1109/wicsa-ecsa.212.40},
    year = {2012}
}

@inproceedings{Shihab2012Industrial,
    abstract = {{Modelling and understanding bugs has been the focus of much of the Software Engineering research today. However, organizations are interested in more than just bugs. In particular, they are more concerned about managing risk, i.e., the likelihood that a code or design change will cause a negative impact on their products and processes, regardless of whether or not it introduces a bug. In this paper, we conduct a year-long study involving more than 450 developers of a large enterprise, spanning more than 60 teams, to better understand risky changes, i.e., changes for which developers believe that additional attention is needed in the form of careful code or design reviewing and/or more testing. Our findings show that different developers and different teams have their own criteria for determining risky changes. Using factors extracted from the changes and the history of the files modified by the changes, we are able to accurately identify risky changes with a recall of more than 67\%, and a precision improvement of 87\% (using developer specific models) and 37\% (using team specific models), over a random model. We find that the number of lines and chunks of code added by the change, the bugginess of the files being changed, the number of bug reports linked to a change and the developer experience are the best indicators of change risk. In addition, we find that when a change has many related changes, the reliability of developers in marking risky changes is negatively affected. Our findings and models are being used today in practice to manage the risk of software projects.}},
    address = {New York, NY, USA},
    author = {Shihab, Emad and Hassan, Ahmed E. and Adams, Bram and Jiang, Zhen M.},
    booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
    citeulike-article-id = {11831220},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=2393670},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/2393596.2393670},
    doi = {10.1145/2393596.2393670},
    isbn = {978-1-4503-1614-9},
    keywords = {metrics, prediction},
    location = {Cary, North Carolina},
    posted-at = {2012-12-06 21:21:56},
    priority = {0},
    publisher = {ACM},
    series = {FSE '12},
    title = {{An industrial study on the risk of software changes}},
    url = {http://dx.doi.org/10.1145/2393596.2393670},
    year = {2012}
}

@article{Dubey2011Assessment,
    abstract = {{Many organizations assess the maintainability of software systems before they are deployed. Object-oriented design has been shown to be a useful technique to develop and deliver quality software. Objectoriented metrics can be used to assess the maintainability of a software system. Various software metrics and models have been developed and described. This paper provides a review of this literature and the related state-of-the-art. It also proposes a maintainability model that is based on the analysis of the relationship between object-oriented metrics and maintainability.}},
    address = {New York, NY, USA},
    author = {Dubey, Sanjay K. and Rana, Ajay},
    citeulike-article-id = {11831190},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=2020983},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/2020976.2020983},
    comment = {(private-note)Pretty much a lit review of oo metrics predicting fault-prone or maintainability of oo systems.  No new sources found so it is a little surprising that a 1993 study on maintainability in ADA has no further validation.},
    doi = {10.1145/2020976.2020983},
    issn = {0163-5948},
    journal = {SIGSOFT Softw. Eng. Notes},
    keywords = {maintainability, metrics},
    month = sep,
    number = {5},
    pages = {1--7},
    posted-at = {2012-12-06 21:03:12},
    priority = {2},
    publisher = {ACM},
    title = {{Assessment of maintainability metrics for object-oriented software system}},
    url = {http://dx.doi.org/10.1145/2020976.2020983},
    volume = {36},
    year = {2011}
}

@article{Kaur2011Determination,
    abstract = {{In Object Oriented System, the quality of software depends significantly on the decision taken at early phases of the development. As per available artefact, quality of the class description is very crucial for system development. Maintenance is to repair defects in software, to adapt the software to different operational environments and to add or modify the functionality of the system. In Object Oriented Systems, maintainability factor needs more aspects to explore. We have proposed Maintainability Index using Package Metrics. We performed empirical evaluation using three case studies.}},
    address = {New York, NY, USA},
    author = {Kaur, Kulwant and Singh, Hardeep},
    citeulike-article-id = {11831168},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1943383},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1943371.1943383},
    comment = {(private-note)Computes maintainability of the package level (package consisting of multiple classes).  Uses S = Avg (Number of Classes in the package and its sub packages)
+ (Number of operations per class per module)
CC = Ln(Relationship between Classes\& interfaces) + Relational
Cohesion + Instability
NC = (Nesting* Conncomp) + Ln(Abstractness + number of
packages on which classes and interfaces)},
    doi = {10.1145/1943371.1943383},
    issn = {0163-5948},
    journal = {SIGSOFT Softw. Eng. Notes},
    keywords = {maintainability, metrics},
    month = may,
    number = {2},
    pages = {1--6},
    posted-at = {2012-12-06 20:46:05},
    priority = {0},
    publisher = {ACM},
    title = {{Determination of Maintainability Index for Object Oriented Systems}},
    url = {http://dx.doi.org/10.1145/1943371.1943383},
    volume = {36},
    year = {2011}
}

@inproceedings{Tang1999Empirical,
    abstract = {{The objective of this study is the investigation of the correlation between object-oriented design metrics and the likelihood of the occurrence of object oriented faults. Such a relationship, if identified, can be utilized to select effective testing techniques that take the characteristics of the program under test into account. Our empirical study was conducted on three industrial real-time systems that contain a number of natural faults reported for the past three years. The faults found in these three systems are classified into three types: object-oriented faults, object management faults and traditional faults. The object-oriented design metrics suite proposed by Chidamber and Kemerer (1994) is validated using these faults. Moreover, we propose a set of new metrics that can serve as an indicator of how strongly object-oriented a program is, so that the decision to adopt object oriented testing techniques can be made, to achieve more reliable testing and yet minimize redundant testing efforts}},
    author = {Tang, Mei-Huei and Kao, Ming-Hung and Chen, Mei-Hwa},
    booktitle = {Software Metrics Symposium, 1999. Proceedings. Sixth International},
    citeulike-article-id = {11831158},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/metric.1999.809745},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=809745},
    doi = {10.1109/metric.1999.809745},
    institution = {Dept. of Comput. Sci., State Univ. of New York, Albany, NY},
    isbn = {0-7695-0403-5},
    keywords = {maintainability, metrics},
    pages = {242--249},
    posted-at = {2012-12-06 20:38:24},
    priority = {2},
    publisher = {IEEE},
    title = {{An empirical study on object-oriented metrics}},
    url = {http://dx.doi.org/10.1109/metric.1999.809745},
    year = {1999}
}

@article{Li1993Objectoriented,
    abstract = {{Software metrics have been studied in the procedural paradigm as a quantitative means of assessing the software development process as well as the quality of software products. Several studies have validated that various metrics are useful indicators of maintenance effort in the procedural paradigm. However, software metrics have rarely been studied in the object-oriented paradigm. Very few metrics have been proposed to measure object-oriented systems, and the proposed ones have not been validated. This research concentrates on several object-oriented software metrics and the validation of these metrics with maintenance effort in two commercial systems. Statistical analyses of a prediction model incorporating 10 metrics were performed. In addition, a more compact model with fewer metrics is presented.}},
    address = {New York, NY, USA},
    author = {Li, Wei and Henry, Sallie},
    citeulike-article-id = {2827568},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=170619.170622},
    citeulike-linkout-1 = {http://dx.doi.org/10.1016/0164-1212(93)90077-b},
    comment = {(private-note)Evaluates object oriented software metrics for their correlation with maintenance effort in ADA software.  It finds that 8 metrics are correlated providing more information than size in the population.  },
    doi = {10.1016/0164-1212(93)90077-b},
    issn = {01641212},
    journal = {Journal of Systems and Software},
    keywords = {maintainability, metrics},
    month = nov,
    number = {2},
    pages = {111--122},
    posted-at = {2012-12-06 20:28:30},
    priority = {0},
    publisher = {Elsevier Science Inc.},
    title = {{Object-oriented metrics that predict maintainability}},
    url = {http://dx.doi.org/10.1016/0164-1212(93)90077-b},
    volume = {23},
    year = {1993}
}

@inproceedings{Dagpinar2003Predicting,
    author = {Dagpinar, Melis and Jahnke, Jens H.},
    booktitle = {Working Conference on Reverse Engineering (WCRE)},
    citeulike-article-id = {11831134},
    comment = {(private-note)Creates a set of object oriented metrics and validates them as relating to maintainability of object oriented software.  Not sure the metrics selected are available in klocwork but the study was commissioned by klocwork.  Metrics selected were Total Number Of Statements (TNOS), Non-inheritance class-method import coupling (NICMIC), Non-inheritance method-method import coupling (NIMMIC) and Non-inheritance import coupling (NIIC).},
    journal = {Proceedings of the 10th Working Conference on Reverse Engineering (WCRE)},
    keywords = {maintainability, metrics},
    pages = {155--164},
    posted-at = {2012-12-06 20:20:04},
    priority = {0},
    title = {{Predicting maintainability with object-oriented metrics-an empirical comparison}},
    year = {2003}
}

@article{Malhotra2011Software,
    abstract = {{There always has been a demand to produce efficient and high quality software. There are various object oriented metrics that measure various properties of the software like coupling, cohesion, inheritance etc. which affect the software to a large extent. These metrics can be used in predicting important quality attributes such as fault proneness, maintainability, effort, productivity and reliability. Early prediction of fault proneness will help us to focus on testing resources and use them only on the classes which are predicted to be fault-prone. Thus, this will help in early phases of software development to give a measurement of quality assessment. This paper provides the review of the previous studies which are related to software metrics and the fault proneness. In other words, it reviews several journals and conference papers on software fault prediction. There is large number of software metrics proposed in the literature. Each study uses a different subset of these metrics and performs the analysis using different datasets. Also, the researchers have used different approaches such as Support vector machines, naive bayes network, random forest, artificial neural network, decision tree, logistic regression etc. Thus, this study focuses on the metrics used, dataset used and the evaluation or analysis method used by various authors. This review will be beneficial for the future studies as various researchers and practitioners can use it for comparative analysis.}},
    address = {New York, NY, USA},
    author = {Malhotra, Ruchika and Jain, Ankita},
    citeulike-article-id = {11828063},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=2020991},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/2020976.2020991},
    doi = {10.1145/2020976.2020991},
    issn = {0163-5948},
    journal = {SIGSOFT Softw. Eng. Notes},
    keywords = {metrics, prediction},
    month = sep,
    number = {5},
    pages = {1--6},
    posted-at = {2012-12-06 19:46:36},
    priority = {2},
    publisher = {ACM},
    title = {{Software fault prediction for object oriented systems: a literature review}},
    url = {http://dx.doi.org/10.1145/2020976.2020991},
    volume = {36},
    year = {2011}
}

@misc{Rizvi2010Maintainability,
    abstract = {{Measuring software maintainability early in the development life cycle,
especially at the design phase, may help designers to incorporate required
enhancement and corrections for improving maintainability of the final
software. This paper developed a multivariate linear model 'Maintainability
Estimation Model for Object-Oriented software in Design phase' (MEMOOD), which
estimates the maintainability of class diagrams in terms of their
understandability and modifiability. While, in order to quantify class
diagram's understandability and modifiability the paper further developed two
more multivariate models. These two models use design level object-oriented
metrics, to quantify understandability and modifiability of class diagram. Such
early quantification of maintainability provides an opportunity to improve the
maintainability of class diagram and consequently the maintainability of final
software. All the three models have been validated through appropriate
statistical measures and contextual interpretation has been drawn.}},
    archivePrefix = {arXiv},
    author = {Rizvi, S. W. A. and Khan, R. A.},
    citeulike-article-id = {11828042},
    citeulike-linkout-0 = {http://arxiv.org/abs/1004.4447},
    citeulike-linkout-1 = {http://arxiv.org/pdf/1004.4447},
    comment = {(private-note)Describes metrics that express the maintainability, understandability, and modifiability of class diagrams.  These metrics do not translate to code measurements.},
    day = {26},
    eprint = {1004.4447},
    keywords = {maintainability, metrics},
    month = apr,
    posted-at = {2012-12-06 19:32:55},
    priority = {0},
    title = {{Maintainability Estimation Model for Object-Oriented Software in Design Phase (MEMOOD)}},
    url = {http://arxiv.org/abs/1004.4447},
    year = {2010}
}

@inproceedings{Marinescu2001Detecting,
    abstract = {{The industry is nowadays confronted with large-scale monolithic and inflexible object-oriented software. Because of their high business value, these legacy systems must be re-engineered. One of the important issues in re-engineering is the detection and location of design flaws, which prevent the efficient maintenance and further development of the system. In this paper, we present a metrics-based approach for detecting design problems, and we describe two concrete techniques for the detection of two well-known design flaws found in the literature. We apply our technique to an industrial case study and discuss the findings. The proposed technique indeed found real flaws in the system and the experiment suggests that, based on the same approach, further detection techniques for other common design flaws could be defined}},
    author = {Marinescu, R.},
    booktitle = {Technology of Object-Oriented Languages and Systems, 2001. TOOLS 39. 39th International Conference and Exhibition on},
    citeulike-article-id = {11827336},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/tools.2001.941671},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=941671},
    doi = {10.1109/tools.2001.941671},
    institution = {Dept. of Comput. Sci, Politehnica Univ. of Timisoara},
    isbn = {0-7695-1251-8},
    issn = {1530-2067},
    keywords = {metrics, td},
    pages = {173--182},
    posted-at = {2012-12-06 14:18:22},
    priority = {2},
    publisher = {IEEE},
    title = {{Detecting design flaws via metrics in object-oriented systems}},
    url = {http://dx.doi.org/10.1109/tools.2001.941671},
    year = {2001}
}

@article{Basili1996Validation,
    abstract = {{This paper presents the results of a study in which we empirically investigated the suite of object-oriented (OO) design metrics introduced in (Chidamber and Kemerer, 1994). More specifically, our goal is to assess these metrics as predictors of fault-prone classes and, therefore, determine whether they can be used as early quality indicators. This study is complementary to the work described in (Li and Henry, 1993) where the same suite of metrics had been used to assess frequencies of maintenance changes to classes. To perform our validation accurately, we collected data on the development of eight medium-sized information management systems based on identical requirements. All eight projects were developed using a sequential life cycle model, a well-known OO analysis/design method and the C++ programming language. Based on empirical and quantitative analysis, the advantages and drawbacks of these OO metrics are discussed. Several of Chidamber and Kemerer's OO metrics appear to be useful to predict class fault-proneness during the early phases of the life-cycle. Also, on our data set, they are better predictors than  ” traditional” code metrics, which can only be collected at a later phase of the software development processes}},
    address = {Piscataway, NJ, USA},
    author = {Basili, V. R. and Briand, L. C. and Melo, W. L.},
    citeulike-article-id = {349966},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=239308},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/32.544352},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=544352},
    day = {06},
    doi = {10.1109/32.544352},
    institution = {Dept. of Comput. Sci., Maryland Univ., College Park, MD},
    issn = {0098-5589},
    journal = {Software Engineering, IEEE Transactions on},
    keywords = {metrics},
    month = oct,
    number = {10},
    pages = {751--761},
    posted-at = {2012-12-05 22:24:47},
    priority = {2},
    publisher = {IEEE},
    title = {{A validation of object-oriented design metrics as quality indicators}},
    url = {http://dx.doi.org/10.1109/32.544352},
    volume = {22},
    year = {1996}
}

@inproceedings{Nord2012In,
    abstract = {{Practices designed to expedite the delivery of stakeholder value can paradoxically lead to unexpected rework costs that ultimately degrade the flow of value over time. This is especially observable when features are developed based on immediate value, while dependencies that may slow down future development efforts are neglected. The technical debt metaphor conceptualizes this tradeoff between short-term and long-term value: taking shortcuts to optimize the delivery of features in the short term incurs debt, analogous to financial debt, that must be paid off later to optimize long-term success. In this paper, we describe taking an architecture-focused and measurement-based approach to develop a metric that assists in strategically managing technical debt. Such an approach can be used to optimize the cost of development over time while continuing to deliver value to the customer. We demonstrate our approach by describing its application to an ongoing system development effort.}},
    author = {Nord, R. L. and Ozkaya, I. and Kruchten, P. and Gonzalez-Rojas, M.},
    booktitle = {Software Architecture (WICSA) and European Conference on Software Architecture (ECSA), 2012 Joint Working IEEE/IFIP Conference on},
    citeulike-article-id = {11826703},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/wicsa-ecsa.212.17},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6337765},
    comment = {(private-note)This paper defines a formula for estimating the impact of architecture debt upon a potential new feature development.  The formula considers change propagation, number of dependencies, and rework estimates for existing components that must be rearchitected.  They illustrate the use of this equation in a case study showing that making an investment in architecture components that do not support functionality delivered in the first release is more cost effective in the long run.},
    doi = {10.1109/wicsa-ecsa.212.17},
    institution = {Software Eng. Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA},
    isbn = {978-1-4673-2809-8},
    keywords = {metrics, td},
    month = aug,
    pages = {91--100},
    posted-at = {2012-12-05 22:12:26},
    priority = {2},
    publisher = {IEEE},
    title = {{In Search of a Metric for Managing Architectural Technical Debt}},
    url = {http://dx.doi.org/10.1109/wicsa-ecsa.212.17},
    year = {2012}
}

@article{Beecham2008Motivation,
    abstract = {{Objective: In this paper, we present a systematic literature review of motivation in Software Engineering. The objective of this review is to plot the landscape of current reported knowledge in terms of what motivates developers, what de-motivates them and how existing models address motivation. Methods: We perform a systematic literature review of peer reviewed published studies that focus on motivation in Software Engineering. Systematic reviews are well established in medical research and are used to systematically analyse the literature addressing specific research questions. Results: We found 92 papers related to motivation in Software Engineering. Fifty-six percent of the studies reported that Software Engineers are distinguishable from other occupational groups. Our findings suggest that Software Engineers are likely to be motivated according to three related factors: their 'characteristics' (for example, their need for variety); internal 'controls' (for example, their personality) and external 'moderators' (for example, their career stage). The literature indicates that de-motivated engineers may leave the organisation or take more sick-leave, while motivated engineers will increase their productivity and remain longer in the organisation. Aspects of the job that motivate Software Engineers include problem solving, working to benefit others and technical challenge. Our key finding is that the published models of motivation in Software Engineering are disparate and do not reflect the complex needs of Software Engineers in their career stages, cultural and environmental settings. Conclusions: The literature on motivation in Software Engineering presents a conflicting and partial picture of the area. It is clear that motivation is context dependent and varies from one engineer to another. The most commonly cited motivator is the job itself, yet we found very little work on what it is about that job that Software Engineers find motivating. Furthermore, surveys are often aimed at how Software Engineers feel about 'the organisation', rather than 'the profession'. Although models of motivation in Software Engineering are reported in the literature, they do not account for the changing roles and environment in which Software Engineers operate. Overall, our findings indicate that there is no clear understanding of the Software Engineers' job, what motivates Software Engineers, how they are motivated, or the outcome and benefits of motivating Software Engineers.}},
    address = {Newton, MA, USA},
    author = {Beecham, Sarah and Baddoo, Nathan and Hall, Tracy and Robinson, Hugh and Sharp, Helen},
    citeulike-article-id = {8184295},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1379990},
    citeulike-linkout-1 = {http://dx.doi.org/10.1016/j.infsof.2007.09.004},
    comment = {(private-note)This paper provides a thorough analysis of existing literature on studies of motivation in Software Engineering.  It has some valuable lists of factors that motivate software engineers with the most common being "the work" motivates us.  The list of de-motivators is also useful with common job satisfaction items like stress and inequity in recognition, plus poor quality software (low accomplishment) and lack of influence in decision making.  The paper also lists characteristics of people in the professions including need for independence (autonomy), desire to learn new skills/tackle challenges.},
    doi = {10.1016/j.infsof.2007.09.004},
    issn = {0950-5849},
    journal = {Inf. Softw. Technol.},
    keywords = {motivation},
    month = aug,
    number = {9-10},
    pages = {860--878},
    posted-at = {2012-11-29 16:00:35},
    priority = {4},
    publisher = {Butterworth-Heinemann},
    title = {{Motivation in Software Engineering: A systematic literature review}},
    url = {http://dx.doi.org/10.1016/j.infsof.2007.09.004},
    volume = {50},
    year = {2008}
}

@inproceedings{Sharp2007Exploring,
    abstract = {{In this paper, we describe our investigation of the motivational differences between project managers and developers. Motivation has been found to be a central factor in successful software projects. However the motivation of software engineers is generally poorly understood and previous work done in the area is thought to be largely out-of-date. We present data collected from 6 software developers and 4 project managers at a workshop we organized at the XP2006 international conference.}},
    address = {New York, NY, USA},
    author = {Sharp, Helen and Hall, Tracy and Baddoo, Nathan and Beecham, Sarah},
    booktitle = {Proceedings of the the 6th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering},
    citeulike-article-id = {11810064},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1287695},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1287624.1287695},
    doi = {10.1145/1287624.1287695},
    isbn = {978-1-59593-811-4},
    keywords = {motivation},
    location = {Dubrovnik, Croatia},
    pages = {501--504},
    posted-at = {2012-11-29 15:59:33},
    priority = {2},
    publisher = {ACM},
    series = {ESEC-FSE '07},
    title = {{Exploring motivational differences between software developers and project managers}},
    url = {http://dx.doi.org/10.1145/1287624.1287695},
    year = {2007}
}

@inproceedings{Steinberga2011Towards,
    abstract = {{Motivation in software engineering is reported to be a source for performance improvement, which leads to project overall success. Since it is a soft factor and difficult to quantify it is usually neglected. Research in this field is rather scarce and outdated. On the basis of a recent systematic review of software engineers' motivation we set an agenda for further investigation of the role of motivation in contemporary projects. As software organizations nowadays seek opportunities inherited in both - global software development (GSD) and agile projects, it is important to understand how different project environments influence motivation.}},
    address = {Washington, DC, USA},
    author = {\v{S}teinberga, Liva and \v{S}mite, Darja},
    booktitle = {Proceedings of the 2011 IEEE Sixth International Conference on Global Software Engineering Workshop},
    citeulike-article-id = {11810062},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=2066309.2067736},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/icgse-w.2011.31},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6065591},
    doi = {10.1109/icgse-w.2011.31},
    institution = {Univ. of Latvia, Riga, Latvia},
    isbn = {978-0-7695-4558-5},
    keywords = {motivation},
    month = aug,
    pages = {117--119},
    posted-at = {2012-11-29 15:57:18},
    priority = {2},
    publisher = {IEEE Computer Society},
    series = {ICGSE-W '11},
    title = {{Towards Understanding of Software Engineer Motivation in Globally Distributed Projects}},
    url = {http://dx.doi.org/10.1109/icgse-w.2011.31},
    year = {2011}
}

@article{Jagacinski2001Comparative,
    abstract = {{As research on achievement goals has increased, so has the number of different measures of goal orientations. In this article, the authors examined three popular measures of task and ego achievement goal orientations in terms of factorial and construct validity, internal consistency reliability, and distributional characteristics. College students completed the Ability and Task Orientation scales from the Patterns of Adaptive Learning Survey (PALS), Task and Ego Motivational Orientation Scales, and the General Learning and Performance Goal Orientations Scales. The first two sets of scales assess goal orientations for schoolwork, whereas the third is more general. To assess construct validity, the scales were correlated with the Jackson Achievement Orientation Scale, beliefs concerning the role of effort and ability in success, and interest/enjoyment of and boredom with schoolwork. The PALS scales generally faired the best in terms of distributional characteristics, factorial validity, and construct validity.}},
    author = {Jagacinski, Carolyn M. and Duda, Joan L.},
    citeulike-article-id = {11806577},
    citeulike-linkout-0 = {http://dx.doi.org/10.1177/00131640121971626},
    citeulike-linkout-1 = {http://epm.sagepub.com/content/61/6/1013.abstract},
    citeulike-linkout-2 = {http://epm.sagepub.com/content/61/6/1013.full.pdf},
    comment = {(private-note)This paper compares 3 survey instruments related to acheivement and motivation.  The focsu of the study determine whether each instrument describes task oriented measures or ego oriented measures.  Task oriented measures are associated with the belief that effort leads to success (accomplishing a task).  Ego oriented meaures are associated with the belief that ability is necessary for success.  From the descriptions provided we picked the General Learning and Performance Orientation scales developed by Button and colleagues because it applies across domains (not just academic performance).},
    day = {01},
    doi = {10.1177/00131640121971626},
    issn = {1552-3888},
    journal = {Educational and Psychological Measurement},
    month = dec,
    number = {6},
    pages = {1013--1039},
    posted-at = {2012-11-28 22:08:34},
    priority = {0},
    publisher = {SAGE Publications},
    title = {{A Comparative Analysis of Contemporary Achievement Goal Orientation Measures}},
    url = {http://dx.doi.org/10.1177/00131640121971626},
    volume = {61},
    year = {2001}
}

@article{Button1996Goal,
    abstract = {{Although some have argued that goal orientation could be beneficially integrated into organizational research, progress in this area has been impeded by several problematic conceptual issues and a lack of validated dispositional measures. This research was intended to address these issues and to provide a foundation for future organizational research in this area. We argue that goal orientation is a two-dimensional construct that has both dispostional and situational components. In each of four independent studies, LISREL VIII confirmatory factor analyses (J\"{o}reskog \& S\"{o}rbom, 1993) illustrated that a two-factor model fit a set of goal orientation items better than a single-factor model. In addition, the latent goal orientations were found to be uncorrelated in each study. Moreover, correlational analyses indicated that demographic and substantive variables exhibited differential relationships with the latent learning goal and performance goal orientation constructs. Other analyses illustrated that the dispositional and situational aspects of goal orientation are distinguishable. Collectively, the results provided ample support for the convergent and discriminant validity of eight-item measures of each goal orientation and help to define the nomological network within which the two goal orientations reside. The importance of goal orientation as a multidimensional construct is discussed and several recommendations for further research are suggested.}},
    author = {Button, Scott B. and Mathieu, John E. and Zajac, Dennis M.},
    citeulike-article-id = {9944204},
    citeulike-linkout-0 = {http://dx.doi.org/10.1006/obhd.1996.0063},
    comment = {(private-note)This article provides a survey instrument for learning goal orientation and performance goal orientation evaluation of individuals.  The survey in validated in 5 studies conducted primarially with students.  The paper says that both performance and learning goal orientation are related to academic achievement.  It theorizes that students benefit from both.  This instrument may or may not help us in our evaluation of whether achievement motivation exists in our developers. Plus there are 20 questions.},
    doi = {10.1006/obhd.1996.0063},
    issn = {07495978},
    journal = {Organizational Behavior and Human Decision Processes},
    keywords = {motivation},
    month = jul,
    number = {1},
    pages = {26--48},
    posted-at = {2012-11-28 21:27:57},
    priority = {0},
    title = {{Goal Orientation in Organizational Research: A Conceptual and Empirical Foundation}},
    url = {http://dx.doi.org/10.1006/obhd.1996.0063},
    volume = {67},
    year = {1996}
}

@article{Vansteenkiste2008Does,
    abstract = {{Previous work within self-determination theory has shown that experimentally framing a learning activity
in terms of extrinsic rather than intrinsic goals results in poorer conceptual learning and performance,
presumably because extrinsic goal framing detracts attention from the learning activity and is less directly
satisfying of basic psychological needs. According to the match perspective, experimental extrinsic,
compared to intrinsic, goal framing should enhance learning and performance for learners who personally
hold a stronger extrinsic than intrinsic goal orientation, as these learners' personally held goals match
with the situationally induced goals. An experimental field study among 5th–6th grade children shows
that extrinsic goal framing resulted in poorer autonomous motivation, conceptual (but not rote) learning,
and persistence compared to intrinsic goal framing, irrespective of participants' personal intrinsic versus
extrinsic goal orientations and their spontaneous perception of the learning activity as serving an intrinsic
or an extrinsic goal. The authors conclude that teachers can best promote intrinsic goals, even when
facing students who personally hold a stronger extrinsic than intrinsic goal orientation.}},
    author = {Vansteenkiste, Maarten and Soenens, Bart and Tinneke Timmermans and Willy Lens and Van den Broeck, Anja},
    citeulike-article-id = {11631119},
    comment = {(private-note)Vansteenkiste et al. discuss how intrinsic vs. extrinsic goal setting for a learning activity affects conceptual learning in 5th-6th grade students \cite{wbsnipes:Vansteenkiste2008Does}.  The study finds that motivating a goal in terms of intrinsic outcomes always improves performance whether the student is predisposed to extrinsic or intrinsic motivation.  Using extrinsic goals distracts learners because their self-worth becomes linked to the activity which changes the focus from learning.  Regardless of whether the learners perceived the subject and learning activity as intrinsic or extrinsic, they learned more when given intrinsic goals vs. extrinsic goals for the activity.   The intrinsic goal must also be realistically connected to the learning activity to have the positive effect on learning.  For example to motivate someone to learn about metrics we might say that measuring software development projects helps people meet their objectives and makes the project or program more successful.},
    journal = {Journal of Educational Psychology},
    keywords = {motivation},
    number = {2},
    pages = {387--397},
    posted-at = {2012-11-07 21:58:05},
    priority = {0},
    title = {{Does Extrinsic Goal Framing Enhance Extrinsic Goal-Oriented Individuals'
Learning and Performance? An Experimental Test of the Match
Perspective Versus Self-Determination Theory}},
    volume = {100},
    year = {2008}
}

@article{Carlos2007Requirements,
    abstract = {{Several business areas, like Management and Negotiation, have used games like a didactic way to simulate 
world reality, to introduce students to the day-to-day generated problems, and to extract from the application 
of  games,  concrete  learning  for  theoretical  knowledge  securing.  In  this  paper  we  present   ” Requirements 
Game”, a practical way to simulate software development conditions in a competitive environment similar to 
real  life.  This  game  has  been  played  by  several  groups  of  students  from  Facultad  de  Minas,  Universidad 
Nacional de Colombia, and this paper summarizes the results from this experience.}},
    author = {Carlos and awad-Aubad, Gabriel},
    citeulike-article-id = {11630972},
    journal = {CLEI ELECTRONIC JOURNAL},
    keywords = {gameification},
    month = jun,
    number = {1},
    posted-at = {2012-11-07 21:29:54},
    priority = {0},
    title = {{Requirements Game: Teaching Software Project Management}},
    volume = {10},
    year = {2007}
}

@inproceedings{Sillitti2003Collecting,
    abstract = {{Measures represent important data in all engineering disciplines. This data allows engineers to understand how things work and how to make changes to produce desired results. In software engineering, it is difficult to collect useful measures because developers do not consider it an important activity, compared to coding. Moreover, manual collected data is often affected by errors, making it unusable. The shortage of automated tools for collecting and analyzing measures does not contribute to the evolution of software engineering. We present PROM (PRO Metrics), an automated tool for collecting and analyzing software metrics as well as personal software process (PSP) data. The tool uses an architecture based on plug-ins that automatically collects data from development tools.}},
    author = {Sillitti, A. and Janes, A. and Succi, G. and Vernazza, T.},
    booktitle = {Proceedings. 29th Euromicro Conference, 2003.},
    citeulike-article-id = {11538054},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/eurmic.2003.1231611},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1231611},
    doi = {10.1109/eurmic.2003.1231611},
    institution = {DIST, Genova Univ., Italy},
    isbn = {0-7695-1996-2},
    issn = {1089-6503},
    keywords = {gamification, ide, metrics},
    pages = {336--342},
    posted-at = {2012-11-07 21:23:31},
    priority = {2},
    publisher = {IEEE},
    title = {{Collecting, integrating and analyzing software metrics and personal software process data}},
    url = {http://dx.doi.org/10.1109/eurmic.2003.1231611},
    year = {2003}
}

@inproceedings{MurphyHill2012Improving,
    abstract = {{Software developers interact with the development environ-
ments they use by issuing commands that execute various
programming  tools,  from  source  code  formatters  to  build
tools.   However,  developers often only use a small subset
of the commands oﬀered by modern development environ-
ments, reducing their overall development ﬂuency.  In this
paper,  we use several existing command recommender al-
gorithms to suggest new commands to developers based on
their existing command usage history,  and also introduce
several  new  algorithms.   By  running  these  algorithms  on
data submitted by several thousand Eclipse users,  we de-
scribe two studies that explore the feasibility of automati-
cally recommending commands to software developers. The
results  suggest  that,  while  recommendation  is  more  diﬃ-
cult in development environments than in other domains,
it  is  still  feasible  to  automatically  recommend  commands
to developers based on their usage history, and that using
patterns of past discovery is a useful way to do so.}},
    author = {Murphy-Hill, Emerson and Jiresal, Rahul and Murphy, Gail C.},
    booktitle = {Foundations of Software Engineering},
    citeulike-article-id = {11630871},
    comment = {(private-note) They study a large usage history data set and apply several different algorithms to accurately suggest to users new commands in their environment to use.  Algorithms based on command history performed well particularly when synchronized chronologically with the recipient's usage history.  Novice users were better served by recommender algorithms that did not require a long history, where more expert users benefited from more sophisticated algorithms that included more history.  The "Most Widely Used" algorithm that recommends commands based on the collective usage profile of the team performed nearly as well as the more sophisticated algorithms.  The paper presents the methodology for making recommendations and the evaluation of the accuracy of the recommendations.  The advantage of this work is the exploration of different recommender algorithms and the evaluation with both automated historical command history and live developer reactions. },
    journal = {Foundations of Software Engineering},
    keywords = {gameification, ide, recommending},
    month = nov,
    posted-at = {2012-11-07 21:14:41},
    priority = {0},
    title = {{Improving Software Developers' Fluency by Recommending Development Environment Commands}},
    year = {2012}
}

@article{Harackiewicz1993Achievement,
    author = {Harackiewicz, Judith M. and Elliot, Andres J.},
    citeulike-article-id = {11630774},
    journal = {Jounral of Parsonality and Social Psychology},
    keywords = {motivation},
    number = {5},
    organization = {American Psychological Association, Inc.},
    pages = {905--915},
    posted-at = {2012-11-07 20:59:55},
    priority = {2},
    title = {{Achievement Goals and Intrinsic Motivation}},
    volume = {65},
    year = {1993}
}

@inproceedings{Antin2011Badges,
    author = {Antin, Judd and Churchill, Elizabeth F.},
    citeulike-article-id = {11628181},
    comment = {(private-note)In Badges in Social Media \cite{wbsnipes:Antin2011Badges} they discuss five primary functions for badges in a social environment.  Goal setting identifies to users the path towards mastery of different aspects of the environment and walks them along this path step by step.  Instruction: intends to help broaden user experiences and knowledge into areas beyond their typical usage profile.  Reputation: provides a snapshot of the user's experience with the system in a way that peers can understand their degree of expertise.  Status/Affirmation: The concept of reputation can motivate users through the self affirmation and status implied with the achievement. Group Identification: Badges increase the shared experiences of the group and strengthen the team identification of group members.  The paper is a brief concept intro to how badges are applied and perceived by end users.  The five categories are good areas to think about and even test when building an achievement recognition system.  The brevity leaves unanswered questions for the reader on design of a recognition system for positive effect that may require further research.},
    day = {7-12},
    journal = {CHI},
    keywords = {gameification},
    month = may,
    posted-at = {2012-11-07 14:42:06},
    priority = {0},
    publisher = {ACM},
    title = {{Badges in Social Media: A Social Psychological Perspective}},
    year = {2011}
}

@article{Smith2005Online,
    abstract = {{This research examines the influence of recommendations on consumer decision making during online shopping experiences. Evidence from two empirical studies suggests that many online consumers seek and accept recommendations in order to effectively manage the amount of information available during online search processes. These findings suggest that consumers use the mere availability of peer recommendations as a decision-making heuristic, irrespective of the peer recommender's personal characteristics. Findings also suggest that consumer preference for peer versus editorial recommendations depends on the specific nature of the consumer's shopping goal: utilitarian or hedonic. Finally, results from this study indicate that consumers prefer peer and editorial recommendations over other types of effort-reducing cues that might be available during online search. As such, retailers must consider a number of factors including recommender characteristics, shopping goals, and product characteristics in their bid to provide consumers with the appropriate type of recommendation for their respective decision-making task.}},
    address = {North Central College, Naperville, IL; Millward Brown, Naperville, IL; Lehigh University, Bethlehem, PA},
    author = {Smith, Donnavieve and Menon, Satya and Sivakumar, K.},
    citeulike-article-id = {750048},
    citeulike-linkout-0 = {http://dx.doi.org/10.1002/dir.20041},
    citeulike-linkout-1 = {http://www3.interscience.wiley.com/cgi-bin/abstract/110557813/ABSTRACT},
    comment = {(private-note)Peer recommendations in online shopping experiences are important factors in consumers choices.  The existence of recommendations is most important, however, the expertise of the authority is important when making decisions on utilitarian purchases.  The ability of the shopper to relate to the recommender is also important.  Most related to online marketing sites, but observations may hold true for software tools as well.},
    doi = {10.1002/dir.20041},
    issn = {10949968},
    journal = {Journal of Interactive Marketing},
    keywords = {gameification},
    month = jan,
    number = {3},
    pages = {15--37},
    posted-at = {2012-10-31 16:55:46},
    priority = {0},
    title = {{Online peer and editorial recommendations, trust, and choice in virtual markets}},
    url = {http://dx.doi.org/10.1002/dir.20041},
    volume = {19},
    year = {2005}
}

@book{prowell1999cleanroom,
    address = {Reading, Mass.},
    author = {Prowell, S. J.},
    citeulike-article-id = {11552873},
    keywords = {markov},
    posted-at = {2012-10-26 20:06:02},
    priority = {0},
    publisher = {Addison-Wesley Professional},
    title = {{Cleanroom software engineering: technology and process}},
    year = {1999}
}

@inproceedings{Robbes2007Characterizing,
    abstract = {{The understanding of development sessions, the phases during which a developer actively modifies a software system, is a valuable asset for program comprehension, since the sessions directly impact the current state and future evolution of a software system. Such information is usually lost by state-of-the-art versioning systems, because of the checkin/checkout model they rely on: a developer must explicitly commit his changes to the repository. Since this happens in arbitrary and sometimes long intervals, recovering the changes between two commits is difficult and inaccurate, and recovering the order of the changes is impossible. We have implemented an evolution monitoring prototype which records every semantic change performed on a system, and is able to completely reconstruct development sessions. In this paper we use this fine-grained information to understand and characterize the development sessions as they were carried out on two object-oriented systems.}},
    address = {Washington, DC, USA},
    author = {Robbes, R. and Lanza, M.},
    booktitle = {Program Comprehension, 2007. ICPC \&\#039;07. 15th IEEE International Conference on},
    citeulike-article-id = {11342587},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1270394.1271356},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/icpc.2007.12},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4268250},
    comment = {(private-note)Analyzes changes between commits classifying them into different types of changes.},
    doi = {10.1109/icpc.2007.12},
    institution = {Fac. of Inf., Univ. of Lugano, Lugano},
    isbn = {0-7695-2860-0},
    issn = {1092-8138},
    keywords = {gameification},
    month = jun,
    pages = {155--166},
    posted-at = {2012-10-26 00:39:45},
    priority = {0},
    publisher = {IEEE},
    series = {ICPC '07},
    title = {{Characterizing and Understanding Development Sessions}},
    url = {http://dx.doi.org/10.1109/icpc.2007.12},
    year = {2007}
}

@inproceedings{Prowell2003JUMBL,
    abstract = {{Statistical testing of software based on a usage model is a cost-effective and efficient means to make inferences about software quality. In order to apply this method, a usage model is developed and analyzed to validate its fitness for use in testing. The model may then be used to generate test cases representing expected usage, and to reason about system reliability given the performance on the set of tests. The J Usage Model Builder Library (JUMBL) is a Java class library and set of command-line tools for working with usage models. The JUMBL supports construction and analysis of models, generation of test cases, automated-execution of tests, and analysis of testing results.}},
    author = {Prowell, S. J.},
    booktitle = {Proceedings of the 36th Annual Hawaii International Conference on System Sciences, 2003.},
    citeulike-article-id = {11548355},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/hicss.2003.1174916},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1174916},
    doi = {10.1109/hicss.2003.1174916},
    institution = {Tennessee Univ., Knoxville, TN, USA},
    isbn = {0-7695-1874-5},
    keywords = {gameification},
    month = jan,
    pages = {6--9},
    posted-at = {2012-10-26 00:01:14},
    priority = {0},
    publisher = {IEEE},
    title = {{JUMBL: a tool for model-based statistical testing}},
    url = {http://dx.doi.org/10.1109/hicss.2003.1174916},
    volume = {9},
    year = {2003}
}

@inproceedings{Johnson2007Automated,
    abstract = {{Zorro is a system designed to automatically determine whether a developer is complying with an operational definition of test-driven development (TDD) practices. Automated recognition of TDD can benefit the software development community in a variety of ways, from inquiry into the "true nature" of TDD, to pedagogical aids to support the practice of test-driven development, to support for more rigorous empirical studies on the effectiveness of TDD in both laboratory and real world settings. This paper introduces the Zorro system, its operational definition of TDD, the analyses made possible by Zorro, and our ongoing efforts to validate the system.}},
    author = {Johnson, P. M. and Kou, Hongbing},
    booktitle = {Agile Conference (AGILE)},
    citeulike-article-id = {11538873},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/agile.2007.16},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4293572},
    doi = {10.1109/agile.2007.16},
    institution = {Univ. of Hawaii, Honolulu},
    isbn = {0-7695-2872-4},
    keywords = {gameification},
    month = aug,
    pages = {15--25},
    posted-at = {2012-10-24 17:52:16},
    priority = {2},
    publisher = {IEEE},
    title = {{Automated Recognition of Test-Driven Development with Zorro}},
    url = {http://dx.doi.org/10.1109/agile.2007.16},
    year = {2007}
}

@inproceedings{Roehm2012Automatically,
    abstract = {{Detecting the current activity of developers and problems they are facing is a prerequisite for a context-aware assistance and for capturing developers\^{a}\^{A}\^{A} experiences during their work. We present an approach to detect the current activity of software developers and if they are facing a problem. By observing developer actions like changing code or searching the web, we detect whether developers are locating the cause of a problem, searching for a solution, or applying a solution. We model development work as recurring problem solution cycle, detect developer\^{a}\^{A}\^{A}s actions by instrumenting the IDE, translate developer actions to observations using ontologies, and infer developer activities by using Hidden Markov Models. In a preliminary evaluation, our approach was able to correctly detect 72\% of all activities. However, a broader more reliable evaluation is still needed.}},
    address = {Piscataway, NJ, USA},
    author = {Roehm, Tobias and Maalej, Walid},
    booktitle = {Proceedings of the 2012 International Conference on Software Engineering},
    citeulike-article-id = {11531000},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=2337390},
    comment = {(private-note)The paper uses automated capture of developer activities and hidden markov models to determine whether the developer is reproducing a bug, performing changes or testing a fix.  The hidden markov models provide the ability to discern when a developer switches from one problem context to another say for an interruption.  They conclude that it is possible to automatically determine what the developer is doing in these key areas.  The difference with our work is we are evaluating actions within the IDE to indicate how to improve and have drawn links between specific work patterns and tools that would make the developer more effective.},
    isbn = {978-1-4673-1067-3},
    keywords = {gameification},
    location = {Zurich, Switzerland},
    pages = {1261--1264},
    posted-at = {2012-10-23 15:02:59},
    priority = {0},
    publisher = {IEEE Press},
    series = {ICSE 2012},
    title = {{Automatically detecting developer activities and problems in software development work}},
    url = {http://portal.acm.org/citation.cfm?id=2337390},
    year = {2012}
}

@article{Robillard2004How,
    abstract = {{Prior to performing a software change task, developers must discover and understand the subset of the system relevant to the task. Since the behavior exhibited by individual developers when investigating a software system is influenced by intuition, experience, and skill, there is often significant variability in developer effectiveness. To understand the factors that contribute to effective program investigation behavior, we conducted a study of five developers performing a change task on a medium-size open source system. We isolated the factors related to effective program investigation behavior by performing a detailed qualitative analysis of the program investigation behavior of successful and unsuccessful developers. We report on these factors as a set of detailed observations, such as evidence of the phenomenon of inattention blindness by developers skimming source code. In general, our results support the intuitive notion that a methodical and structured approach to program investigation is the most effective.}},
    address = {Piscataway, NJ, USA},
    author = {Robillard, M. P. and Coelho, W. and Murphy, G. C.},
    citeulike-article-id = {2673204},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1042417},
    citeulike-linkout-1 = {http://doi.ieeecomputersociety.org/10.1109/TSE.2004.101},
    citeulike-linkout-2 = {http://dx.doi.org/10.1109/tse.2004.101},
    citeulike-linkout-3 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1377187},
    comment = {(private-note)explores hypotheses around how developers can be more effective at performing a maintenance task.  Conclusions are that developers can be more effective using a methodological and structured approach relying on the structure of the program than approaches relying upon naming conventions and browsing the code.  Key points proven is that developers who understand the code and create a detailed plan for implementing the change are faster and more succesful.  Developers who use structured navigation search through dependencies and relationships and keyword searchs in the code are more successful at finding the method containing the bug than developers who scan the code looking for methods or other elements.   One factor may be a psychological effect known as inattention blindness where people not searching for something will miss the occurrence.  "e.g. gorilla on basketball court video".  Developers who did not reinvestigate methods frequently were more successful.  
---=note-separator=---
Navigation patterns/techniques:
Scrolling (find method via scrolling through source file)
Browsing (select it in a code browser)
Cross-reference (follow a cross-reference in the code)
Recall (by switching windows)
Keyword search (using search to find the method)
Note taking - recording information about methods visited
Planning - planning in detail a change to implement the functionality
Reinvestigation - number of methods visited between visits to a specific method (known to be important).  },
    doi = {10.1109/tse.2004.101},
    institution = {Sch. of Comput. Sci., McGill Univ., Montreal, Que., Canada},
    issn = {0098-5589},
    journal = {IEEE Transactions on Software Engineering},
    keywords = {gameification, productivity, recommending},
    month = dec,
    number = {12},
    pages = {889--903},
    posted-at = {2012-10-22 18:19:43},
    priority = {0},
    publisher = {IEEE Press},
    title = {{How Effective Developers Investigate Source Code: An Exploratory Study}},
    url = {http://dx.doi.org/10.1109/tse.2004.101},
    volume = {30},
    year = {2004}
}

@electronic{PinkDrive,
    author = {Pink, Dan},
    citeulike-article-id = {11469422},
    citeulike-linkout-0 = {http://www.youtube.com/watch?v=X\_mMxhvzVmQ},
    journal = {RSA},
    keywords = {motivation},
    posted-at = {2012-10-15 18:47:19},
    priority = {2},
    title = {{Drive, The surprising truth about what motivates us}},
    url = {http://www.youtube.com/watch?v=X\_mMxhvzVmQ}
}

@electronic{Bartle1996Hearts,
    author = {Bartle, Richard},
    citeulike-article-id = {11469336},
    citeulike-linkout-0 = {http://www.mud.co.uk/richard/hcds.htm},
    citeulike-linkout-1 = {http://playertypes.org/research/?q=node/2},
    journal = {Journal of MUD research},
    keywords = {gameification},
    posted-at = {2012-10-15 18:32:38},
    priority = {0},
    title = {{Hearts, clubs, diamonds, spades: Players who suit MUDs}},
    url = {http://www.mud.co.uk/richard/hcds.htm},
    year = {1996}
}

@electronic{Chatfield20107,
    author = {Chatfield, Tom},
    citeulike-article-id = {11469060},
    citeulike-linkout-0 = {http://www.ted.com/talks/tom\_chatfield\_7\_ways\_games\_reward\_the\_brain.html},
    journal = {TED},
    keywords = {gameification, motivation},
    month = nov,
    posted-at = {2012-10-15 17:51:01},
    priority = {2},
    title = {{7 Ways Games Reward the Brain}},
    url = {http://www.ted.com/talks/tom\_chatfield\_7\_ways\_games\_reward\_the\_brain.html},
    year = {2010}
}

@electronic{SaatchiandSaatchi2011Gameification,
    author = {Saatchi and Saatchi, S.},
    citeulike-article-id = {11469041},
    citeulike-linkout-0 = {http://www.slideshare.net/Saatchi\_S/gamification-study},
    comment = {(private-note)A positive indicator is that 55\% would be interested in working for a company that offers games as a way to increase productivity.  A work setting could ramp up the competitive motivation that is fairly low 31\% compared with the findings that 57\% use social games for  'passing time when bored'.

It is also interesting that 85\% of people would spend a half hour or more playing a game for a chance at a monetary award.  This counters some of the motivation literature I've studied who say that monetary awards are much lower on the motivation scale than recognition. 
},
    journal = {http://www.slideshare.net/Saatchi\_S/gamification-study},
    keywords = {gameification},
    month = jun,
    posted-at = {2012-10-15 17:48:20},
    priority = {0},
    title = {{Gameification for Business Brands and Loyalty}},
    url = {http://www.slideshare.net/Saatchi\_S/gamification-study},
    year = {2011}
}

@inproceedings{Antin2011Badges,
    abstract = {{Representing achievements as badges or trophies is a standard practice in online gaming. Awarding badges has also become a key ingredient in  ” gamifying” online social media experiences. Social systems such as Foursquare, StackOverflow, and Wikipedia have popularized badges as a way of engaging and motivating users. In this paper we deconstruct badges and present five social psychological functions for badges in social media contexts: goal setting, instruction, reputation, status/affirmation, and group identification. We argue that future research should further explore these five functions and their application in specific contexts.}},
    author = {Antin, Judd and Churchill, Elizabeth F.},
    citeulike-article-id = {11425302},
    day = {7-12},
    journal = {Gamification Workshop, CHI2011},
    keywords = {game-theory},
    location = {Vancouver, B.C. Canada},
    month = may,
    posted-at = {2012-10-09 21:53:03},
    priority = {2},
    title = {{Badges in Social Media: A Social Psychological Perspective}},
    year = {2011}
}

@inproceedings{Hamari2011Framework,
    author = {Hamari, Juho and Eranti, Veikko},
    booktitle = {Digra Conference},
    citeulike-article-id = {11424566},
    day = {14-17},
    journal = {Proceedings of Digra Conference: Think Design Play},
    keywords = {gameification},
    month = sep,
    posted-at = {2012-10-09 20:18:16},
    priority = {0},
    title = {{Framework for Designing and Evaluating Game Achievements}},
    year = {2011}
}

@article{Hermans1970Questionnaire,
    abstract = {{10.1037/h0029675}},
    author = {Hermans, Hubert J.},
    citeulike-article-id = {11420473},
    comment = {(private-note)Hermans describes results from applying questionnaire to evaluate achievement motivation in students \cite{wbsnipes:Hermans1970Questionnaire}.  Each question is ranked for its correlation to achievement motivation. Results indicate that some key questions about the diligence with which students approach their work are correlated with achievement motivation.  Questions that are most correlated are listed in the Pre-Study Questionnaire below for potential use in a developer survey.},
    journal = {Journal of Applied Psychology},
    keywords = {motivation},
    month = aug,
    number = {4},
    pages = {353--363},
    posted-at = {2012-10-08 19:08:05},
    priority = {0},
    title = {{A questionnaire measure of achievement motivation.}},
    volume = {54},
    year = {1970}
}

@book{MJOsborneCourse,
    author = {MJ Osborne, A. Rubinstein},
    citeulike-article-id = {11329456},
    keywords = {game-theory},
    posted-at = {2012-09-26 21:40:32},
    priority = {2},
    title = {{A course in game theory}}
}

@book{NisanAlgorithmic,
    author = {Nisan},
    citeulike-article-id = {11329455},
    keywords = {game-theory},
    posted-at = {2012-09-26 21:39:38},
    priority = {2},
    title = {{Algorithmic Game Theory}}
}

@book{PrenskyDigital,
    author = {Prensky, Marc},
    citeulike-article-id = {11329022},
    comment = {(private-note)Describes the problem with learners of the new generation who are more engaged by computer games than classroom learning.  The proposed solution is to turn learning into a game.  Weave an engaging story into the lesson so that students are eager to get to the next level of the story as they go through the actual lesson.  Describes training for a 3-D CAD system that completely turns the training in to a story backed game.},
    keywords = {game-theory},
    posted-at = {2012-09-26 21:11:03},
    priority = {0},
    title = {{The digital game-based learning revolution}}
}

@article{Amory1999Use,
    abstract = {{Playing games is an important part of our social and mental development. This research was initiated to identify the game type most suitable to our teaching environment and to identify game elements that students found interesting or useful within the different game types. A group of twenty students played four commercial games (SimIsle, Red Alert, Zork Nemesis and Duke Nukem 3D). Results suggest that students prefer 3D-adventure (Zork Nemesis) and strategy (Red Alert) games to the other types ( ” shoot-em-up”, simulation) with Zork Nemesis ranked as the best. Students rated game elements such as logic, memory, visualisation and problem solving as the most important game elements. Such elements are integral to adventure games and are also required during the learning process. We present a model that links pedagogical issues with game elements. The game space contains a number of components, each encapsulates specific abstract or concrete interfaces. Understanding the relationship between educational needs and game elements will allow us to develop educational games that include visualisation and problem solving skills. Such tools could provide sufficient stimulation to engage learners in knowledge discovery, while at the same time developing new skills.}},
    author = {Amory, Alan and Naicker, Kevin and Vincent, Jacky and Adams, Claudia},
    citeulike-article-id = {2208020},
    citeulike-linkout-0 = {http://www.blackwell-synergy.com/doi/abs/10.1111/1467-8535.00121},
    citeulike-linkout-1 = {http://dx.doi.org/10.1111/1467-8535.00121},
    doi = {10.1111/1467-8535.00121},
    journal = {British Journal of Educational Technology},
    keywords = {motivation},
    number = {4},
    pages = {311--321},
    posted-at = {2012-09-26 21:05:10},
    priority = {0},
    publisher = {Blackwell Publishers Ltd},
    title = {{The use of computer games as an educational tool: identification of appropriate game types and game elements}},
    url = {http://dx.doi.org/10.1111/1467-8535.00121},
    volume = {30},
    year = {1999}
}

@article{Eskelinen2001Towards,
    abstract = {{It is relatively stress-free to write about computer games as nothing too much has been said yet, and almost anything goes. The situation is pretty much the same when it comes to writing about games and gaming in general. The sad fact with alarming cumulative consequences is that they are under-theorized; there are Huizinga, Caillois and Ehrmann of course, and libraries full of board game studies,in addition to game theory and bits and pieces of philosophy ? most notably those of Wittgenstein ? but they won't get us very far with computer games. So if there already is or soon will be a legitimate field for computer game studies, this field is also very open to intrusions and colonisations from the already organized scholarly tribes. Resisting and beating them is the goal of our first survival game in this paper, as what these emerging studies need is independence, or at least relative independence.}},
    author = {Eskelinen, Markku},
    citeulike-article-id = {11329013},
    citeulike-linkout-0 = {http://dx.doi.org/10.1076/digc.12.3.175.3232},
    citeulike-linkout-1 = {http://www.tandfonline.com/doi/abs/10.1076/digc.12.3.175.3232},
    day = {1},
    doi = {10.1076/digc.12.3.175.3232},
    journal = {Digital Creativity},
    keywords = {game-theory},
    month = sep,
    number = {3},
    pages = {175--183},
    posted-at = {2012-09-26 20:59:22},
    priority = {2},
    publisher = {Routledge},
    title = {{Towards computer game studies}},
    url = {http://dx.doi.org/10.1076/digc.12.3.175.3232},
    volume = {12},
    year = {2001}
}

@article{Amory2001Building,
    abstract = {{Creation of complex educational software involves many differ-ent activities that include the use of appropriate educational ped-agogy, development of resources, software creation and author-ing. Some authors have argued that play is part of the learning process and that simulation and adventure games could be used as viable educational tools. This article presents a model for the development of an "edventure" (educational adventure game) game based on educational theory. It highlights the lessons learned in the development of Zadarh, a game developed to ad-dress misconceptions held by biology students. The intent is to present information that could foster discussion and a greater un-derstanding of the processes involved in the construction of complex interactive learning tools.}},
    address = {Norfolk, VA},
    author = {Amory, Alan},
    citeulike-article-id = {2889937},
    citeulike-linkout-0 = {http://editlib.org/p/8421},
    issn = {1093-023X},
    journal = {Journal of Interactive Learning Research},
    keywords = {motivation},
    number = {2},
    pages = {249--263},
    posted-at = {2012-09-26 20:57:04},
    priority = {2},
    publisher = {AACE},
    title = {{Building an Educational Adventure Game: Theory, Design, and Lessons}},
    url = {http://editlib.org/p/8421},
    volume = {12},
    year = {2001}
}

@article{Malone1981What,
    abstract = {{Two questions are examined: (1) What are the features that make computer games so captivating; and (2) How can these features be used to make learning, especially learning with computers, interesting? (MP)}},
    author = {Malone, Thomas W.},
    citeulike-article-id = {11329001},
    citeulike-linkout-0 = {http://www.eric.ed.gov/ERICWebPortal/detail?accno=EJ255512},
    day = {0},
    journal = {Pipeline},
    keywords = {motivation},
    month = {},
    number = {2},
    pages = {50--51,49},
    posted-at = {2012-09-26 20:54:41},
    priority = {4},
    title = {{What Makes Things Fun to Learn? A Study of Intrinsically Motivating Computer Games.}},
    url = {http://www.eric.ed.gov/ERICWebPortal/detail?accno=EJ255512},
    volume = {6},
    year = {1981}
}

@article{MaehrCulture,
    author = {Maehr, M.},
    citeulike-article-id = {11328991},
    comment = {(private-note)Achievement motivation is typically evaluated in the context of structured mesurable environments with a challenging task and the chance of success of failure.  E.g. classroom.  However, individuals without the classroom background exhibit achievement motivation in the context of what they do.  The theory is that individuals achieve as a member of a social group choosing the behavior that will meet the expectations and values of the group that is significant to them.  So the presence of a social group where the person is motivated to belong and excel within is important for creating achievement motivation.  It was found by Zander and Forward that individuals placed in a leadership role who previously had low achievement motivation then exhibited behaviors of high achievement motivation.  The author states that since these behaviors can be triggered by circumstance, that they are normally developd in most people.  They further state that "achievement motivation is universal, perhaps also evenly distributed, in all people and that it only needs establishling of the right cues and context to elicit these behaviors".  Locus of control dimension: "DeCharms (1968, 1972) has shown that those contextual conditions that communicate to persons that they are "origins" rather than "pawns" will increase their achievement motivation." Interpersonal dimension: indiviudals exhibit achievement motivation within the context of their social group which can be influenced by direct feedback from a significant other in that group.  Task dimensions: define the rules of the achivement and the scale by which achivement may be rewarded.  Overall the artical offers a critique of how achievement motivation should be evaluated in different cultural settings.  It does this from the point of view that achivement motivation may be universal when evaluated with the correct tasks and social settings.},
    howpublished = {PsycARTICLES, Ipswich, MA.},
    journal = {American Psychologist},
    keywords = {motivation},
    number = {29},
    pages = {887--896},
    posted-at = {2012-09-26 20:38:04},
    priority = {0},
    title = {{Culture and achievement motivation.}},
    volume = {1974}
}

@article{Smith1975Achievement,
    abstract = {{This  study  investigated  the  achievement  motivation  training  component of  psychological  education.  The  subjects  were  54 late-adolescent  pupils. The  experimental  subjects  were  stratified  and  randomly  placed  into  the control and experimental groups. In addition to a reduction of test anxiety, the  experimental  training  program  had  as  its  objectives  an  increase  in
academic  achievement  motivation,  internal  feelings  of control, and  school performance.  Results   of  the  study  indicated  that  significant  differences existed  between the experimental  and control  groups in achievement  motivation  and  internal  feelings  of control.}},
    author = {Smith, Robert L. and Troth, William A.},
    citeulike-article-id = {11328979},
    citeulike-linkout-0 = {http://ehis.ebscohost.com.prox.lib.ncsu.edu/ehost/detail?sid=2a1cd956-a725-4ad5-b157-6f2b674d1b89\%40sessionmgr112\&\#38;vid=1\&\#38;hid=103\&\#38;bdata=JnNpdGU9ZWhvc3QtbGl2ZSZzY29wZT1zaXRl\#db=pdh\&\#38;AN=cou-22-6-500},
    journal = {Journal of Counseling Psychology},
    keywords = {motivation},
    number = {6},
    pages = {500--504},
    posted-at = {2012-09-26 20:27:51},
    priority = {0},
    title = {{Achievement motivation: A rational approach to psychological education}},
    url = {http://ehis.ebscohost.com.prox.lib.ncsu.edu/ehost/detail?sid=2a1cd956-a725-4ad5-b157-6f2b674d1b89\%40sessionmgr112\&\#38;vid=1\&\#38;hid=103\&\#38;bdata=JnNpdGU9ZWhvc3QtbGl2ZSZzY29wZT1zaXRl\#db=pdh\&\#38;AN=cou-22-6-500},
    volume = {22},
    year = {1975}
}

@electronic{Examining,
    citeulike-article-id = {11288850},
    citeulike-linkout-0 = {http://www.irrodl.org/index.php/irrodl/article/view/1030/1954},
    keywords = {motivation},
    posted-at = {2012-09-21 22:08:30},
    priority = {2},
    title = {{Examining Motivation in Online Distance Learning Environments: Complex, Multifaceted, and Situation-Dependent}},
    url = {http://www.irrodl.org/index.php/irrodl/article/view/1030/1954}
}

@inproceedings{Maldonado2009Elearning,
    abstract = {{We argue that e-learning motivation can play important rule in technology adoption and use and conversely technology use can foster student's e-learning motivation. In addition, e-learning motivation has been mainly considered same as conventional learning motivation. But we propose that e-learning motivation is different than conventional learning. For e-learning motivation technology characteristic such effort expectancy must be incorporated. Moreover, there is no empirical proof of the role of students' e-learning motivation on use and adoption of the e-learning system and vice versa in South American contexts. Thus the purpose of this research is to (1) empirically validate a modified UTAUT model, by adding \^{A}¿e-learning motivation\^{A}¿ construct in a South American context, especially in the case of Peru (2). Try to figure out the role that e-learning motivation can play in the use and adoption of e-learning system vice versa. (3) In addition, we are considering \^{A}¿region\^{A}¿ and \^{A}¿gender\^{A}¿ as moderator in our model. Based on the findings we produced useful recommendations to policy makers and practitioners particularly in Peru and developing countries in general.}},
    author = {Maldonado, U. P. T. and Khan, G. F. and Moon, Junghoon and Rho, Jae J.},
    booktitle = {Computer Sciences and Convergence Information Technology, 2009. ICCIT \&\#039;09. Fourth International Conference on},
    citeulike-article-id = {11288848},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/iccit.2009.77},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5370335},
    doi = {10.1109/iccit.2009.77},
    institution = {Sch. of IT Bus., Korea Adv. Inst. of Sci. \& Technol.(KAIST ), Daejeon, South Korea},
    isbn = {978-1-4244-5244-6},
    keywords = {motivation},
    month = nov,
    pages = {1431--1441},
    posted-at = {2012-09-21 22:06:25},
    priority = {0},
    publisher = {IEEE},
    title = {{E-learning motivation, Students' Acceptance/Use of Educational Portal in Developing Countries: A Case Study of Peru}},
    url = {http://dx.doi.org/10.1109/iccit.2009.77},
    year = {2009}
}

@electronic{MaclellanSignificance,
    abstract = {{The theoretical underpinnings of student-centred learning suggest motivation to be an integral component. However, lack of clarification of what is involved in motivation in education often results in unchallenged assumptions that fail to recognise that what motivates some students may alienate others. This case study, using socio-cognitive motivational theory to analyse previously collected data, derives three fuzzy propositions which, collectively, suggest that motivation interacts with the whole cycle of episodes in the teachinglearning process. It argues that the development of the higherlevel cognitive competencies that are implied by the term, student-centred learning, must integrate motivational constructs such as goal orientation, volition, interest and attributions into pedagogical practices.}},
    author = {Maclellan, Effie},
    citeulike-article-id = {11288827},
    comment = {(private-note)This article finds that achievement motivation can be increased through training in students.  However, that did not translate to reduced test anxiety or grade performance in the classroom.  The students did develop self confidence and "internal control".},
    issn = {1356-2517},
    keywords = {motivation},
    posted-at = {2012-09-21 21:30:56},
    priority = {4},
    title = {{The significance of motivation in student-centred learning: a reflective case study}}
}

@article{Pan2009Toward,
    abstract = {{Twenty-seven automatically extractable bug fix patterns are defined using the syntax components and context of the source code involved in bug fix changes. Bug fix patterns are extracted from the configuration management repositories of seven open source projects, all written in Java (Eclipse, Columba, JEdit, Scarab, ArgoUML, Lucene, and MegaMek). Defined bug fix patterns cover 45.7\% to 63.3\% of the total bug fix hunk pairs in these projects. The frequency of occurrence of each bug fix pattern is computed across all projects. The most common individual patterns are MC-DAP (method call with different actual parameter values) at 14.9–25.5\%, IF-CC (change in if conditional) at 5.6–18.6\%, and AS-CE (change of assignment expression) at 6.0–14.2\%. A correlation analysis on the extracted pattern instances on the seven projects shows that six have very similar bug fix pattern frequencies. Analysis of if conditional bug fix sub-patterns shows a trend towards increasing conditional complexity in if conditional fixes. Analysis of five developers in the Eclipse projects shows overall consistency with project-level bug fix pattern frequencies, as well as distinct variations among developers in their rates of producing various bug patterns. Overall, data in the paper suggest that developers have difficulty with specific code situations at surprisingly consistent rates. There appear to be broad mechanisms causing the injection of bugs that are largely independent of the type of software being produced.}},
    address = {Hingham, MA, USA},
    author = {Pan, Kai and Kim, Sunghun and Whitehead, E. James},
    citeulike-article-id = {3638662},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1553586.1553603},
    citeulike-linkout-1 = {http://dx.doi.org/10.1007/s10664-008-9077-5},
    citeulike-linkout-2 = {http://www.springerlink.com/content/c6p611q61580370x},
    day = {1},
    doi = {10.1007/s10664-008-9077-5},
    issn = {1382-3256},
    journal = {Empirical Software Engineering},
    keywords = {bugdifficulty},
    month = jun,
    number = {3},
    pages = {286--315},
    posted-at = {2012-08-27 13:56:24},
    priority = {2},
    publisher = {Springer Netherlands},
    title = {{Toward an understanding of bug fix patterns}},
    url = {http://dx.doi.org/10.1007/s10664-008-9077-5},
    volume = {14},
    year = {2009}
}

@inproceedings{Gokhale2008Software,
    abstract = {{We analyzed over 10,000 software defect repair times collected for nine products at Cisco Systems, to confirm our hypothesis that software defect repair times can be characterized by the Laplace Transform of the Lognormal (LTLN) distribution. This hypothesis originated from the observation that software defect repair times are influenced by the multiplicative interplay of several factors. The Lognormal distribution is a natural choice to model rates of occurrence of such phenomenon. Conversion of the Lognormal rate distribution to an occurrence time distribution yields the LTLN. Our results also confirm that the LTLN distribution provides a statistically better fit to the observed repair times than either of the two most widely used repair time distributions, the lognormal and the exponential.}},
    address = {New York, NY, USA},
    author = {Gokhale, Swapna S. and Mullen, Robert},
    booktitle = {Proceedings of the 4th international workshop on Predictor models in software engineering},
    citeulike-article-id = {11140400},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1370810},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1370788.1370810},
    doi = {10.1145/1370788.1370810},
    isbn = {978-1-60558-036-4},
    keywords = {bugdifficulty},
    location = {Leipzig, Germany},
    pages = {93--100},
    posted-at = {2012-08-27 13:23:42},
    priority = {2},
    publisher = {ACM},
    series = {PROMISE '08},
    title = {{Software defect repair times: a multiplicative model}},
    url = {http://dx.doi.org/10.1145/1370788.1370810},
    year = {2008}
}

@article{Katz1987Debugging,
    abstract = {{This article presents a series of four experiments investigating students' debugging of LISP programs. The experiments involve a population of students who know LISP reasonably well in that their errors are best classified as slips (Brown \& Van Lehn, 1980). That is, students are unlikely to repeat the same errors either within their program or across programs (Experiment 1). The students' understanding of LISP is also reflected in their debugging behavior: They can usually fix a bug once they locate it. Students' difficulties are in locating the erroneous line of code. We observe that students use a variety of bug-location strategies during debugging (Experiment 2) and that the choice of strategy differs depending on whether students are debugging their own programs or other students' programs (Experiment 3). In addition, we observe that although the different bug-location strategies affect which lines of a program are searched, once students decide on a line, their ability to judge whether or not the line is correct and their ability to correct an error are not substantially affected by the strategy used to locate the line (Experiment 4). Finally, we argue that our results have implications not only for debugging in other computer languages, but for the general processes involved in troubleshooting as well.}},
    author = {Katz, Irvin R. and Anderson, John R.},
    citeulike-article-id = {11140387},
    citeulike-linkout-0 = {http://dx.doi.org/10.1207/s15327051hci0304\_2},
    citeulike-linkout-1 = {http://www.tandfonline.com/doi/abs/10.1207/s15327051hci0304\_2},
    day = {1},
    doi = {10.1207/s15327051hci0304\_2},
    journal = {Human–Computer Interaction},
    keywords = {bugdifficulty},
    month = dec,
    number = {4},
    pages = {351--399},
    posted-at = {2012-08-27 13:19:55},
    priority = {2},
    publisher = {Taylor \& Francis},
    title = {{Debugging: An Analysis of Bug-Location Strategies}},
    url = {http://dx.doi.org/10.1207/s15327051hci0304\_2},
    volume = {3},
    year = {1987}
}

@article{YehudaKorenMATRIX,
    author = {Yehuda Koren, Robert B.},
    citeulike-article-id = {10925880},
    citeulike-linkout-0 = {http://www2.research.att.com/\~{}volinsky/papers/ieeecomputer.pdf},
    keywords = {devcoach},
    posted-at = {2012-07-24 20:02:41},
    priority = {3},
    title = {MATRIX
FACTORIZATION TECHNIQUES FOR RECOMMENDER
SYSTEMS},
    url = {http://www2.research.att.com/\~{}volinsky/papers/ieeecomputer.pdf}
}

@inproceedings{Hill2011Peer,
    abstract = {{Computer users rely on software tools to work effectively and efficiently, but it is difficult for users to be aware of all the tools that might be useful to them. While there are several potential technical solutions to this difficulty, we know little about social solutions, such as one user telling a peer about a tool. To explore these social solutions in one particular domain, we describe a series of interviews with 18 programmers in industry that explore how tool discovery takes place. These interviews provide a rich set of qualitative data that give us detailed insights into how programmers discover tools. One finding was that, while programmers believe that discovery from peers is effective, they actually discover tools from peers relatively infrequently. Another finding was that programmers can effectively discover tools from their peers both in a co-located and remote settings. We describe several implications of our findings, such as that discovery from peers can be enhanced by improving programmers' ability to communicate openly and concisely about tools.}},
    address = {New York, NY, USA},
    author = {Hill, Emerson M. and Murphy, Gail C.},
    booktitle = {Proceedings of the ACM Conference on Computer supported cooperative work},
    citeulike-article-id = {10925814},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1958888},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1958824.1958888},
    doi = {10.1145/1958824.1958888},
    isbn = {978-1-4503-0556-3},
    keywords = {devcoach},
    location = {Hangzhou, China},
    pages = {405--414},
    posted-at = {2012-07-24 18:46:58},
    priority = {4},
    publisher = {ACM},
    series = {CSCW},
    title = {{Peer interaction effectively, yet infrequently, enables programmers to discover new tools}},
    url = {http://dx.doi.org/10.1145/1958824.1958888},
    year = {2011}
}

@inproceedings{Hill2009How,
    abstract = {{Much of what we know about how programmers refactor in the wild is based on studies that examine just a few software projects. Researchers have rarely taken the time to replicate these studies in other contexts or to examine the assumptions on which they are based. To help put refactoring research on a sound scientific basis, we draw conclusions using four data sets spanning more than 13 000 developers, 240 000 tool-assisted refactorings, 2500 developer hours, and 3400 version control commits. Using these data, we cast doubt on several previously stated assumptions about how programmers refactor, while validating others. For example, we find that programmers frequently do not indicate refactoring activity in commit logs, which contradicts assumptions made by several previous researchers. In contrast, we were able to confirm the assumption that programmers do frequently intersperse refactoring with other program changes. By confirming assumptions and replicating studies made by other researchers, we can have greater confidence that those researchers' conclusions are generalizable.}},
    address = {Washington, DC, USA},
    author = {Hill, Emerson M. and Parnin, Chris and Black, Andrew P.},
    booktitle = {Proceedings of the 31st International Conference on Software Engineering},
    citeulike-article-id = {5400371},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1555044},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/icse.2009.5070529},
    comment = {This article performs a deep study of refactoring and finds that developers do floss refactoring frequently and usually while making other changes.  Root-canal refactoring where larger changes are made that are pure refactoring changes are infrequent.  },
    doi = {10.1109/icse.2009.5070529},
    isbn = {978-1-4244-3453-4},
    keywords = {chs-fse2012},
    pages = {287--297},
    posted-at = {2012-06-23 23:00:55},
    priority = {0},
    publisher = {IEEE Computer Society},
    series = {ICSE '09},
    title = {{How we refactor, and how we know it}},
    url = {http://dx.doi.org/10.1109/icse.2009.5070529},
    year = {2009}
}

@inproceedings{Mantyla2006Drivers,
    abstract = {{This paper presents an empirical study of drivers for software refactoring decisions. We studied the refactoring decisions made by 37 students evaluating ten methods of a purposefully constructed Java program. The decision rationales reported by the evaluators were coded to identify the drivers behind the decisions. The identified drivers were categorized into Structure, Documentation, Visual Representation, and General drivers. The evaluators had conflicting opinions both regarding the internal quality of the methods and refactoring decisions. Complex code problems were detected only by experienced evaluators. Using regression analysis, we looked at the predictive value of drivers explaining the refactoring decisions. The most salient driver leading to a favourable refactoring decision was method size. This study provides information of the refactoring decisions and helps form a basis for creating code problem detectors. By comparing automatic detection and the identified drivers we gained understanding of code problems that are difficult or impossible to detect automatically, for example Poor Algorithm. Issues detected only by experienced developers, and code problems for which the human eye surpasses automatic detection indicate good areas for developer education.}},
    address = {New York, NY, USA},
    author = {M\"{a}ntyl\"{a}, Mika V. and Lassenius, Casper},
    booktitle = {Proceedings of the 2006 ACM/IEEE international symposium on Empirical software engineering},
    citeulike-article-id = {1074628},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1159778},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1159733.1159778},
    doi = {10.1145/1159733.1159778},
    isbn = {1-59593-218-6},
    keywords = {ds},
    location = {Rio de Janeiro, Brazil},
    pages = {297--306},
    posted-at = {2012-06-20 20:45:18},
    priority = {2},
    publisher = {ACM},
    series = {ISESE '06},
    title = {{Drivers for software refactoring decisions}},
    url = {http://dx.doi.org/10.1145/1159733.1159778},
    year = {2006}
}

@inproceedings{Nowak2011Goals,
    abstract = {{Architectural decisions are the key element behind the design process leading to a software architecture. Making software architects aware of the implications of their decisions is only the beginning of what can be achieved by capturing the rationale and the constraints influencing the decision making process in a reusable body of architectural knowledge. In this paper we propose a metric-based approach to the analysis of architectural decision models. Using a hierarchically-structured approach we identify a number of useful goals and stakeholders involved in the architectural design process. Next, we sketch a set of metrics to provide data for the evaluation of the aforementioned goals. Our aim is to stimulate a discussion on how to find indicators relevant for software architects by measuring the intrinsic properties of architectural knowledge.}},
    address = {New York, NY, USA},
    author = {Nowak, Marcin and Pautasso, Cesare},
    booktitle = {Proceedings of the 6th International Workshop on SHAring and Reusing Architectural Knowledge},
    citeulike-article-id = {10807087},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1988676.1988682},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1988676.1988682},
    doi = {10.1145/1988676.1988682},
    isbn = {978-1-4503-0596-9},
    keywords = {ds},
    location = {Waikiki, Honolulu, HI, USA},
    pages = {21--28},
    posted-at = {2012-06-20 20:42:27},
    priority = {2},
    publisher = {ACM},
    series = {SHARK '11},
    title = {{Goals, questions and metrics for architectural decision models}},
    url = {http://dx.doi.org/10.1145/1988676.1988682},
    year = {2011}
}

@article{Zhao2011Rankbased,
    abstract = {{Refactoring can result in code with improved maintainability and is considered a preventive maintenance activity. Managers of large projects need ways to decide where to apply scarce resources when performing refactoring. There is a lack of tools for supporting such decisions. We introduce a rank-based software measure-driven refactoring decision support approach to assist managers. The approach uses various static measures to develop a weighted rank, ranking classes or packages that need refactoring. We undertook two case studies to examine the effectiveness of the approach. Specifically, we wanted to see if the decision support tool yielded results similar to those of human analysts/managers and in less time so that it can be used to augment human decision making. In the first study, we found that our approach identified classes as needing refactoring that were also identified by humans. In the second study, a hierarchical approach was used to identify packages that had actually been refactored in 15 releases of the open source project Tomcat. We examined the overlap between the tool's findings and the actual refactoring activities. The tool reached 100/86.7\% recall on the package/class level. Though these studies were limited in size and scope, it appears that this approach is worthy of further examination.}},
    address = {Secaucus, NJ, USA},
    author = {Zhao, Liming and Hayes, Jane H.},
    citeulike-article-id = {9664345},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=2070618.2070633},
    citeulike-linkout-1 = {http://dx.doi.org/10.1007/s11334-011-0154-3},
    citeulike-linkout-2 = {http://www.springerlink.com/content/n663p7t5385484v1},
    comment = {(private-note)
The article provides a model for Ranking the classes that would be edit most from refactoring and validates this model against developer opinion of the same.  The model uses coupling, halsteads effort, volume, and size with metrics to count the frequency of changes to the class as inputs to the ranking.  The model correlates well with de slop opinion of the classes that should be refactored when impaired n a blind test.  The paper supports the idea of using e model as a decision support tool to sleet classes for refactoring.
 },
    day = {5},
    doi = {10.1007/s11334-011-0154-3},
    issn = {1614-5046},
    journal = {Innov. Syst. Softw. Eng.},
    keywords = {chs-fse2012, ds},
    month = sep,
    number = {3},
    pages = {171--189},
    posted-at = {2012-06-20 20:23:46},
    priority = {4},
    publisher = {Springer-Verlag New York, Inc.},
    title = {{Rank-based refactoring decision support: two studies}},
    url = {http://dx.doi.org/10.1007/s11334-011-0154-3},
    volume = {7},
    year = {2011}
}

@inproceedings{Maletic2011MosaiCode,
    abstract = {{A software visualization tool called MosaiCode is introduced and described. MosaiCode uses a Seesoft metaphor to support the visualization and understanding of various characteristics for large scale software systems. A usage scenario is given to demonstrate the tool.}},
    author = {Maletic, Jonathan I. and Mosora, Daniel J. and Newman, Christian D. and Collard, Michael L. and Sutton, Andrew and Robinson, Brian P.},
    booktitle = {2011 6th International Workshop on Visualizing Software for Understanding and Analysis (VISSOFT)},
    citeulike-article-id = {10803294},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/vissof.2011.6069457},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6069457},
    doi = {10.1109/vissof.2011.6069457},
    isbn = {978-1-4577-0822-0},
    keywords = {chs, chs-fse2012},
    location = {Williamsburg, VA, USA},
    month = sep,
    pages = {1--4},
    posted-at = {2012-06-19 18:34:55},
    priority = {0},
    publisher = {IEEE},
    title = {{MosaiCode: Visualizing large scale software: A tool demonstration}},
    url = {http://dx.doi.org/10.1109/vissof.2011.6069457},
    year = {2011}
}

@inproceedings{Nagappan2008Influence,
    abstract = {{Often software systems are developed by organizations consisting of many teams of individuals working together. Brooks states in the Mythical Man Month book that product quality is strongly affected by organization structure. Unfortunately there has been little empirical evidence to date to substantiate this assertion. In this paper we present a metric scheme to quantify organizational complexity, in relation to the product development process to identify if the metrics impact failure-proneness. In our case study, the organizational metrics when applied to data from Windows Vista were statistically significant predictors of failure-proneness. The precision and recall measures for identifying failure-prone binaries, using the organizational metrics, was significantly higher than using traditional metrics like churn, complexity, coverage, dependencies, and pre-release bug measures that have been used to date to predict failure-proneness. Our results provide empirical evidence that the organizational metrics are related to, and are effective predictors of failure-proneness.}},
    address = {New York, NY, USA},
    author = {Nagappan, Nachiappan and Murphy, Brendan and Basili, Victor},
    booktitle = {Proceedings of the 30th international conference on Software engineering},
    citeulike-article-id = {6282979},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1368160},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1368088.1368160},
    doi = {10.1145/1368088.1368160},
    isbn = {978-1-60558-079-1},
    keywords = {devnetworks, ds},
    location = {Leipzig, Germany},
    pages = {521--530},
    posted-at = {2012-06-12 20:47:50},
    priority = {5},
    publisher = {ACM},
    series = {ICSE '08},
    title = {{The influence of organizational structure on software quality: an empirical case study}},
    url = {http://dx.doi.org/10.1145/1368088.1368160},
    year = {2008}
}

@inproceedings{Snipes2011Code,
    abstract = {{Commercial software development teams have limited time available to focus on improvements to their software. These teams need a way to quickly identify areas of the source code that would benefit from improvement, as well as quantifiable data to defend the selected improvements to management. Past research has shown that mining configuration management systems for change information can be useful in determining faulty areas of the code. We present a tool named Code Hot Spot, which mines change records out of Microsoft's TFS configuration management system and creates a report of hot spots. Hot spots are contiguous areas of the code that have higher values of metrics that are indicators of faulty code. We present a study where we use this tool to study projects at ABB to determine areas that need improvement. The resulting data have been used to prioritize areas for additional code reviews and unit testing, as well as identifying change prone areas in need of refactoring.}},
    author = {Snipes, W. and Robinson, B. and Murphy-Hill, E.},
    booktitle = {Software Maintenance (ICSM), 2011 27th IEEE International Conference on},
    citeulike-article-id = {10676850},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/icsm.2011.6080806},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6080806},
    doi = {10.1109/icsm.2011.6080806},
    institution = {Ind. Software Syst., ABB Corp. Res., Raleigh, NC, USA},
    isbn = {978-1-4577-0663-9},
    issn = {1063-6773},
    keywords = {chs, chs-fse2012},
    location = {Williamsburg, VA, USA},
    month = sep,
    pages = {392--401},
    posted-at = {2012-05-16 16:33:23},
    priority = {2},
    publisher = {IEEE},
    title = {{Code Hot Spot: A tool for extraction and analysis of code change history}},
    url = {http://dx.doi.org/10.1109/icsm.2011.6080806},
    year = {2011}
}

@article{basili-GQM,
    abstract = {{Experience from a dozen years of analyzing software engineering processes and products is summarized as a set of software engineering and measurement principles that argue for software engineering process models that integrate sound planning and analysis into the construction process. In the TAME (Tailoring A Measurement Environment) project at the University of Maryland, such an improvement-oriented software engineering process model was developed that uses the goal/question/metric paradigm to integrate the constructive and analytic aspects of software development. The model provides a mechanism for formalizing the characterization and planning tasks, controlling and improving projects based on quantitative analysis, learning in a deeper and more systematic way about the software process and product, and feeding the appropriate experience back into the current and future projects. The TAME system is an instantiation of the TAME software engineering process model as an ISEE (integrated software engineering environment). The first in a series of TAME system prototypes has been developed. An assessment of experience with this first limited prototype is presented including a reassessment of its initial architecture}},
    author = {Basili, V. R. and Rombach, H. D.},
    booktitle = {Software Engineering, IEEE Transactions on},
    citeulike-article-id = {692514},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/32.6156},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6156},
    comment = {This is the reference that Basili wants us to refer to as the original work on GQM},
    doi = {10.1109/32.6156},
    journal = {Software Engineering, IEEE Transactions on},
    keywords = {asd, metrics, scientist},
    number = {6},
    pages = {758--773},
    posted-at = {2012-04-24 19:37:09},
    priority = {3},
    title = {{The TAME project: towards improvement-oriented software environments}},
    url = {http://dx.doi.org/10.1109/32.6156},
    volume = {14},
    year = {1988}
}

@techreport{gqm-paradigm,
    abstract = {{This paper discusses the use of the Goal/Question/Metric paradigm as a mechanism for defining and interpreting software measuremente. Templates are provided for defining goals and generating questions. Different types of metrics are discussed. Examples of both processe and product goals are defined.}},
    author = {Basili, Victor R.},
    citeulike-article-id = {3936246},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=137076},
    comment = {(private-note)Don't reference this, reference the TAME paper},
    institution = {Techreport UMIACS TR-92-96, University of Maryland at College Park, College Park, MD, USA},
    keywords = {metrics, scientist},
    posted-at = {2012-04-21 22:29:27},
    priority = {0},
    title = {{Software modeling and measurement: the Goal/Question/Metric paradigm}},
    url = {http://portal.acm.org/citation.cfm?id=137076},
    year = {1992}
}

@article{Glockner2010Multiple,
    abstract = {{I will discuss how choices, decision times and confidence judgments can be combined to make inferences about individuals' decision strategies even if there is a high overlap in predicted choices. To make the chapter more easily comprehensible, the different steps of the method will be explained using a sample research question and a sample data set, and analysis syntax and outputs will be included. First, I will introduce the sample research question and explain how to derive predictions of choice strategies from different recent decision models. Then pros and cons will be discussed of simple statistical approaches (e.g., t-test and correlations) that include additional measures above choice, extending choice-based strategy classification. Finally, the multiple-measure maximum likelihood (MM-ML) strategy classification method will be introduced. The MM-ML method estimates the total (maximum) likelihood for the observed choice behaviour (including choices, decision times, and confidence ratings) of a participant given the application of a certain intuitive or deliberate strategy. The resulting likelihoods for different strategies can be compared and people should be classified as users of the strategy with the highest likelihood. An estimation program will be provided that makes the method easy to apply. Finally, the broad field of applications of the method is outlined and advantages and disadvantages for decision research will be discussed. (PsycINFO Database Record (c) 2010 APA, all rights reserved)}},
    author = {Gl\"{o}ckner, Andreas},
    citeulike-article-id = {10594154},
    edition = {viii},
    editor = {Gl\"{o}ckner, Andreas E.},
    journal = {Foundations for tracing intuition: Challenges and methods},
    keywords = {re-change},
    location = {New York, NY},
    pages = {83--105},
    posted-at = {2012-04-21 01:54:21},
    priority = {2},
    publisher = {Psychology Press},
    title = {{Multiple measure strategy classification: Outcomes, decision times and confidence ratings.}},
    year = {2010}
}

@article{Balakrishnan1996Testing,
    author = {Balakrishnan, J. D. and Ratcliff, Roger},
    citeulike-article-id = {10594152},
    citeulike-linkout-0 = {http://dx.doi.org/10.1037/0096-1523.22.3.615},
    comment = {(private-note)Article discusses empirically consistent ways to measure confidence in two-way and 3-way decision scenarios.  },
    doi = {10.1037/0096-1523.22.3.615},
    issn = {1939-1277},
    journal = {Journal of Experimental Psychology: Human Perception and Performance},
    keywords = {re-change},
    number = {3},
    pages = {615--633},
    posted-at = {2012-04-21 01:48:48},
    priority = {0},
    title = {{Testing models of decision making using confidence ratings in classification.}},
    url = {http://dx.doi.org/10.1037/0096-1523.22.3.615},
    volume = {22},
    year = {1996}
}

@inproceedings{Lutz1993Analyzing,
    abstract = {{The root causes of safety-related software errors in safety-critical embedded systems are analyzed. The results show that software errors identified as potentially hazardous to the system tend to be produced by different error mechanisms than those that produce nonsafety-related software errors. Safety-related software errors are shown to arise most commonly from: discrepancies between the documented requirements specifications and the requirements needed for correct functioning of the system; and misunderstandings of the interface of the software with the rest of the system. These results are used to identify methods by which requirements errors can be prevented. The goal is to reduce safety-related software errors and to enhance the safety of complex, embedded systems}},
    author = {Lutz, R. R.},
    booktitle = {Requirements Engineering, 1993., Proceedings of IEEE International Symposium on},
    citeulike-article-id = {10572093},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/isre.1993.324825},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=324825},
    comment = {(private-note)Lutz presents a causal analysis of discovered safety critical defects
in two NASA projects Voyager and Galileo.  He identifies significant
contribution to these errors from insufficient requirements analysis
and change management. Lutz provides six key recommendations for
reducing the frequency of safety-critical errors including use of
formal methods, formal change management, and defensive design.  I
like the approach and the classfication scheme Lutz defines.  It
would be great if follow-on studies confirmed the effects of applying
Lutz's recommendations.},
    doi = {10.1109/isre.1993.324825},
    institution = {Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA},
    isbn = {0-8186-3120-1},
    keywords = {re-w16},
    location = {San Diego, CA, USA},
    month = jan,
    pages = {126--133},
    posted-at = {2012-04-16 16:22:07},
    priority = {0},
    publisher = {IEEE},
    title = {{Analyzing software requirements errors in safety-critical, embedded systems}},
    url = {http://dx.doi.org/10.1109/isre.1993.324825},
    year = {1993}
}

@inproceedings{Fickas1995Requirements,
    abstract = {{We propose requirements monitoring to aid in the maintenance of systems that reside in dynamic environments. By requirements monitoring we mean the insertion of code into a running system to gather information from which it can he determined whether, and to what degree, that running system is meeting its requirements. Monitoring is a commonly applied technique in support of performance tuning, but the focus therein is primarily on computational performance requirements in short runs of systems. We wish to address systems that operate in a long lived, ongoing fashion in nonscientific enterprise applications. We argue that the results of requirements monitoring can be of benefit to the designers, maintainers and users of a system-alerting them when the system is being used in an environment for which it was not designed, and giving them the information they need to direct their redesign of the system. Studies of two commercial systems are used to illustrate and justify our claims.}},
    author = {Fickas, S. and Feather, M. S.},
    booktitle = {Proceedings of 1995 IEEE International Symposium on Requirements Engineering (RE'95)},
    citeulike-article-id = {10572086},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/isre.1995.512555},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=512555},
    comment = {(private-note)Fickas and Feather describe a method for abstracting from the
operational environment and system requirements some high-level system
goals.  They propose to continuously and autonomously monitor the
system for achievement of these goals to direct evolution of future
software versions.  It was not obvious to me how the automation of
goal achievement monitoring is possible in a general case (simple
examples like system performance and license key wait times excluded).
 With system requirements there can be many goals some of which like
usability elude automated monitoring.},
    doi = {10.1109/isre.1995.512555},
    isbn = {0-8186-7017-7},
    keywords = {re-w16},
    location = {York, UK},
    pages = {140--147},
    posted-at = {2012-04-16 16:20:08},
    priority = {0},
    publisher = {IEEE Comput. Soc. Press},
    title = {{Requirements monitoring in dynamic environments}},
    url = {http://dx.doi.org/10.1109/isre.1995.512555},
    year = {1995}
}

@inproceedings{Kauppinen2002Introducing,
    abstract = {{Introducing requirements engineering appears to involve a cultural change in organizations. Such a cultural change requires that requirements are defined and managed systematically, not only from a technical point of view, but also from the customers' and users' points of view. This paper describes experiences gained from four Finnish organizations that have started to introduce requirements engineering to their product development. The goal of this study was to evaluate which factors support, and which prevent, a cultural change. Linking business goals to technical requirements via user needs and user requirements was one of the key improvement actions that supported cultural change. Eliciting needs directly front real users and representing user requirements in the form of use cases were also key activities. However, bringing about a change of culture was challenging because both managers and product development engineers held beliefs that prevented active user need elicitation and systematic user requirement documentation.}},
    author = {Kauppinen, M. and Kujala, S. and Aaltio, T. and Lehtola, L.},
    booktitle = {Proceedings IEEE Joint International Conference on Requirements Engineering},
    citeulike-article-id = {832351},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/icre.2002.1048504},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1048504},
    doi = {10.1109/icre.2002.1048504},
    isbn = {0-7695-1465-0},
    journal = {Requirements Engineering, 2002. Proceedings. IEEE Joint International Conference on},
    keywords = {re-w14},
    location = {Essen, Germany},
    pages = {43--51},
    posted-at = {2012-04-09 14:36:24},
    priority = {2},
    publisher = {IEEE Comput. Soc},
    title = {{Introducing requirements engineering: how to make a cultural change happen in practice}},
    url = {http://dx.doi.org/10.1109/icre.2002.1048504},
    year = {2002}
}

@inproceedings{Nawrocki2002Extreme,
    abstract = {{Extreme programming (XP) is an agile (lightweight) software development methodology and it becomes more and more popular. XP proposes many interesting practices, but it also has some weaknesses. From the software engineering point of view the most important issues are: maintenance problems resulting from very limited documentation (XP relies on code and test cases only), and lack of wider perspective of a system to be built. Moreover, XP assumes that there is only one customer representative. In many cases there are several representatives (each one with his own view of the system and different priorities) and then some XP practices should be modified. In the paper we assess XP from two points of view: the capability maturity model and the Sommerville-Sawyer model (1997). We also propose how to introduce documented requirements to XP, how to modify the planning game to allow many customer representatives and how to get a wider perspective of a system to be built at the beginning of the project lifecycle.}},
    author = {Nawrocki, J. and Jasinski, M. and Walter, B. and Wojciechowski, A.},
    booktitle = {Proceedings IEEE Joint International Conference on Requirements Engineering},
    citeulike-article-id = {10548433},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/icre.2002.1048543},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1048543},
    doi = {10.1109/icre.2002.1048543},
    isbn = {0-7695-1465-0},
    keywords = {re-w14},
    location = {Essen, Germany},
    pages = {303--310},
    posted-at = {2012-04-09 14:35:46},
    priority = {2},
    publisher = {IEEE Comput. Soc},
    title = {{Extreme programming modified: embrace requirements engineering practices}},
    url = {http://dx.doi.org/10.1109/icre.2002.1048543},
    year = {2002}
}

@inproceedings{Aranda2007Requirements,
    abstract = {{Small companies form a large part of the software industry, but have mostly been overlooked by the requirements engineering research community. We know very little about the techniques these companies use to elicit and track requirements and about their contexts of operations. This paper presents preliminary results from an ongoing exploratory case study of requirements management in seven small companies, which found that (a) successful small companies exhibit a huge diversity of requirements practices that work well enough for their contexts; (b) these companies display strong cultural cohesion; (c) the principal of the company tends to retain control of the requirements processes long after other tasks have been delegated; and (d) the evidence rejects the simplistic view of a current "software crisis", as requirements errors for these companies, though problematic, are rarely catastrophic. We develop a number of hypotheses to explain these findings.}},
    author = {Aranda, Jorge and Easterbrook, Steve and Wilson, Greg},
    booktitle = {15th IEEE International Requirements Engineering Conference (RE 2007)},
    citeulike-article-id = {10546756},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/re.2007.54},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4384165},
    comment = {(private-note)The paper provides a study of how small companies perform requiremens engineering activities.  They interact with a set of small companies informally to understand their strucutre and mehtods for managing requirements.  The main idea is that each company has a self-defined way of handling requirements that may not be formal, reflects their unique culture, and is successful in meeting their customers' expectations.  The strength of this work is the conduct of the case study without preconceived structure to the outcome.  The main weakness is the use of hypothesis techniques without empirical evidence.},
    doi = {10.1109/re.2007.54},
    isbn = {0-7695-2935-6},
    keywords = {re-w14},
    location = {Delhi, India},
    month = oct,
    pages = {39--48},
    posted-at = {2012-04-09 00:15:32},
    priority = {5},
    publisher = {IEEE},
    title = {{Requirements in the wild: How small companies do it}},
    url = {http://dx.doi.org/10.1109/re.2007.54},
    year = {2007}
}

@electronic{SylvieTrudel1andLuigiBuglioneGuideline,
    author = {Sylvie Trudel1 and Luigi Buglione},
    citeulike-article-id = {10490689},
    journal = {IWSM/MetriKon 2010},
    keywords = {benchmarking, size},
    organization = {IWSM/MetriKon 2010},
    posted-at = {2012-03-23 13:35:54},
    priority = {4},
    publisher = {http://www.cosmicon.com/portal/},
    title = {{Guideline for Sizing Agile Projects with COSMIC}}
}

@article{Heimdahl1996Completeness,
    abstract = {{This paper describes methods for automatically analyzing formal,
state-based requirements specifications for some aspects of completeness
and consistency. The approach uses a low-level functional formalism,
simplifying the analysis process. State-space explosion problems are
eliminated by applying the analysis at a high level of abstraction;
i.e., instead of generating a reachability graph for analysis, the
analysis is performed directly on the model. The method scales up to
large systems by decomposing the specification into smaller, analyzable
parts and then using functional composition rules to ensure that
verified properties hold for the entire specification. The analysis
algorithms and tools have been validated on TCAS II, a complex,
airborne, collision-avoidance system required on all commercial aircraft
with more than 30 passengers that fly in U.S. Airspace}},
    address = {Piscataway, NJ, USA},
    author = {Heimdahl, M. P. E. and Leveson, N. G.},
    citeulike-article-id = {573625},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=231231},
    citeulike-linkout-1 = {http://doi.ieeecomputersociety.org/10.1109/32.508311},
    citeulike-linkout-2 = {http://dx.doi.org/10.1109/32.508311},
    citeulike-linkout-3 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=508311},
    doi = {10.1109/32.508311},
    institution = {Dept. of Comput. Sci., Minnesota Univ., Minneapolis, MN, USA},
    issn = {0098-5589},
    journal = {Software Engineering, IEEE Transactions on},
    keywords = {re-w10},
    month = jun,
    number = {6},
    pages = {363--377},
    posted-at = {2012-03-12 01:28:26},
    priority = {3},
    publisher = {IEEE},
    title = {{Completeness and consistency in hierarchical state-based requirements}},
    url = {http://dx.doi.org/10.1109/32.508311},
    volume = {22},
    year = {1996}
}

@article{Heitmeyer1996Automated,
    abstract = {{This article describes a formal analysis technique, called consistency checking, for automatic detection of errors, such as type errors, nondeterminism, missing cases, and circular definitions, in requirements specifications. The technique is designed to analyze requirements specifications expressed in the SCR (Software Cost Reduction) tabular notation. As background, the SCR approach to specifying requirements is reviewed. To provide a formal semantics for the  SCR notation and a foundation for consistency checking, a formal requirements model is introduced; the model represents a software system as a finite-state automation which produces externally visible outputs in response to changes in monitored environmental quantities. Results of two experiments are presented which evaluated the utility and scalability of our technique for consistency checking in real-world avionics application. The role of consistency checking during the requirements phase of software development is discussed.}},
    address = {New York, NY, USA},
    author = {Heitmeyer, Constance L. and Jeffords, Ralph D. and Labaw, Bruce G.},
    citeulike-article-id = {658437},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=234431},
    citeulike-linkout-1 = {http://dblp.uni-trier.de/rec/bibtex/journals/tosem/HeitmeyerJL96},
    citeulike-linkout-2 = {http://dx.doi.org/10.1145/234426.234431},
    doi = {10.1145/234426.234431},
    issn = {1049-331X},
    journal = {ACM Trans. Softw. Eng. Methodol.},
    keywords = {re-w10},
    month = jul,
    number = {3},
    pages = {231--261},
    posted-at = {2012-03-12 01:22:33},
    priority = {3},
    publisher = {ACM},
    title = {{Automated consistency checking of requirements specifications}},
    url = {http://dx.doi.org/10.1145/234426.234431},
    volume = {5},
    year = {1996}
}

@article{Easterbrook1998Experiences,
    abstract = {{The paper describes three case studies in the lightweight
application of formal methods to requirements modeling for spacecraft
fault protection systems. The case studies differ from previously
reported applications of formal methods in that formal methods were
applied very early in the requirements engineering process to validate
the evolving requirements. The results were fed back into the projects
to improve the informal specifications. For each case study, we describe
what methods were applied, how they were applied, how much effort was
involved, and what the findings were. In all three cases, formal methods
enhanced the existing verification and validation processes by testing
key properties of the evolving requirements and helping to identify
weaknesses. We conclude that the benefits gained from early modeling of
unstable requirements more than outweigh the effort needed to maintain
multiple representations}},
    author = {Easterbrook, S. and Lutz, R. and Covington, R. and Kelly, J. and Ampo, Y. and Hamilton, D.},
    citeulike-article-id = {266189},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/32.663994},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=663994},
    comment = {(private-note)The authors conduct 3 case studies applying formal methods to requirements analysis for space systems.  The studies attempt to verify and validate requirements for fault protection software for the ISS and Cassini (a deep space mission).  As a case study they contribute the benefits and issues with applying formal methods in these projects.  

FDIR = Fault Detection Isolation and Recovery
FCD=functional concept diagram
SCR?

The first study and third study used PVS a formal specification language
The second study used SCR tables (what is SCR?) that elaborated the specific sets of conditions under which each step of recovery is taken.  There were errors of missing requirements, ambiguities, and timing constraints identified with this method.  
The third study used OMT diagrams and PVS  the third study identified issues with assumptions, boundary/fringe cases, traceability or inconsistency (requirements to design), imprecise terms (use of synonyms), a starvation issue, 

Aside from the methods applied, the dialog between the modelers and the developers was helpful to both parties.  They encourage light weight formal methods during early requirements analysis.},
    doi = {10.1109/32.663994},
    institution = {NASA IV\&V Fac., Fairmont, WV},
    issn = {0098-5589},
    journal = {Software Engineering, IEEE Transactions on},
    keywords = {re-w10},
    month = jan,
    number = {1},
    pages = {4--14},
    posted-at = {2012-03-12 01:16:42},
    priority = {5},
    publisher = {IEEE},
    title = {{Experiences using lightweight formal methods for requirements
modeling}},
    url = {http://dx.doi.org/10.1109/32.663994},
    volume = {24},
    year = {1998}
}

@inproceedings{Greenspan1994Formal,
    abstract = {{Research issues related to requirements modeling are introduced
and discussed through a review of the requirements modeling language
RML, its peers and its successors from the time it was first proposed at
the Sixth International Conference on Software Engineering (ICSE-6) to
the present - ten ICSEs later. We note that the central theme of
 ” Capturing More World Knowledge” in the original RML
proposal is becoming increasingly important in requirements engineering.
The paper highlights key ideas and research issues that have driven RML
and its peers, evaluates them retrospectively in the context of
experience and more recent developments, and points out significant
remaining problems and directions for requirements modeling research}},
    author = {Greenspan, S. and Mylopoulos, J. and Borgida, A.},
    booktitle = {Software Engineering, 1994. Proceedings. ICSE-16., 16th International Conference on},
    citeulike-article-id = {10439937},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/icse.1994.296773},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=296773},
    comment = {(private-note)Requirements definition is a careful assessment of the needs that a system is to fulfill.  It says why it is needed what it will do (features) and how it will be made (construction blueprint).  the paper contributes an analysis of requirements modeling as expressed in RML including motivation, features, competing languages, experiences using, and tool support needs.  The paper is strong in describing the development of requirements modeling methods and their applicability.  The paper provides some research interest but does not overtly state research goals.

},
    doi = {10.1109/icse.1994.296773},
    institution = {GTE Labs. Inc., Waltham, MA},
    isbn = {0-8186-5855-X},
    issn = {0270-5257},
    keywords = {re-w10},
    month = may,
    pages = {135--147},
    posted-at = {2012-03-12 01:13:39},
    priority = {5},
    publisher = {IEEE},
    title = {{On formal requirements modeling languages: RML revisited}},
    url = {http://dx.doi.org/10.1109/icse.1994.296773},
    year = {1994}
}

@book{Pressman2004Software,
    abstract = {{For over 20 years, \_Software Engineering: A Practitioner's Approach\_ has been
the best selling guide to software engineering for students and industry
professionals alike.

The sixth edition continues to lead the way in software engineering. A new
Part 4 on Web Engineering presents a complete engineering approach for the
analysis, design, and testing of Web Applications, increasingly important for
today's students. Additionally, the UML coverage has been enhanced and
signficantly increased in this new edition.

The pedagogy has also been improved in the new edition to include sidebars.
They provide information on relevant softare tools, specific work flow for
specific kinds of projects, and additional information on various topics.
Additionally, Pressman provides a running case study called "Safe Home"
throughout the book, which provides the application of software engineering to
an industry project.

New additions to the book also include chapters on the Agile Process Models,
Requirements Engineering, and Design Engineering. The book has been completely
updated and contains hundreds of new references to software tools that address
all important topics in the book.

The ancillary material for the book includes an expansion of the case study,
which illustrates it with UML diagrams. The On-Line Learning Center includes
resources for both instructors and students such as checklists, 700
categorized web references, Powerpoints, a test bank, and a software
engineering library-containing over 500 software engineering papers. TAKEAWY
HERE IS THE FOLLOWING: 1. AGILE PROCESS METHODS ARE COVERED EARLY IN CH. 4 2.
NEW PART ON WEB APPLICATIONS --5 CHAPTERS}},
    author = {Pressman, Roger and Pressman, Roger},
    citeulike-article-id = {453686},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/007301933X},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/007301933X},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/007301933X},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/007301933X},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/007301933X/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/007301933X},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/007301933X},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN007301933X},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=007301933X\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/007301933X},
    day = {02},
    edition = {6},
    howpublished = {Hardcover},
    isbn = {007301933X},
    keywords = {re-change},
    month = apr,
    posted-at = {2012-03-03 01:14:22},
    priority = {0},
    publisher = {McGraw-Hill Science/Engineering/Math},
    title = {{Software Engineering: A Practitioner's Approach}},
    url = {http://www.worldcat.org/isbn/007301933X},
    year = {2004}
}

@book{Chrissis2003CMMI,
    author = {Chrissis, Mary B. and Konrad, Mike and Shrum, Sandy},
    citeulike-article-id = {318944},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0321154967},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0321154967},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0321154967},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0321154967},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0321154967/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0321154967},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0321154967},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0321154967},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0321154967\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0321154967},
    day = {24},
    howpublished = {Hardcover},
    isbn = {0321154967},
    keywords = {re-change},
    month = feb,
    posted-at = {2012-03-02 01:36:20},
    priority = {0},
    publisher = {{Addison-Wesley Professional}},
    title = {{CMMI : Guidelines for Process Integration and Product Improvement}},
    url = {http://www.worldcat.org/isbn/0321154967},
    year = {2003}
}

@article{Boehm1996Identifying,
    abstract = {{Without a well-defined set of quality-attribute requirements,
software projects are vulnerable to failure. The authors have developed
QARCC, a knowledge-based tool that helps users, developers, and
customers analyze requirements and identify conflicts among them}},
    address = {Los Alamitos, CA, USA},
    author = {Boehm, B. and In, H.},
    citeulike-article-id = {1090999},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=625567},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/52.506460},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=506460},
    doi = {10.1109/52.506460},
    institution = {Univ. of Southern California, Los Angeles, CA},
    issn = {0740-7459},
    journal = {Software, IEEE},
    keywords = {re-change, re-w12},
    month = mar,
    number = {2},
    pages = {25--35},
    posted-at = {2012-02-28 23:46:19},
    priority = {5},
    publisher = {IEEE},
    title = {{Identifying quality-requirement conflicts}},
    url = {http://dx.doi.org/10.1109/52.506460},
    volume = {13},
    year = {1996}
}

@article{Anton2003Precluding,
    abstract = {{Keeping sensitive information secure is increasingly important in e-commerce and web-based applications in which personally identifiable information is electronically transmitted and disseminated. This paper discusses techniques to aid in aligning security and privacy policies with system requirements. Early conflict identification between requirements and policies enables analysts to prevent incongruous behavior, misalignments and unfulfilled requirements, ensuring that security and privacy are built in rather than added on as an afterthought. Validated techniques to identify conflicts between system requirements and the governing security and privacy policies are presented. The techniques are generalizable to other domains, in which systems contain sensitive information.}},
    author = {Ant\'{o}n, Annie I. and Earp, Julia B. and Carter, Ryan A.},
    citeulike-article-id = {10395095},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/s0950-5849(03)00095-8},
    doi = {10.1016/s0950-5849(03)00095-8},
    issn = {09505849},
    journal = {Information and Software Technology},
    keywords = {re-change, re-w12},
    month = nov,
    number = {14},
    pages = {967--977},
    posted-at = {2012-02-28 23:45:11},
    priority = {5},
    title = {{Precluding incongruous behavior by aligning software requirements with security and privacy policies}},
    url = {http://dx.doi.org/10.1016/s0950-5849(03)00095-8},
    volume = {45},
    year = {2003}
}

@article{Runeson2009Guidelines,
    abstract = {{Case study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies. The content is based on the authors' own experience from conducting and reading case studies. The terminology and guidelines are compiled from different methodology handbooks in other research domains, in particular social science and information systems, and adapted to the needs in software engineering. We present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case study research.}},
    author = {Runeson, Per and H\"{o}st, Martin},
    citeulike-article-id = {3818753},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/s10664-008-9102-8},
    citeulike-linkout-1 = {http://www.springerlink.com/content/t22r8l65q7h31636},
    day = {1},
    doi = {10.1007/s10664-008-9102-8},
    issn = {1382-3256},
    journal = {Empirical Software Engineering},
    keywords = {scientist, thesis-ms},
    month = apr,
    number = {2},
    pages = {131--164},
    posted-at = {2012-02-28 21:26:41},
    priority = {3},
    publisher = {Springer Netherlands},
    title = {{Guidelines for conducting and reporting case study research in software engineering}},
    url = {http://dx.doi.org/10.1007/s10664-008-9102-8},
    volume = {14},
    year = {2009}
}

@article{Barrett1995Knowledge,
    abstract = {{This paper describes the knowledge elicitation and knowledge representation aspects of a system being developed to help with the design and maintenance of relational data bases. The size algorithmic components. In addition, the domain contains multiple experts, but any given expert's knowledge of this large domain is only partial. The paper discusses the methods and techniques used for knowledge elicitation, which was based on a  ” broad and shallow” approach at first, moving to a  ” narrow and deep” one later, and describes the models used for knowledge representation, which were based on a layered  ” generic and variants” approach.}},
    author = {Barrett, A. R. and Edwards, J. S.},
    citeulike-article-id = {10391977},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/0957-4174(94)e0007-h},
    doi = {10.1016/0957-4174(94)e0007-h},
    issn = {09574174},
    journal = {Expert Systems with Applications},
    keywords = {re-change},
    month = jan,
    number = {1},
    pages = {169--176},
    posted-at = {2012-02-28 00:15:47},
    priority = {2},
    title = {{Knowledge elicitation and knowledge representation in a large domain with multiple experts}},
    url = {http://dx.doi.org/10.1016/0957-4174(94)e0007-h},
    volume = {8},
    year = {1995}
}

@article{Schmidt2001Identifying,
    abstract = {{Advocates of software risk management claim that by identifying and analyzing threats to success (i.e., risks) action can be taken to reduce the chance of failure of a project. The first step in the risk management process is to identify the risk itself, so that appropriate countermeasures can be taken. One problem in this task, however, is that no validated lists are available to help the project manager understand the nature and types of risks typically faced in a software project. This paper represents a first step toward alleviating this problem by developing an authoritative list of common risk factors. We deploy a rigorous data collection method called a ranking-type Delphi survey to produce a rank-order list of risk factors. This data collection method is designed to elicit and organize opinions of a panel of experts through iterative, controlled feedback. Three simultaneous surveys were conducted in three different settings: Hong Kong, Finland, and the United States. This was done to broaden our view of the types of risks, rather than relying on the view of a single culture-an aspect that has been ignored in past risk management research. In forming the three panels, we recruited experienced project managers in each country. The paper presents the obtained risk factor list, compares it with other published risk factor lists for completeness and variation, and analyzes common features and differences in risk factor rankings in the three countries. We conclude by discussing implications of our findings for both research and improving risk management practice.}},
    address = {Armonk, NY, USA},
    author = {Schmidt, Roy and Lyytinen, Kalle and Keil, Mark and Cule, Paul},
    citeulike-article-id = {10388393},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1289668.1289670},
    issn = {0742-1222},
    journal = {J. Manage. Inf. Syst.},
    keywords = {re-change},
    month = mar,
    pages = {5--36},
    posted-at = {2012-02-27 01:42:04},
    priority = {4},
    publisher = {M. E. Sharpe, Inc.},
    title = {{Identifying Software Project Risks: An International Delphi Study}},
    url = {http://portal.acm.org/citation.cfm?id=1289668.1289670},
    volume = {17},
    year = {2001}
}

@inproceedings{Glinz2000Problems,
    abstract = {{In recent years, UML has become a standard language for modeling
software requirements and design. We investigate the suitability of UML
as a semiformal requirements specification language. Using the
Teleservices and Remote Medical Care (TRMCS) case study as an example,
we identify and demonstrate various problems and deficiencies of UML,
particularly concerning use case models and system decomposition. We
also investigate whether and how the deficiencies can be overcome and
how potential alternatives could look}},
    author = {Glinz, M.},
    booktitle = {Software Specification and Design, 2000. Tenth International Workshop on},
    citeulike-article-id = {10388296},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/iwssd.2000.891122},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=891122},
    comment = {(private-note)The paper contributes a detailed analysis of deficiencies of UML use case models when applied to a full system context design.  Deficicenies are described in detail and graphically shown such as UML use cases cannot specify a system to actor initiated action, and UML cannot express use case interaction.  It states that UML use cases cannot express state information in a system (though another paper proposes enchancements that enable this).  The authors propose enchancements that address some of the deficiencies and refer to a specification language called ADORA which they created to address the deficiencies.  The paper does not discuss details of the ADORA language in contrast to the deficiencies of UML which would be useful.  },
    doi = {10.1109/iwssd.2000.891122},
    institution = {Inst. fur Inf., Zurich Univ.},
    isbn = {0-7695-0884-7},
    keywords = {re-w8},
    pages = {11--22},
    posted-at = {2012-02-27 00:49:33},
    priority = {0},
    publisher = {IEEE},
    title = {{Problems and deficiencies of UML as a requirements specification
language}},
    url = {http://dx.doi.org/10.1109/iwssd.2000.891122},
    year = {2000}
}

@inproceedings{Whittle2000Generating,
    abstract = {{This paper presents an algorithm for automatically generating UML statecharts from a collection of UML sequence diagrams. Computer support for this transition between requirements and design is important for a successful application of UML's highly iterative, distributed software development process. There are three main issues which must be addressed when generating statecharts from sequence diagrams. Firstly, conflicts arising from the merging of independently developed sequence diagrams must be detected and resolved. Secondly, different sequence diagrams often contain identical or similar behaviors. For a true interleaving of the sequence diagrams, these behaviors must be recognized and merged. Finally, generated statecharts usually are only an approximation of the system and thus must be hand-modified and refined by designers. As such, the generated artifact should be highly structured and readable. In terms of statecharts, this corresponds to the introduction of hierarchy. Our algorithm successfully tackles all three of these aspects and will be illustrated in this paper with a well-known ATM example.}},
    address = {New York, NY, USA},
    author = {Whittle, Jon and Schumann, Johann},
    booktitle = {Proceedings of the 22nd international conference on Software engineering},
    citeulike-article-id = {467351},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=337217},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/337180.337217},
    comment = {(private-note)This work contributes an algorithm that together with notation addtions allows automated translation between multiple UML sequence diagrams and a unified optimized state chart. The algorithm requires additions to the UML notation to simplify resoloving similar messages and uses class hierarchy information establish the system domain and merge similar state transitions onto one. The article provides detailed information on the algorithm and applies it to the ATM problem. There is a research opporutnity to apply the method in an industrial study where more complex problems may present new challenges. },
    doi = {10.1145/337180.337217},
    isbn = {1-58113-206-9},
    keywords = {re-w8},
    location = {Limerick, Ireland},
    pages = {314--323},
    posted-at = {2012-02-27 00:48:13},
    priority = {0},
    publisher = {ACM},
    series = {ICSE '00},
    title = {{Generating statechart designs from scenarios}},
    url = {http://dx.doi.org/10.1145/337180.337217},
    year = {2000}
}

@inproceedings{Harker1993Change,
    abstract = {{The difficulty of handling changing requirements within
traditional development processes is described. The origins of changing
user and organizational requirements are discussed and different types
are classified. The author identifies a number of ways in which
different approaches to design may help to deal with change as well as
mechanisms which should underpin effective communication between users
and designers}},
    author = {Harker, S. D. P. and Eason, K. D. and Dobson, J. E.},
    booktitle = {Requirements Engineering, 1993., Proceedings of IEEE International Symposium on},
    citeulike-article-id = {590133},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/isre.1993.324847},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=324847},
    comment = {(private-note)
"Most structured development activities  [4, 5, 63 are underpined by a sequential or "waterfall" approach which regulates the relationship between the client who "owns" the problem and the developer who seeks to solve it. Within this framework, requirements are agreed at fixed points within the life-cycle in order that system development can be undertaken in  an orderly and controlled manner." 


 


 


"If a requirements statement is seen as a statement of current policy (the client's view) rather than as justification for a design decision (the developer's view), it becomes clear that some requirements may not exist in an objective sense, but only  as subjective interpretations, and these interpretations can only emerge and be clarified and agreed through the creation of a suitable agenda for debate." 


 


The paper creates a classification of requirement type that provides a dock upon which to attach change triggers.  The classification also highlights a common origin for each requirement type: 








 
 



Type of Requirement 
 



Origins 
 



 
 





Stable 
 



Enduring 
 



Technical Core of the Business 
 



 
 





Changing 
 



Mutable 
 



Environmental Turbulence 
 



 
 





 
 



Emergent 
 



Stakeholder Engagement in Requirements 


Elicitation 
 



 
 





 
 



Consequential  
 



System Use and User Development 
 



 
 





 
 



Adaptive 
 



Situated Action and Task Variation 
 



 
 





 
 



Migration 
 



Constraints of Planned Organisational 


Development 
 



 
 


 


 


Figure 1: Types of Requirement 


 


Mutable requirements are functional requirements that are a current or future need.  They may become "Enduring" at some point or may be a passing fancy 


 


Emergent requirements are those that arise from further detailed system specification (such as through scenario analysis) and design. 


 


Consequential are those requirements that surface after a prototype or system is delivered to the customer and the customer thinks of new ways to use it or better ways of working with it. 


 


Adaptive requirements describe the ability for the system to adapt within the user environment without having to be rewritten. 


 


Migration requirements facilitate the evolution of work practices from current state to new state with minimal disruption.   


 


The paper goes on the suggest ways of deal with changing requirements that sound suspiciously like agile methods such as: proceeding to development based on a minimum critical specification with further details of the project requirements evolving as the project progresses, evolving the client-developer relationship to allow for incremental development,  involvement of the end user in the specification process, 
},
    doi = {10.1109/isre.1993.324847},
    institution = {Dept. of Human Sci., Loughborough Univ. of Technol.},
    isbn = {0-8186-3120-1},
    journal = {Requirements Engineering, 1993., Proceedings of IEEE International Symposium on},
    keywords = {re-change},
    month = jan,
    pages = {266--272},
    posted-at = {2012-02-26 01:00:48},
    priority = {0},
    publisher = {IEEE},
    title = {{The change and evolution of requirements as a challenge to the
practice of software engineering}},
    url = {http://dx.doi.org/10.1109/isre.1993.324847},
    year = {1993}
}

@book{Tate2006Sustainable,
    author = {Tate, Kevin},
    booktitle = {Sustainable software development : an agile perspective},
    chapter = {2},
    citeulike-article-id = {10384311},
    editor = {Cockburn, Alistair and Highsmith, Jim},
    isbn = {0321286081},
    keywords = {td},
    location = {Upper Saddle River, NJ},
    posted-at = {2012-02-25 02:28:45},
    priority = {2},
    publisher = {Addison-Wesley},
    title = {{Sustainable software development : an agile perspective}},
    year = {2006}
}

@inproceedings{Karg2008Analysis,
    abstract = {{The majority of software quality cost models is by design capable of describing costs retrospectively but relies on defect estimation in order to provide a cost forecast. We identify two major approaches to defect estimation and evaluate them in a large scale industrial software development project with special focus on applicability in quality cost models. Our studies show that neither static models based on code metrics nor dynamic software reliability growth models are suitable for an industrial application.}},
    author = {Karg, L. M. and Beckhaus, A.},
    booktitle = {Industrial Engineering and Engineering Management, 2008. IEEM 2008. IEEE International Conference on},
    citeulike-article-id = {10383845},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/ieem.2008.4737876},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4737876},
    doi = {10.1109/ieem.2008.4737876},
    institution = {SAP Res., Darmstadt, Germany},
    isbn = {978-1-4244-2629-4},
    keywords = {ds, prediction, td},
    month = dec,
    pages = {287--291},
    posted-at = {2012-02-24 21:35:26},
    priority = {2},
    publisher = {IEEE},
    title = {{Analysis of software quality cost modeling's industrial applicability with focus on defect estimation}},
    url = {http://dx.doi.org/10.1109/ieem.2008.4737876},
    year = {2008}
}

@techreport{1998IEEE,
    abstract = {{The minimum required contents of a Software Configuration Management Plan (SCMP) are established, and the specific activities to be addressed and their requirements for any portion of a software product's life cycle are defined.}},
    citeulike-article-id = {10381607},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/ieeestd.1998.88281},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=720569},
    doi = {10.1109/ieeestd.1998.88281},
    isbn = {0-7381-0331-4},
    keywords = {re-change, td},
    pages = {i--17},
    posted-at = {2012-02-24 03:07:17},
    priority = {2},
    publisher = {IEEE},
    title = {{IEEE Standard for Software Configuration Management Plans}},
    url = {http://dx.doi.org/10.1109/ieeestd.1998.88281},
    year = {1998}
}

@inproceedings{Bhatti2010Methodology,
    abstract = {{Requirements evolve and managing requirements evolution is necessary to meet the business needs of a customer. The continuous changing requirements can affect the cost, schedule and quality of a software project. Inability to manage the changing requirements may lead to inconsistencies in system requirements and results in failure of the software project. An effective methodology to manage changing requirements is, therefore, necessary for successful project execution. This paper suggests a formal methodology to manage the changing requirements of a software project. In proposed methodology the requirements change management process is break-downed in multiple phases. The major stakeholders and typical work products of each phase are identified. The role of each stakeholder is explained with respect to relevant phase of the process. It has been identified that the Change Control Board (CCB) is the process owner and the main stakeholder of the change management process. The proposed methodology, which is based upon six phases, continuously manages the change requests throughout the whole life cycle of the project.}},
    author = {Bhatti, M. W. and Hayat, F. and Ehsan, N. and Ahmed, S. and Ishaque, A. and Mirza, E.},
    booktitle = {Computer Information Systems and Industrial Management Applications (CISIM), 2010 International Conference on},
    citeulike-article-id = {10381582},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/cisim.2010.5643642},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5643642},
    comment = {(private-note)A short paper that defines a Change Control Board and sets out some of the functions of the CCB in the context of managing requirements changes.},
    doi = {10.1109/cisim.2010.5643642},
    institution = {Eng. Manage. Dept., Center for Adv. Studies in Eng., Islamabad, Pakistan},
    isbn = {978-1-4244-7817-0},
    keywords = {re-change},
    month = oct,
    pages = {319--322},
    posted-at = {2012-02-24 02:56:26},
    priority = {0},
    publisher = {IEEE},
    title = {{A methodology to manage the changing requirements of a software project}},
    url = {http://dx.doi.org/10.1109/cisim.2010.5643642},
    year = {2010}
}

@techreport{1990IEEE,
    abstract = {{Describes the IEEE Std 610.12-1990, IEEE standard glossary of software engineering terminology, which identifies terms currently in use in the field of software engineering. Standard definitions for those terms are established.>}},
    citeulike-article-id = {1341545},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/ieeestd.1990.101064},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=159342},
    doi = {10.1109/ieeestd.1990.101064},
    journal = {IEEE Std 610.12-1990},
    keywords = {td},
    pages = {1+},
    posted-at = {2012-02-24 02:54:55},
    priority = {2},
    publisher = {IEEE},
    title = {{IEEE Standard Glossary of Software Engineering Terminology}},
    url = {http://dx.doi.org/10.1109/ieeestd.1990.101064},
    year = {1990}
}

@article{Maiden1998CREWSSAVRE,
    abstract = {{This paper reports research into semi-automatic generation of scenarios for validating software-intensive system requirements. The research was undertaken as part of the ESPRIT IV 21903 'CREWS' long-term research project. The paper presents the underlying theoretical models of domain knowledge, computational mechanisms and user-driven dialogues needed for scenario generation. It describes how CREWS draws on theoretical results from the ESPRIT III 6353 'NATURE' basic research action, that is object system models which are abstractions of the fundamental features of different categories of problem domain. CREWS uses these models to generate normal course scenarios, then draws on theoretical and empirical research from cognitive science, human-computer interaction, collaborative systems and software engineering to generate alternative courses for these scenarios. The paper describes a computational mechanism for deriving use cases from object system models, simple rules to link actions in a use case, taxonomies of classes of exceptions which give rise to alternative courses in scenarios, and a computational mechanism for generation of multiple scenarios from a use case specification.}},
    author = {Maiden, N. A. M.},
    citeulike-article-id = {3475715},
    citeulike-linkout-0 = {http://dx.doi.org/10.1023/a:1008605412971},
    citeulike-linkout-1 = {http://www.springerlink.com/content/t044057158770w65},
    day = {1},
    doi = {10.1023/a:1008605412971},
    issn = {09288910},
    journal = {Automated Software Engineering},
    keywords = {re-w7},
    month = oct,
    number = {4},
    pages = {419--446},
    posted-at = {2012-02-20 03:36:10},
    priority = {2},
    publisher = {Springer Netherlands},
    title = {{CREWS-SAVRE: Scenarios for Acquiring and Validating Requirements}},
    url = {http://dx.doi.org/10.1023/a:1008605412971},
    volume = {5},
    year = {1998}
}

@article{Weidenhaupt1998Scenarios,
    abstract = {{Scenario based approaches are becoming ubiquitous in systems
analysis and design but remain vague in definition and scope. A survey
of current practices indicates we must offer better means for
structuring, managing, and developing their use in diverse contexts. The
European Esprit project Crews (Cooperative Requirements Engineering with
Scenarios) are seeking a deeper understanding of scenario diversity,
necessary to improve methodological and tool support for scenario based
requirements engineering. They follow a two pronged strategy to gain
this understanding. First, following the  ” 3 dimensions”
requirements engineering framework developed in the precursor Nature
project (K. Pohl, 1994), they developed a scenario classification
framework based on a comprehensive survey of scenario literature in
requirements engineering, human computer interaction, and other fields.
They used the framework to classify 11 prominent scenario based
approaches. Secondly, to complement this research framework, they
investigated scenario applications in industrial projects through site
visits with scenario user projects. The article focuses on these site
visits. It was found that while many companies express interest in
Jacobson's use case approach, actual scenario usage often falls outside
what is described in textbooks and standard methodologies. Users
therefore face significant scenario management problems not yet
addressed adequately in theory or practice, and are demanding solutions
to these problems}},
    author = {Weidenhaupt, K. and Pohl, K. and Jarke, M. and Haumer, P.},
    citeulike-article-id = {4050475},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/52.663783},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=663783},
    doi = {10.1109/52.663783},
    institution = {Tech. Hochschule Aachen},
    issn = {0740-7459},
    journal = {Software, IEEE},
    keywords = {re-w7},
    month = mar,
    number = {2},
    pages = {34--45},
    posted-at = {2012-02-20 03:33:24},
    priority = {5},
    publisher = {IEEE},
    title = {{Scenarios in system development: current practice}},
    url = {http://dx.doi.org/10.1109/52.663783},
    volume = {15},
    year = {1998}
}

@article{Jarke1998Scenario,
    abstract = {{Scenario management (SM) means different things to different people, even though everyone seems to admit its current importance and its further potential. In this paper, we seek to provide an interdisciplinary framework for SM from three major disciplines that use scenarios – strategic management, human–computer interaction, and software and systems engineering – to deal with description of current and future realities. In particular, we attempt to answer the following questions: How are scenarios developed and used in each of the three disciplines? Why are they becoming important? What are current research contributions in scenario management? What are the research and practical issues related to the creation and use of scenarios, in particular in the area of requirements engineering? Based on brainstorming techniques, this paper proposes an interdisciplinary definition of scenarios, frameworks for scenario development, use and evaluation, and directions for future research.}},
    author = {Jarke, Matthias and Bui, X. Tung and Carroll, John M.},
    citeulike-article-id = {625343},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/s007660050002},
    citeulike-linkout-1 = {http://www.springerlink.com/content/5e7dty2811013mtc},
    day = {20},
    doi = {10.1007/s007660050002},
    issn = {0947-3602},
    journal = {Requirements Engineering},
    keywords = {re-w7},
    month = mar,
    number = {3},
    pages = {155--173},
    posted-at = {2012-02-20 03:32:26},
    priority = {5},
    publisher = {Springer London},
    title = {{Scenario Management: An Interdisciplinary Approach}},
    url = {http://dx.doi.org/10.1007/s007660050002},
    volume = {3},
    year = {1998}
}

@article{Selby2009AnalyticsDriven,
    abstract = {{Mining software repositories using analytics-driven dashboards provides a unifying mechanism for understanding, evaluating, and predicting the development, management, and economics of large-scale systems and processes. Dashboards enable measurement and interactive graphical displays of complex information and support flexible analytic capabilities for user customizability and extensibility. Dashboards commonly include system requirements and design metrics because they provide leading indicators for project size, growth, and volatility. This article focuses on dashboards that have been used on actual large-scale software projects as well as example empirical relationships revealed by the dashboards. The empirical results focus on leading indicators for requirements and designs of large-scale software systems based on insights from two sets of software projects containing 14 systems and 23 systems.}},
    author = {Selby, R. W.},
    booktitle = {Software, IEEE},
    citeulike-article-id = {4414528},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/ms.2009.4},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4721182},
    doi = {10.1109/ms.2009.4},
    institution = {Northrop Grumman Space Technol., Univ. of Southern California, Los Angeles, CA},
    issn = {0740-7459},
    journal = {Software, IEEE},
    keywords = {re-metrics},
    month = jan,
    number = {1},
    pages = {41--49},
    posted-at = {2012-02-16 22:36:50},
    priority = {2},
    publisher = {IEEE},
    title = {{Analytics-Driven Dashboards Enable Leading Indicators for Requirements and Designs of Large-Scale Systems}},
    url = {http://dx.doi.org/10.1109/ms.2009.4},
    volume = {26},
    year = {2009}
}

@inproceedings{Santillo2005Early,
    abstract = {{The Early \& Quick technique was originally proposed in 1997 for IFPUG Function Points, to size software in early stages of the development process, when functional requirements are still to be established in a detailed form and/or when a rapid measure is needed for existing software from a high-level viewpoint, within limited time. Typical lack of measurement details and requirements volatility in early project stages are overcome by the E\&Q approach to provide a size estimate as a significant contribution to early project planning needs. Fundamental principles of the technique are classification by analogy, functionality structured aggregation, and multilevel approach, with statistical validation of numerical ranges. Recently, the technique has evolved, to fully comply with any Functional Size Measurement method (ISO/IEC 14143:1998), so to cover new generation methods (e.g. COSMIC Full FP 2.2) and updated releases of existing methods (e.g. IFPUG FP 4.1 and 4.2). This paper describes the current technique release 2.0, application cases, validation results, supporting tools, and further improvement directions.}},
    author = {Santillo, L. and Conte, M. and Meli, R.},
    booktitle = {Software Metrics, 2005. 11th IEEE International Symposium},
    citeulike-article-id = {10360585},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/metrics.2005.18},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1509319},
    doi = {10.1109/metrics.2005.18},
    institution = {Data Processing Organization},
    isbn = {0-7695-2371-4},
    issn = {1530-1435},
    keywords = {benchmarking},
    pages = {41},
    posted-at = {2012-02-16 22:34:11},
    priority = {4},
    publisher = {IEEE},
    title = {{Early \& Quick Function Point: Sizing More with Less}},
    url = {http://dx.doi.org/10.1109/metrics.2005.18},
    year = {2005}
}

@inproceedings{McGee2009Software,
    abstract = {{Requirements changes during software development pose a risk to cost, schedule and quality while at the same time providing an opportunity to add value. Provision of a generic change source taxonomy which makes the distinction between factors contributing to requirements uncertainty and events that trigger change will support requirements change risk visibility, and also facilitate richer recording of change data. In this paper we present a collaborative study to investigate and support the management of software requirements volatility within the development lifecycle. Previously published change `causes' are elicited from the literature, consolidated using expert knowledge and classified using card sorting. The resulting change trigger taxonomy constructs were initially validated using a small set of requirements change data, and deemed sufficient and practical as a means to collect common requirements change source statistics across multiple projects.}},
    author = {McGee, S. and Greer, D.},
    booktitle = {Software Engineering Advances, 2009. ICSEA '09. Fourth International Conference on},
    citeulike-article-id = {10360582},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/icsea.2009.17},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5298458},
    comment = {(private-note)A case study was used to create a taxonomy of requirements change triggers and their related uncertainties.  The taxonomy is useful and detailed account of the actual change influences in industrial software projects particularly those that are delviered to a single customer (custom development).  This paper has several references to other taxonomies and does compare the new one with the others.

The paper develops a case study approach to define a taxonomy for why requirements change.  Project manager practitioners are involved in the case study to classify reasons for change requests to change software project reuqirements.  They develop detailed trigger events that are classified into higher level trigger categories.  The top level categories are Market, Customer Organization, Project Vision, Requirements Specification, and Solution.  Market is a response to a competitive change, a regulation or policy change, or a general change in market demand.  Customer Orgaization is initiated by a change in the customer's strategy, internal organization structure, political climate, or hardware or software configuration.  Project Vision trigger is change in business process, business case, project cost or schedule overrun, a new opportunity, change in stakeholder representative, change in stakeholder roles, or a result of participative learning.  Requirements Specification is a change due to increased customer understanding, engagement with a new user, increased developer understanding, resolution of a misunderstanding, resolution of a mis-communication, or an incorrect requirement that is identified.  A Solution trigger is the result of a change in understanding of the technical solution, new tools or technology, a deisgn or elegance improvement.},
    doi = {10.1109/icsea.2009.17},
    institution = {Sch. of Electron., Electr. Eng. \& Comput. Sci., Queens Univ., Belfast, UK},
    isbn = {978-1-4244-4779-4},
    keywords = {re, re-change},
    pages = {51--58},
    posted-at = {2012-02-16 22:33:04},
    priority = {0},
    publisher = {IEEE},
    title = {{A Software Requirements Change Source Taxonomy}},
    url = {http://dx.doi.org/10.1109/icsea.2009.17},
    year = {2009}
}

@inproceedings{Nurmuliani2004Using,
    abstract = {{Requirements volatility is considered to be a major source of risk to the management of large and complex software projects. The ability to characterise the nature and origins of requirements change during software development is important and can lead organisations towards more effective management of changing requirements. This work focuses on a study to establish how practitioners classify requirements change requests. We used the card sorting method to identify categories of change requests that software developers use in practice. Card sorting is a knowledge elicitation method that is commonly used for capturing information about different ways of representing domain knowledge. This study has allowed us to get valuable insights into the way practitioners classify change requests and to understand their perspectives on classification. This classification is a valuable source of information in prioritizing change requests and assessing their impact. Our findings from the card sorting exercise further reveal that the criteria used for categorization are related to the role the practitioner plays in the software development team and the nature and extent of their responsibilities.}},
    author = {Nurmuliani, N. and Zowghi, D. and Williams, S. P.},
    booktitle = {Requirements Engineering Conference, 2004. Proceedings. 12th IEEE International},
    citeulike-article-id = {10360580},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/icre.2004.1335681},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1335681},
    doi = {10.1109/icre.2004.1335681},
    institution = {Fac. of Inf. Technol., Technol. Univ., Sydney, NSW, Australia},
    isbn = {0-7695-2174-6},
    issn = {1090-705X},
    keywords = {re, re-change},
    pages = {240--248},
    posted-at = {2012-02-16 22:30:58},
    priority = {0},
    publisher = {IEEE},
    title = {{Using card sorting technique to classify requirements change}},
    url = {http://dx.doi.org/10.1109/icre.2004.1335681},
    year = {2004}
}

@inproceedings{Zowghi2002Study,
    abstract = {{Software development is considered to be a dynamic process where demands for changes seem to be inevitable. Modifications to software are prompted by all kinds of changes including changes to the requirements. This type of changes gives rise to an intrinsic volatility, which has several impacts on the software development lifecycle. This paper describes our findings of an extensive survey based empirical study of requirement volatility (RV) and its impact on software project performance. In particular, findings reveal that requirement volatility has a significant impact on schedule overrun and cost overrun in software projects. Our investigation also examined factors that contribute to the extent of requirement volatility and found that variables such as frequent communications between users and developers and usage of a definable methodology in requirements analysis and modeling have impact on the stability of requirements.}},
    author = {Zowghi, D. and Nurmuliani, N.},
    booktitle = {Software Engineering Conference, 2002. Ninth Asia-Pacific},
    citeulike-article-id = {3475660},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/apsec.2002.1182970},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1182970},
    doi = {10.1109/apsec.2002.1182970},
    institution = {Fac. of Inf. Technol., Univ. of Technol., Sydney, NSW, Australia},
    isbn = {0-7695-1850-8},
    issn = {1530-1362},
    keywords = {cmds, re},
    pages = {3--11},
    posted-at = {2012-02-16 22:30:06},
    priority = {2},
    publisher = {IEEE},
    title = {{A study of the impact of requirements volatility on software project performance}},
    url = {http://dx.doi.org/10.1109/apsec.2002.1182970},
    year = {2002}
}

@inproceedings{Nurmuliani2004Analysis,
    abstract = {{Investigating the factors that drive requirements change is an important prerequisite for understanding the nature of requirements volatility. This increased understanding will improve the process of requirements change management. We mainly focus on change analysis to identify and characterize the causes of requirements volatility. We apply a causal analysis method on change request data to develop a taxonomy of change. This taxonomy allows us to identify and trace the problems, reasons and sources of changes. Adopting an industrial case study approach, our findings reveal that the main causes of requirements volatility were changes in customer needs (or market demands), developers' increased understanding of the products, and changes in the organization policy. During the development process, we also examined the extent of requirements volatility and discovered that the rate of volatility was high at the time of requirements specification completion and while functional specification reviews were conducted.}},
    author = {Nurmuliani, N. and Zowghi, D. and Powell, S.},
    booktitle = {2004 Australian Software Engineering Conference. Proceedings.},
    citeulike-article-id = {776819},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/aswec.2004.1290455},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1290455},
    doi = {10.1109/aswec.2004.1290455},
    institution = {Univ. of Technol., Sydney, NSW, Australia},
    isbn = {0-7695-2089-8},
    journal = {Software Engineering Conference, 2004. Proceedings. 2004 Australian},
    keywords = {re-change},
    location = {Melbourne, Vic., Australia},
    pages = {28--37},
    posted-at = {2012-02-16 22:29:29},
    priority = {0},
    publisher = {IEEE},
    title = {{Analysis of requirements volatility during software development life cycle}},
    url = {http://dx.doi.org/10.1109/aswec.2004.1290455},
    year = {2004}
}

@inproceedings{Thakurta2010Understanding,
    abstract = {{Requirements volatility during software project development is known to be the most critical risk, and managing this is paramount to success in software project. The research described in this paper is based on a combination of interviews and a survey in two phases and aims to investigate the organizational practices in dealing with this risk, and how it is influenced by the adopted project execution strategy with regard to process model selection decisions. The results indicate study participants' heightened perception of the risk of requirements volatility. Thirteen different approaches to managing projects under volatility could be identified, of which the practice of involving the business side was the most frequent. Differences could be observed in the usage of these approaches based on the chosen project execution strategy. The influence of business emerged as the highest determinant of the seven identified factors governing execution strategy selection. The variation in these factors under volatility has also been pointed out. The current practice regarding usage of these execution frameworks reveals a gradual shift towards customization. In this regard some incongruence between perception and practice under requirements volatility was also evident, and has been addressed in the paper. The study findings are expected to assist project managers in their choices related to project administration under requirements volatility.}},
    author = {Thakurta, R. and Ahlemann, F.},
    booktitle = {System Sciences (HICSS), 2010 43rd Hawaii International Conference on},
    citeulike-article-id = {10360578},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/hicss.2010.420},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5428606},
    doi = {10.1109/hicss.2010.420},
    institution = {Indian Inst. of Manage. Calcutta, Kolkata, India},
    isbn = {978-1-4244-5509-6},
    issn = {1530-1605},
    keywords = {re-change},
    month = jan,
    pages = {1--10},
    posted-at = {2012-02-16 22:27:52},
    priority = {0},
    publisher = {IEEE},
    title = {{Understanding Requirements Volatility in Software Projects - An Empirical Investigation of Volatility Awareness, Management Approaches and their Applicability}},
    url = {http://dx.doi.org/10.1109/hicss.2010.420},
    year = {2010}
}

@article{Nemati2002Knowledge,
    abstract = {{Decision support systems (DSS) are becoming increasingly more critical to the daily operation of organizations. Data warehousing, an integral part of this, provides an infrastructure that enables businesses to extract, cleanse, and store vast amounts of data. The basic purpose of a data warehouse is to empower the knowledge workers with information that allows them to make decisions based on a solid foundation of fact. However, only a fraction of the needed information exists on computers; the vast majority of a firm's intellectual assets exist as knowledge in the minds of its employees. What is needed is a new generation of knowledge-enabled systems that provides the infrastructure needed to capture, cleanse, store, organize, leverage, and disseminate not only data and information but also the knowledge of the firm. The purpose of this paper is to propose, as an extension to the data warehouse model, a knowledge warehouse (KW) architecture that will not only facilitate the capturing and coding of knowledge but also enhance the retrieval and sharing of knowledge across the organization. The knowledge warehouse proposed here suggests a different direction for DSS in the next decade. This new direction is based on an expanded purpose of DSS. That is, the purpose of DSS in knowledge improvement. This expanded purpose of DSS also suggests that the effectiveness of a DSS will, in the future, be measured based on how well it promotes and enhances knowledge, how well it improves the mental model(s) and understanding of the decision maker(s) and thereby how well it improves his/her decision making.}},
    author = {Nemati, Hamid R. and Steiger, David M. and Iyer, Lakshmi S. and Herschel, Richard T.},
    citeulike-article-id = {10360393},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/s0167-9236(01)00141-5},
    doi = {10.1016/s0167-9236(01)00141-5},
    issn = {01679236},
    journal = {Decision Support Systems},
    keywords = {re-metrics},
    month = jun,
    number = {2},
    pages = {143--161},
    posted-at = {2012-02-16 20:02:37},
    priority = {2},
    title = {{Knowledge warehouse: an architectural integration of knowledge management, decision support, artificial intelligence and data warehousing}},
    url = {http://dx.doi.org/10.1016/s0167-9236(01)00141-5},
    volume = {33},
    year = {2002}
}

@incollection{Vassiliadis1999Towards,
    abstract = {{As a decision support information system, a data warehouse must provide high level quality of data and quality of service. In the DWQ project we have proposed an architectural framework and a repository of metadata which describes all the data warehouse components in a set of metamodels to which is added a quality metamodel, defining for each data warehouse metaobject the corresponding relevant quality dimensions and quality factors. Apart from this static definition of quality, we also provide an operational complement, that is a methodology on how to use quality factors and to achieve user quality goals. This methodology is an extension of the Goal-Question-Metric (GQM) approach, which allows to capture (a) the inter-relationships between different quality factors and (b) to organize them in order to fulfil specific quality goals. After summarizing the DWQ quality model, this paper describes the methodology we propose to use this quality model, as well as its impact on the data warehouse evolution.}},
    address = {Berlin, Heidelberg},
    author = {Vassiliadis, Panos and Bouzeghoub, Mokrane and Quix, Christoph},
    booktitle = {Advanced Information Systems Engineering},
    chapter = {13},
    citeulike-article-id = {10360390},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/3-540-48738-7\_13},
    citeulike-linkout-1 = {http://www.springerlink.com/content/cxra9jc6mpknqmm4},
    day = {11},
    doi = {10.1007/3-540-48738-7\_13},
    editor = {Jarke, Matthias and Oberweis, Andreas},
    isbn = {978-3-540-66157-3},
    keywords = {re-metrics},
    month = jun,
    pages = {164--179},
    posted-at = {2012-02-16 19:59:15},
    priority = {2},
    publisher = {Springer Berlin / Heidelberg},
    series = {Lecture Notes in Computer Science},
    title = {{Towards Quality-Oriented Data Warehouse Usage and Evolution Advanced Information Systems Engineering}},
    url = {http://dx.doi.org/10.1007/3-540-48738-7\_13},
    volume = {1626},
    year = {1999}
}

@incollection{Mazon2007ModelDriven,
    abstract = {{The development of a data warehouse has been traditionally guided by an in-depth analysis of the underlying operational data sources, thus overlooking an explicit development phase in which information requirements of decision makers are addressed. This scenario has prompted that the deployed data warehouse often fails in delivering the expected support of the decision making process. To overcome this problem, we propose to use the i* modeling framework and the model driven architecture (MDA) in order to describe (i) how to model goals and information requirements for data warehouses, and (ii) how to derive a conceptual multidimensional model that provides the required information to support the decision making process.}},
    address = {Berlin, Heidelberg},
    author = {Maz\'{o}n, Jose-Norberto and Pardillo, Jes\'{u}s and Trujillo, Juan},
    booktitle = {Advances in Conceptual Modeling – Foundations and Applications},
    chapter = {31},
    citeulike-article-id = {8855081},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/978-3-540-76292-8\_31},
    citeulike-linkout-1 = {http://www.springerlink.com/content/32425720h6210157},
    doi = {10.1007/978-3-540-76292-8\_31},
    editor = {Hainaut, Jean-Luc and Rundensteiner, Elke A. and Kirchberg, Markus and Bertolotto, Michela and Brochhausen, Mathias and Chen, Yi-Ping P. and Cherfi, Samira S. and Doerr, Martin and Han, Hyoil and Hartmann, Sven and Parsons, Jeffrey and Poels, Geert and Rolland, Colette and Trujillo, Juan and Yu, Eric and Zim\'{a}nyie, Esteban},
    isbn = {978-3-540-76291-1},
    keywords = {re-metrics},
    pages = {255--264},
    posted-at = {2012-02-16 19:57:26},
    priority = {2},
    publisher = {Springer Berlin / Heidelberg},
    series = {Lecture Notes in Computer Science},
    title = {{A Model-Driven Goal-Oriented Requirement Engineering Approach for Data Warehouses Advances in Conceptual Modeling – Foundations and Applications}},
    url = {http://dx.doi.org/10.1007/978-3-540-76292-8\_31},
    volume = {4802},
    year = {2007}
}

@inproceedings{Paim2003DWARF,
    abstract = {{In the novel domain of data warehouse systems, software engineers are required to define a solution that integrates with a number of heterogeneous sources to extract, transform and aggregate data, as well as to offer flexibility to run adhoc queries that retrieve analytic information. Moreover, these activities should be performed based on a concise dimensional schema. This intricate process with its particular multidimensionality claims for a requirements engineering approach to aid the precise definition of data warehouse applications. We adapt the traditional requirements engineering process and propose DWARF, a data warehouse requirements definition method. A case study demonstrates how the method has been successfully applied in the company wise development of a large-scale data warehouse system that stores hundreds of gigabytes of strategic data for the Brazilian Federal Revenue Service.}},
    author = {Paim, F. R. S. and de Castro, J. F. B.},
    booktitle = {Requirements Engineering Conference, 2003. Proceedings. 11th IEEE International},
    citeulike-article-id = {832376},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/icre.2003.1232739},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1232739},
    doi = {10.1109/icre.2003.1232739},
    institution = {SERPRO, Recife, Brazil},
    isbn = {0-7695-1980-6},
    issn = {1090-705X},
    journal = {Requirements Engineering Conference, 2003. Proceedings. 11th IEEE International},
    keywords = {re-metrics},
    pages = {75--84},
    posted-at = {2012-02-16 19:56:13},
    priority = {3},
    publisher = {IEEE},
    title = {{DWARF: an approach for requirements definition and management of data warehouse systems}},
    url = {http://dx.doi.org/10.1109/icre.2003.1232739},
    year = {2003}
}

@electronic{RobertBrucknerDeveloping,
    author = {Robert Bruckner, Beate L.},
    citeulike-article-id = {10360382},
    citeulike-linkout-0 = {http://www.ifs.tuwien.ac.at/\~{}js/download/rc22042.pdf},
    keywords = {re-metrics},
    posted-at = {2012-02-16 19:50:06},
    priority = {2},
    title = {{Developing Requirements for Data Warehouse Systems with Use Cases}},
    url = {http://www.ifs.tuwien.ac.at/\~{}js/download/rc22042.pdf}
}

@inproceedings{Winter2003Method,
    abstract = {{Information requirements analysis for data warehouse systems differs significantly from requirements analysis for conventional information systems. Existing data warehouse specific approaches are reviewed. A comprehensive methodology that supports the entire process of determining information requirements of data warehouse users, matching information requirements with actual information supply, evaluating and homogenizing resulting information requirements, establishing priorities for unsatisfied information requirements, and formally specifying the results as a basis for subsequent phases of the data warehouse development (sub)project is proposed. Its components as well as its overall design are based partially on literature review, but mainly on findings from a four year collaboration project with several large companies, mostly from the service sector. While an application of the entire methodology is still outstanding, some components have been successfully applied in actual data warehouse development projects of the participating companies.}},
    author = {Winter, R. and Strauch, B.},
    booktitle = {System Sciences, 2003. Proceedings of the 36th Annual Hawaii International Conference on},
    citeulike-article-id = {10360379},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/hicss.2003.1174602},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1174602},
    doi = {10.1109/hicss.2003.1174602},
    institution = {Inst. of Inf. Manage., St. Gallen Univ., Switzerland},
    isbn = {0-7695-1874-5},
    keywords = {re-metrics},
    month = jan,
    pages = {9 pp.+},
    posted-at = {2012-02-16 19:47:39},
    priority = {4},
    publisher = {IEEE},
    title = {{A method for demand-driven information requirements analysis in data warehousing projects}},
    url = {http://dx.doi.org/10.1109/hicss.2003.1174602},
    year = {2003}
}

@electronic{Experiences,
    citeulike-article-id = {10351121},
    citeulike-linkout-0 = {http://repository.cmu.edu/sei/641/},
    keywords = {metrics},
    posted-at = {2012-02-14 17:18:34},
    priority = {4},
    title = {{"Experiences in Implementing Measurement Programs" by Wolfhart Goethert and William Hayes}},
    url = {http://repository.cmu.edu/sei/641/}
}

@electronic{Advances,
    citeulike-article-id = {10350576},
    citeulike-linkout-0 = {http://books.google.com/books?hl=en\&\#38;lr=lang\_en\&\#38;id=eFGi\_414Y-cC\&\#38;oi=fnd\&\#38;pg=PA25\&\#38;dq=software+technical+debt\&\#38;ots=0YO0oFJav\_\&\#38;sig=OxCsqiiXY7e0cpuUUOSp\_CCnzA4\#v=onepage\&\#38;q=software\%20technical\%20debt\&\#38;f=false},
    keywords = {td},
    posted-at = {2012-02-14 12:08:17},
    priority = {2},
    title = {{Advances in Computers - Marvin Zelkowitz - Google Books}},
    url = {http://books.google.com/books?hl=en\&\#38;lr=lang\_en\&\#38;id=eFGi\_414Y-cC\&\#38;oi=fnd\&\#38;pg=PA25\&\#38;dq=software+technical+debt\&\#38;ots=0YO0oFJav\_\&\#38;sig=OxCsqiiXY7e0cpuUUOSp\_CCnzA4\#v=onepage\&\#38;q=software\%20technical\%20debt\&\#38;f=false}
}

@inproceedings{Guo2011Tracking,
    abstract = {{The technical debt metaphor is increasingly being used to describe the effect of delaying certain software maintenance tasks on software projects. Practitioners understand intuitively how technical debt can turn into a serious problem if it is left unattended. However, it remains unknown how serious the problem is and whether explicit measurement and management of technical debt is useful. In this paper, we explore the effect of technical debt by tracking a single delayed maintenance task in a real software project throughout its lifecycle and simulate how explicit technical debt management might have changed project outcomes. The results from this study demonstrate how and to what extent technical debt affects software projects. The study also sheds light on the research methodologies that can be used to investigate the technical debt management problem.}},
    author = {Guo, Yuepu and Seaman, C. and Gomes, R. and Cavalcanti, A. and Tonin, G. and Da Silva, F. Q. B. and Santos, A. L. M. and Siebra, C.},
    booktitle = {Software Maintenance (ICSM), 2011 27th IEEE International Conference on},
    citeulike-article-id = {10350571},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/icsm.2011.6080824},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6080824},
    doi = {10.1109/icsm.2011.6080824},
    institution = {Dept. of Inf. Syst., Univ. of Maryland Baltimore County, Baltimore, MD, USA},
    isbn = {978-1-4577-0663-9},
    issn = {1063-6773},
    keywords = {td},
    pages = {528--531},
    posted-at = {2012-02-14 12:03:09},
    priority = {2},
    publisher = {IEEE},
    title = {{Tracking technical debt — An exploratory case study}},
    url = {http://dx.doi.org/10.1109/icsm.2011.6080824},
    year = {2011}
}

@inproceedings{Kaiser2011Selling,
    abstract = {{Extreme programming (XP) techniques and other advances in software development allow for creation of a code base which is more easily understood and maintained. This often leads to faster enhancements, at a lower cost, and with fewer defects. However, when these practices are not followed, problems within the code base can manifest themselves as technical debt. The following report details the experience of one team in trying to influence toward greater understanding in and investment toward their technical debt problem by making that problem visible. The team hangs large representations of their code base in the walkway by their desks. The charts create conversation in and around the team and eventually shows results.}},
    author = {Kaiser, M. and Royse, G.},
    booktitle = {AGILE Conference (AGILE), 2011},
    citeulike-article-id = {10350570},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/agile.2011.50},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6005521},
    doi = {10.1109/agile.2011.50},
    institution = {Centric Consulting, Columbus, OH, USA},
    isbn = {978-1-61284-426-8},
    keywords = {td},
    month = aug,
    pages = {175--180},
    posted-at = {2012-02-14 12:02:29},
    priority = {2},
    publisher = {IEEE},
    title = {{Selling the Investment to Pay Down Technical Debt: The Code Christmas Tree}},
    url = {http://dx.doi.org/10.1109/agile.2011.50},
    year = {2011}
}

@inproceedings{Ozkaya2011Second,
    abstract = {{The technical debt metaphor is gaining significant traction in the software development community as a way to understand and communicate issues of intrinsic quality, value, and cost. The idea is that developers sometimes accept compromises in a system in one dimension (e.g., modularity) to meet an urgent demand in some other dimension (e.g., a deadline), and that such compromises incur a "debt": on which "interest" has to be paid and which should be repaid at some point for the long-term health of the project. Little is known about technical debt, beyond feelings and opinions. The software engineering research community has an opportunity to study this phenomenon and improve the way it is handled. We can offer software engineers a foundation for managing such trade-offs based on models of their economic impacts. The goal of this second workshop is to discuss managing technical debt as a part of the research agenda for the software engineering field.}},
    address = {New York, NY, USA},
    author = {Ozkaya, Ipek and Kruchten, Philippe and Nord, Robert and Brown, Nanette},
    booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
    citeulike-article-id = {10350566},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1986051},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1985793.1986051},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6032641},
    doi = {10.1145/1985793.1986051},
    institution = {Carnegie Mellon University, Pittsburgh, PA, USA},
    isbn = {978-1-4503-0445-0},
    issn = {0270-5257},
    keywords = {td},
    location = {Waikiki, Honolulu, HI, USA},
    month = may,
    pages = {1212--1213},
    posted-at = {2012-02-14 11:56:26},
    priority = {2},
    publisher = {ACM},
    series = {ICSE '11},
    title = {{Second international workshop on managing technical debt (MTD 2011)}},
    url = {http://dx.doi.org/10.1145/1985793.1986051},
    year = {2011}
}

@article{Buschmann2011To,
    abstract = {{Ward Cunningham coined the term technical debt as a metaphor for the trade-off between writing clean code at higher cost and delayed de livery, and writing messy code cheap and fast at the cost of higher maintenance efforts once it's shipped. Joshua Kerievsky extended the metaphor to architecture and design. Technical debt is similar to financial debt: it supports quick development at the cost of compound interest to be paid later. The longer we wait to garden our design and code, the larger the amount of interest. Discussions of the metaphor have distinguished different types of technical debt and how and when to best pay them off. Most agree that, sooner or later, technical debt will come due. But is this assumption universally true? If it's better to pay interest, what factors influence the decision to service the debt? And if we decide to retire it, what approach should we take?}},
    address = {Los Alamitos, CA, USA},
    author = {Buschmann, F.},
    citeulike-article-id = {10316452},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=2068549},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/ms.2011.150},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6055661},
    doi = {10.1109/ms.2011.150},
    issn = {0740-7459},
    journal = {Software, IEEE},
    keywords = {td},
    month = nov,
    number = {6},
    pages = {29--31},
    posted-at = {2012-02-14 11:53:31},
    priority = {4},
    publisher = {IEEE},
    title = {{To Pay or Not to Pay Technical Debt}},
    url = {http://dx.doi.org/10.1109/ms.2011.150},
    volume = {28},
    year = {2011}
}

@inproceedings{Brown2010Managing,
    abstract = {{Delivering increasingly complex software-reliant systems demands better ways to manage the long-term effects of short-term expedients. The technical debt metaphor is gaining significant traction in the agile development community as a way to understand and communicate such issues. The idea is that developers sometimes accept compromises in a system in one dimension (e.g., modularity) to meet an urgent demand in some other dimension (e.g., a deadline), and that such compromises incur a "debt": on which "interest" has to be paid and which the "principal" should be repaid at some point for the long-term health of the project. We argue that the software engineering research community has an opportunity to study and improve this concept. We can offer software engineers a foundation for managing such trade-offs based on models of their economic impacts. Therefore, we propose managing technical debt as a part of the future research agenda for the software engineering field.}},
    address = {New York, NY, USA},
    author = {Brown, Nanette and Cai, Yuanfang and Guo, Yuepu and Kazman, Rick and Kim, Miryung and Kruchten, Philippe and Lim, Erin and MacCormack, Alan and Nord, Robert and Ozkaya, Ipek and Sangwan, Raghvinder and Seaman, Carolyn and Sullivan, Kevin and Zazworka, Nico},
    booktitle = {Proceedings of the FSE/SDP workshop on Future of software engineering research},
    citeulike-article-id = {8531446},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1882373},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1882362.1882373},
    doi = {10.1145/1882362.1882373},
    isbn = {978-1-4503-0427-6},
    keywords = {td},
    location = {Santa Fe, New Mexico, USA},
    pages = {47--52},
    posted-at = {2012-02-14 11:40:15},
    priority = {4},
    publisher = {ACM},
    series = {FoSER '10},
    title = {{Managing technical debt in software-reliant systems}},
    url = {http://dx.doi.org/10.1145/1882362.1882373},
    year = {2010}
}

@inproceedings{Yu1997Towards,
    abstract = {{Requirements are usually understood as stating what a system is
supposed to do, as apposed to how it should do it. However,
understanding the organizational context and rationales (the
 ” Whys”) that lead up to systems requirements can be just as
important for the ongoing success of the system. Requirements modelling
techniques can be used to help deal with the knowledge and reasoning
needed in this earlier phase of requirements engineering. However most
existing requirements techniques are intended more for the later phase
of requirements engineering, which focuses on completeness, consistency,
and automated verification of requirements. In contrast, the early phase
aims to model and analyze stakeholder interests and how they might be
addressed, or compromised, by various system-and-environment
alternatives. This paper argues, therefore, that a different kind of
modelling and reasoning support is needed for the early phase. An
outline of the i* framework is given as an example of a step in this
direction. Meeting scheduling is used as a domain example}},
    author = {Yu, E. S. K.},
    booktitle = {Requirements Engineering, 1997., Proceedings of the Third IEEE International Symposium on},
    citeulike-article-id = {166377},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/isre.1997.566873},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=566873},
    comment = {(private-note)The paper contirbutes an evaluation of Early Phase requirements modeling using the i* technique and makes the case that different techniques are applicable in early phase versus late phase requirements engienering.  The paper provides a detailed example of applying the i* technique to a meeting scheduler system.  One research idea from this paper would be to evaluate whehter early modeling work can achieve traceability to later phase requirements models.},
    doi = {10.1109/isre.1997.566873},
    institution = {Fac. of Inf. Studies, Toronto Univ., Ont.},
    isbn = {0-8186-7740-6},
    journal = {Requirements Engineering, 1997., Proceedings of the Third IEEE International Symposium on},
    keywords = {re-w6},
    location = {Annapolis, MD, USA},
    month = jan,
    pages = {226--235},
    posted-at = {2012-02-13 01:11:52},
    priority = {5},
    publisher = {IEEE},
    title = {{Towards modelling and reasoning support for early-phase
requirements engineering}},
    url = {http://dx.doi.org/10.1109/isre.1997.566873},
    year = {1997}
}

@inproceedings{Anton1996Goalbased,
    abstract = {{Goals are a logical mechanism for identifying, organizing and
justifying software requirements. Strategies are needed for the initial
identification and construction of goals. We discuss goals from the
perspective of two themes: goal analysis and goal evolution. We begin
with an overview of the goal-based method we have developed and
summarize our experiences in applying our method to a relatively large
example. We illustrate some of the issues that practitioners face when
using a goal-based approach to specify the requirements for a system and
close the paper with a discussion of needed future research on
goal-based requirements analysis and evolution}},
    author = {Anton, A. I.},
    booktitle = {Requirements Engineering, 1996., Proceedings of the Second International Conference on},
    citeulike-article-id = {10345619},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/icre.1996.491438},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=491438},
    comment = {(private-note)The article dicusses the lessons learned by applying a goal-based requirements analysis method (GBRAM) to a subject system and environment.  The paper discusses the enumeration of goals, applying scenarios (including blocking scenarios) to further elaborate requirements, and refining the goals to reduce their number and tighten the overall scope.  The strength of this paper is in the demonstration of the process by which goals are developed and refined in a real-world situation.  A weakness is that the nascent method was not further applied to other situations to expand the validation.  My opportunity for futher research would be to explore the use of goal-based requirements analysis method in defining a system for reporting software project metrics that were defined using GQM.},
    doi = {10.1109/icre.1996.491438},
    institution = {Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA},
    isbn = {0-8186-7252-8},
    keywords = {re-w6},
    month = apr,
    pages = {136--144},
    posted-at = {2012-02-13 01:09:59},
    priority = {5},
    publisher = {IEEE},
    title = {{Goal-based requirements analysis}},
    url = {http://dx.doi.org/10.1109/icre.1996.491438},
    year = {1996}
}

@electronic{Unsustainable,
    citeulike-article-id = {10325242},
    citeulike-linkout-0 = {http://www.informit.com/articles/article.aspx?p=422306},
    keywords = {td},
    posted-at = {2012-02-08 02:50:22},
    priority = {2},
    title = {{Unsustainable Development}},
    url = {http://www.informit.com/articles/article.aspx?p=422306}
}

@electronic{StroudKano,
    author = {Stroud, J. DeLayne},
    citeulike-article-id = {10325046},
    citeulike-linkout-0 = {http://www.isixsigma.com/tools-templates/kano-analysis/kano-analysis-customer-needs-are-ever-changing/},
    keywords = {l6s, re},
    posted-at = {2012-02-08 02:45:50},
    priority = {2},
    title = {{The Kano Analysis: Customer Needs Are Ever Changing}},
    url = {http://www.isixsigma.com/tools-templates/kano-analysis/kano-analysis-customer-needs-are-ever-changing/}
}

@inproceedings{Ebert2005Requirements,
    abstract = {{Practically all industry studies on software project results conclude that good requirements engineering plays a pivotal role for successful projects. A key reason for project failures is insufficient management of changing requirements during all stages of the project life cycle. This article investigates one of the root causes for changing requirements, namely requirements uncertainty. In an experimental field study we looked into four underlying drivers for requirements uncertainty. We found several techniques must be used simultaneously to see tangible success. Using only one such technique in isolation doesn't make a difference. The field study is supported by extensive data from well over 200 projects stemming from very different business areas of Alcatel over a period of two years. Results are presented with practical experiences to allow effective transfer.}},
    address = {New York, NY, USA},
    author = {Ebert, Christof and De Man, Jozef},
    booktitle = {Proceedings of the 27th international conference on Software engineering},
    citeulike-article-id = {321639},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1062554},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1062455.1062554},
    doi = {10.1145/1062455.1062554},
    isbn = {1-58113-963-2},
    keywords = {cmds, re-change},
    location = {St. Louis, MO, USA},
    pages = {553--560},
    posted-at = {2012-02-08 02:35:49},
    priority = {0},
    publisher = {ACM},
    series = {ICSE '05},
    title = {{requirements uncertainty: influencing factors and concrete improvements}},
    url = {http://dx.doi.org/10.1145/1062455.1062554},
    year = {2005}
}

@inproceedings{Lam1999Managing,
    abstract = {{Software evolves and managing software evolution, in particular
requirements evolution, is necessary to sustain the effectiveness of the
system to meet the changing needs of its users. Measurement is central
to underpinning sound and rational management decisions. However, few
measurement schemes or metrics have been proposed that can aid software
managers manage requirements change and evolution. This paper addresses
this problem, and proposes a measurement-action framework based on case
study experience with a commercial software organisation. The
measurement-action framework is organised around four main areas of
concern: planning for change; assessing impact of change; determining
changeability; and assessing effectiveness at handling change. The
framework provides a set of indicators that can assist managers in
measuring requirements change, and importantly, generic action plans
that provide managers with practical guidance on how to use the
indicators as a basis for taking managerial action. The paper describes
the measurement-action framework in detail, and comments on the general
applicability of the work}},
    author = {Lam, W. and Loomes, M. and Shankararaman, V.},
    booktitle = {Software Maintenance and Reengineering, 1999. Proceedings of the Third European Conference on},
    citeulike-article-id = {10324848},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/csmr.1999.756689},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=756689},
    doi = {10.1109/csmr.1999.756689},
    institution = {Dept. of Comput. Sci., Hertfordshire Univ., Hatfield},
    isbn = {0-7695-0090-0},
    keywords = {cmds, re},
    pages = {122--128},
    posted-at = {2012-02-08 02:34:23},
    priority = {3},
    publisher = {IEEE},
    title = {{Managing requirements change using metrics and action planning}},
    url = {http://dx.doi.org/10.1109/csmr.1999.756689},
    year = {1999}
}

@article{Chudge1996Trust,
    abstract = {{Although some progress has been made in recent years towards
developing more effective methods for eliciting and representing
requirements for software systems, little progress has been made towards
developing tools and techniques that address the impact of changing
requirements or of proposing changes to the a priori processes and
structures for requirements analysis that dominate current systems
development practice. The paper presents some of the interim results
from PROTEUS, a DTI Project looking at requirements change practice in
British industry, with a particular emphasis on safety related system
developments. Two sets of case study results, one drawn from
collaboration with an industrial partner and the other looking at
requirements handling experiences in a range of companies, present a
disturbing picture of the state of requirements change practice. The
results point to predominantly social, rather than technical, problems
primarily in the professional relationships between client and developer
on software development projects. The paper concludes by arguing that
contractual relationships on fixed cost projects, the lack of trust
between partners, and the overwhelming level of software control
required to handle change all marginalise the opportunity to create
efficient software systems}},
    author = {Chudge, J. and Fulton, D.},
    citeulike-article-id = {10324824},
    citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=503679},
    institution = {Husat Res. \& Consultancy, Loughborough},
    issn = {0268-6961},
    journal = {Software Engineering Journal},
    keywords = {cmds, re-change},
    month = may,
    number = {3},
    pages = {193--204},
    posted-at = {2012-02-08 02:23:27},
    priority = {0},
    publisher = {IET},
    title = {{Trust and co-operation in system development: applying
responsibility modelling to the problem of changing requirements}},
    url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=503679},
    volume = {11},
    year = {1996}
}

@inproceedings{Datta2006Effects,
    abstract = {{Managing the effects of changing requirements remains one of the greatest challenges of enterprise software development. The iterative and incremental model provides an expedient framework for addressing such concerns. This paper presents a set of metrics - Mutation Index, Component Set, Dependency Index - and a methodology to measure the effects of requirement changes in the analysis workflow from one iteration to another. Results from a sample case study are included to highlight a usage scenario. Future directions of our work based on this mechanism are also discussed.}},
    address = {New York, NY, USA},
    author = {Datta, Subhajit and van Engelen, Robert},
    booktitle = {Proceedings of the 2006 ACM symposium on Applied computing},
    citeulike-article-id = {10324820},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1141689},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1141277.1141689},
    doi = {10.1145/1141277.1141689},
    isbn = {1-59593-108-2},
    keywords = {cmds, re},
    location = {Dijon, France},
    pages = {1739--1744},
    posted-at = {2012-02-08 02:20:05},
    priority = {4},
    publisher = {ACM},
    series = {SAC '06},
    title = {{Effects of changing requirements: a tracking mechanism for the analysis workflow}},
    url = {http://dx.doi.org/10.1145/1141277.1141689},
    year = {2006}
}

@inproceedings{ONeal2001Analyzing,
    abstract = {{Determining the impact of requirement changes on software
development is critical to project management. We present an impact
analysis method to evaluate requirement changes for software development
projects that is based on requirements traceability. By using attributes
of the work products and traces, we create classes of requirement
changes prioritized according to the potential impact. We present a case
study that shows a favorable comparison between the actual impact and
the predicted impact. Finally, we discuss the expansion of the method}},
    author = {O'Neal, J. S. and Carver, D. L.},
    booktitle = {Software Maintenance, 2001. Proceedings. IEEE International Conference on},
    citeulike-article-id = {10324812},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/icsm.2001.972729},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=972729},
    comment = {(private-note)This paper seeks to determine the impact of a change in terms of the amount of effort required to modify completed work artifacts in the project.    The work only evaluates the "rework" portion of the effort excluding the effort to implement the new portion of the requirement from the effort to rework existing items.  Thus it is expressing the "wasted" effort impact of changing requirements.},
    doi = {10.1109/icsm.2001.972729},
    institution = {Dept. of Comput. Sci., Louisiana State Univ., Baton Rouge, LA},
    isbn = {0-7695-1189-9},
    keywords = {cmds, re-change},
    pages = {190--195},
    posted-at = {2012-02-08 02:18:18},
    priority = {0},
    publisher = {IEEE},
    title = {{Analyzing the impact of changing requirements}},
    url = {http://dx.doi.org/10.1109/icsm.2001.972729},
    year = {2001}
}

@inproceedings{Strens1996Change,
    abstract = {{Changing requirements are a major source of risk to software development projects, but are generally not included in hazard identification and risk assessment procedures at present because the information needed is not collected or recorded by current requirements methods. It is proposed that change analysis should be an integral part of the entire development process so that change can be handled in a better-informed way with the associated risks being made apparent. Sensitivity analysis is proposed as the principal means of predicting two categories of hazard: which requirements are unstable and which design areas are most susceptible to changes in requirements. Impact analysis supports the decision-making process concerning the implementation of changes, and is also used to predict the impact of potential change for the assessment of risk. The paper considers what information is needed to enable sensitivity and impact analysis to provide an effective means of change analysis and what methods provide such information, emphasizing the importance of traceability and the capture of design rationale. Because the potential for change would be made apparent early in the lifecycle the outcome should be a substantial contribution towards the goal of minimizing the adverse impact of changing requirements on the project objectives.}},
    address = {Washington, DC, USA},
    author = {Strens, M. R. and Sugden, R. C.},
    booktitle = {Proceedings of the IEEE Symposium and Workshop on Engineering of Computer Based Systems},
    citeulike-article-id = {10324799},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=787291},
    isbn = {0-8186-7355-9},
    keywords = {cmds, re},
    posted-at = {2012-02-08 02:15:34},
    priority = {5},
    publisher = {IEEE Computer Society},
    series = {ECBS '96},
    title = {{Change Analysis: A Step towards Meeting the Challenge of Changing Requirements}},
    url = {http://portal.acm.org/citation.cfm?id=787291},
    year = {1996}
}

@article{Serrano2002Validating,
    abstract = {{Organisations are adopting data warehouses to manage information efficiently as the main organisational asset. This success of data warehouses (DW) can be explained because a data warehouse is a set of data and technologies aimed at enabling the executives, managers and analysts to make better and faster decisions. Due to the principal role of data warehouses in taking strategic decisions, quality is fundamental. One of the most important factors that affects the quality of the final system is its design. Although in recent years different authors have proposed some useful guidelines to design a data warehouse, more objective indicators are needed to help designers and managers to develop quality data warehouses. A set of metrics for data warehouse models is presented, and an empirical validation is carried out in order to prove practically their usefulness as quality indicators.}},
    author = {Serrano, M. and Calero, C. and Piattini, M.},
    citeulike-article-id = {10312316},
    citeulike-linkout-0 = {http://dx.doi.org/10.1049/ip-sen:20020697},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1049206},
    doi = {10.1049/ip-sen:20020697},
    institution = {Alarcos Res. Group, Castilla Univ., Ciudad Real},
    issn = {1462-5970},
    journal = {Software, IEE Proceedings -},
    keywords = {cmds, metrics},
    month = oct,
    number = {5},
    pages = {161--166},
    posted-at = {2012-02-03 21:48:07},
    priority = {3},
    publisher = {IET},
    title = {{Validating metrics for data warehouses}},
    url = {http://dx.doi.org/10.1049/ip-sen:20020697},
    volume = {149},
    year = {2002}
}

@inproceedings{Hickey2003Elicitation,
    abstract = {{Requirements elicitation techniques are methods used by analysts to determine the needs of customers and users, so that systems can be built with a high probability of satisfying those needs. Analysts with extensive experience seem to be more successful than less experienced analysts in uncovering the user needs. Less experienced analysts often select a technique based on one of two reasons: (a) it is the only one they know, or (b) they think that a technique that worked well last time must surely be appropriate this time. We present the results of in-depth interviews with some of the world's most experienced analysts. These results demonstrate how they select elicitation techniques based on a variety of situational assessments.}},
    author = {Hickey, A. M. and Davis, A. M.},
    booktitle = {Requirements Engineering Conference, 2003. Proceedings. 11th IEEE International},
    citeulike-article-id = {7205820},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/icre.2003.1232748},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1232748},
    comment = {(private-note)The paper contributes a study of methods experts used when performing requirements elicitation and under what conditions or problems to use each of them.  A panel of 9 experts were interviewed with both open-ended questions and structured scenarios of requirements.  Identified technques are as follows:
- Collaborative sessions that gather stakeholders together are a default approach
- Interviewing used to identify conflicts that need to be addressed between stakeholders
- Team-Building  is a sub-technique to raise team comfort levels with each other
- Ethnography (observation) is the second most popular technique
- Issues List keeps a parking lot that gets addressed by volunteer stakeholders
- Models is the most popular include data flow diagrams, ER diagrams, statecharts, or "UML"
- Questionnaires only when problems are concrete or in marketing surveys
- Data from Existing Systems to determine issues or constraints
- Requirements Categorization using Volere template plus a dimension of requirement criticality
- Conflict Awareness and Resolution establish a stakeholder map prior to eliciting requirements
- Prototyping only when it can be "rapid" was even then not popular
- Role Playing especially when stakehoders are inaccessible (terrorist)
- Formal Methods should be reserved for a later step
- Extreme Programming (omni-present customer) NO BDUF was not popular

The weakness of this paper is in organization and wordiness I think it was section 6 before the information in the paper began to be conveyed.  It also lacks a useful organization of the findings (by technique) and lacks organized conclusions such as a comparison table. I would take this idea of the scenario prompted study and develop it so that experts can provide stronger guidance (perhaps reach a consensus) on techniques to apply in different scenarios.},
    doi = {10.1109/icre.2003.1232748},
    institution = {Dept. of Inf. Syst., Colorado Univ., Colorado Springs, CO, USA},
    isbn = {0-7695-1980-6},
    issn = {1090-705X},
    keywords = {re-w5},
    pages = {169--178},
    posted-at = {2012-02-01 21:58:46},
    priority = {5},
    publisher = {IEEE},
    title = {{Elicitation technique selection: how do experts do it?}},
    url = {http://dx.doi.org/10.1109/icre.2003.1232748},
    year = {2003}
}

@inproceedings{Milne2011Power,
    abstract = {{This vision paper considers the role of power and politics in requirements engineering (RE). It offers a working definition of both terms and reviews the existing literature both in RE and related disciplines. It argues that, given the increased complexity, uncertainty and organisational embeddedness faced by RE in practice, power and politics have become increasingly relevant factors, and that they have not as yet been given adequate consideration. Building upon recent relevant research, a research agenda is proposed that presents a methodological framework which examines power and politics through the structure of power relations and the process of decision-making. This framework will require validation through empirical research as a first step to developing models of power and politics that could be of practical use for RE.}},
    author = {Milne, A. and Maiden, N.},
    booktitle = {Requirements Engineering Conference (RE), 2011 19th IEEE International},
    citeulike-article-id = {10287620},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/re.2011.6051646},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6051646},
    doi = {10.1109/re.2011.6051646},
    institution = {Centre for Human-Comput. Interaction Design, City Univ. London, London, UK},
    isbn = {978-1-4577-0921-0},
    issn = {1090-705X},
    keywords = {re-w5},
    month = aug,
    pages = {187--196},
    posted-at = {2012-01-30 22:21:52},
    priority = {5},
    publisher = {IEEE},
    title = {{Power and politics in requirements engineering: A proposed research agenda}},
    url = {http://dx.doi.org/10.1109/re.2011.6051646},
    year = {2011}
}

@inproceedings{Potts1997Naturalistic,
    abstract = {{A growing awareness of the need to take into account social and
contextual factors requirements engineering (RE) has led to expanded use
of naturalistic inquiry (NI) methods, such as ethnography, for capturing
relevant data. There is little debate about the potential value of NI to
the development of systems; however, most previous discussions have
emphasized practical techniques and benefits. Less attention has been
given to the ontological and epistemological commitments that a
naturalistic research paradigm assume and the extent to which these
assumptions conflict with those that pervade RE practice. In this paper
we present the axioms that NI. In each case we address both the points
of agreement and tension that arise when these axioms are compared with
the implicit assumptions upon which RE practice and research methods are
based. We illustrate the discussion with specific examples from
published sources and our experience}},
    author = {Potts, C. and Newstetter, W. C.},
    booktitle = {Requirements Engineering, 1997., Proceedings of the Third IEEE International Symposium on},
    citeulike-article-id = {10287605},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/isre.1997.566849},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=566849},
    doi = {10.1109/isre.1997.566849},
    institution = {Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA},
    isbn = {0-8186-7740-6},
    keywords = {re-w5},
    month = jan,
    pages = {118--127},
    posted-at = {2012-01-30 22:19:52},
    priority = {2},
    publisher = {IEEE},
    title = {{Naturalistic inquiry and requirements engineering: reconciling
their theoretical foundations}},
    url = {http://dx.doi.org/10.1109/isre.1997.566849},
    year = {1997}
}

@article{Breaux2008Analyzing,
    abstract = {{Information practices that use personal, financial, and health-related information are governed by US laws and regulations to prevent unauthorized use and disclosure. To ensure compliance under the law, the security and privacy requirements of relevant software systems must properly be aligned with these regulations. However, these regulations describe stakeholder rules, called rights and obligations, in complex and sometimes ambiguous legal language. These "rules" are often precursors to software requirements that must undergo considerable refinement and analysis before they become implementable. To support the software engineering effort to derive security requirements from regulations, we present a methodology for directly extracting access rights and obligations from regulation texts. The methodology provides statement-level coverage for an entire regulatory document to consistently identify and infer six types of data access constraints, handle complex cross references, resolve ambiguities, and assign required priorities between access rights and obligations to avoid unlawful information disclosures. We present results from applying this methodology to the entire regulation text of the US Health Insurance Portability and Accountability Act (HIPAA) Privacy Rule.}},
    address = {Los Alamitos, CA, USA},
    author = {Breaux, T. D. and Anton, A. I.},
    citeulike-article-id = {9643863},
    citeulike-linkout-0 = {http://proxying.lib.ncsu.edu/index.php?url=http://dx.doi.org/10.1109\%2FTSE.2007.70746},
    citeulike-linkout-1 = {http://doi.ieeecomputersociety.org/10.1109/TSE.2007.70746},
    citeulike-linkout-2 = {http://dx.doi.org/10.1109/tse.2007.70746},
    citeulike-linkout-3 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4359472},
    doi = {10.1109/tse.2007.70746},
    institution = {North Carolina State Univ., Raleigh},
    issn = {0098-5589},
    journal = {Software Engineering, IEEE Transactions on},
    keywords = {re-w4},
    month = jan,
    number = {1},
    pages = {5--20},
    posted-at = {2012-01-28 01:44:24},
    priority = {2},
    publisher = {IEEE},
    title = {{Analyzing Regulatory Rules for Privacy and Security Requirements}},
    url = {http://proxying.lib.ncsu.edu/index.php?url=http://dx.doi.org/10.1109\%2FTSE.2007.70746},
    volume = {34},
    year = {2008}
}

@inproceedings{Otto2007Addressing,
    abstract = {{Legal texts, such as regulations and legislation, are playing an increasingly important role in requirements engineering and system development. Monitoring systems for requirements and policy compliance has been recognized in the requirements engineering community as a key area for research. Similarly, regulatory compliance is critical in systems that are governed by regulations and law, especially given that non-compliance can result in both financial and criminal penalties. Working with legal texts can be very challenging, however, because they contain numerous ambiguities, cross-references, domain-specific definitions, and acronyms, and are frequently amended via new regulations and case law. Requirements engineers and compliance auditors must be able to identify relevant regulations, extract requirements and other key concepts, and monitor compliance throughout the software lifecycle. This paper surveys research efforts over the past 50 years in handling legal texts for systems development. These efforts include the use of symbolic logic, logic programming, first-order temporal logic, deontic logic, defeasible logic, goal modeling, and semi-structured representations. This survey can aid requirements engineers and auditors to better specify, monitor, and test software systems for compliance.}},
    author = {Otto, P. N. and Anton, A. I.},
    booktitle = {Requirements Engineering Conference, 2007. RE '07. 15th IEEE International},
    citeulike-article-id = {10279669},
    citeulike-linkout-0 = {http://proxying.lib.ncsu.edu/index.php?url=http://dx.doi.org/10.1109\%2FRE.2007.65},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/re.2007.65},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4384161},
    doi = {10.1109/re.2007.65},
    institution = {North Carolina State Univ., Raleigh},
    isbn = {978-0-7695-2935-6},
    issn = {1090-705X},
    keywords = {re-w4},
    month = oct,
    pages = {5--14},
    posted-at = {2012-01-28 01:41:54},
    priority = {2},
    publisher = {IEEE},
    title = {{Addressing Legal Requirements in Requirements Engineering}},
    url = {http://proxying.lib.ncsu.edu/index.php?url=http://dx.doi.org/10.1109\%2FRE.2007.65},
    year = {2007}
}

@inproceedings{Massey2011Assessing,
    abstract = {{Software engineers regularly build systems that are required to comply with laws and regulations. To this end, software engineers must determine which requirements have met or exceeded their legal obligations and which requirements have not. Requirements that have met or exceeded their legal obligations are legally implementation ready, whereas requirements that have not met or exceeded their legal obligations need further refinement. Research is needed to better understand how to support software engineers in making these determinations. In this paper, we describe a case study in which we asked graduate-level software engineering students to assess whether a set of software requirements for an electronic health record system met or exceeded their corresponding legal obligations as expressed in regulations created pursuant to the U.S. Health Insurance Portability and Accountability Act (HIPAA). We compare the assessment made by graduate students with an assessment made by HIPAA compliance subject matter experts. Additionally, we contrast these results with those generated by a legal requirements triage algorithm. Our findings suggest that the average graduate-level software engineering student is ill-prepared to write legally compliant software with any confidence and that domain experts are an absolute necessity. Our findings also indicate the potential utility of legal requirements metrics in aiding software engineers as they make legal compliance decisions.}},
    author = {Massey, A. K. and Smith, B. and Otto, P. N. and Anton, A. I.},
    booktitle = {Requirements Engineering Conference (RE), 2011 19th IEEE International},
    citeulike-article-id = {10279666},
    citeulike-linkout-0 = {http://proxying.lib.ncsu.edu/index.php?url=http://dx.doi.org/10.1109\%2FRE.2011.6051661},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/re.2011.6051661},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6051661},
    comment = {(private-note)This paper studies methods to determine whether requirements for an electronic health records (EHR) system are legally implementation ready (LIR).  The paper states that a requirement is LIR if it meets or exceeds the legal obligations expressed in the relevant regulations.  The paper uses a case study of graduate students to determine how well graduate students with some software engineering training can determine whether a requirement is LIR.  It also uses a regression model based on requirements metrics expressing the complexity, dependency, and maturity of a requirement.  The performance of the regression model at classifying requirements as LIR is compare with the performance of the students.  The standard of accuracy in all cases is the consensus classification of subject matter experts for each requirement.  The study found that graduate students "cannot accurately classify the LIR status of a requirement..".  The study found that using metrics with the regression model the LIR status of a requirement can be predicted with a 1 in 5 chance of classifying a requirement as LIR that is not LIR.  There was also an evaluation of an earlier published triage method that did not accurately classify requirements as LIR.  

The strength of this paper is in a very strong experimental design and conducting the evaluation of LIR requirements with both automated (metrics) and manual analysis methods.  Initially I considered these as potentially two studies that could be separately published, however, combining them strengthens the result both in the evaluation of the students and the evaluation of the regression model method.  The main weakness in the study was the experience and training of the students using a brief tutorial.  It was not mentioned whether the students' understanding of the tutorial was independently verified (via. homework or test).  Thus the results for the students could reflect their understanding and retention of the tutorial material and abilities to apply it.  In exploring further research opportunities I would look at non-student evaluators as suggested in the paper and determine whether a set of metrics of the regulations can be useful in explaining the errors for the regression model.

Questions: What was the motivation for combining the study of students with the study of the metrics and regression model?

Do these references [6, 33] certify the Wideband Delphi method for use determining the SME opinion on something besides an estimate, in this case the suitability of a requirement?  

I'm intereted in the metrics so may look at those references [22,25].},
    doi = {10.1109/re.2011.6051661},
    institution = {North Carolina State Univ., Raleigh, NC, USA},
    isbn = {978-1-4577-0921-0},
    issn = {1090-705X},
    keywords = {re, re-w4},
    month = aug,
    pages = {207--216},
    posted-at = {2012-01-28 01:39:13},
    priority = {5},
    publisher = {IEEE},
    title = {{Assessing the accuracy of legal implementation readiness decisions}},
    url = {http://proxying.lib.ncsu.edu/index.php?url=http://dx.doi.org/10.1109\%2FRE.2011.6051661},
    year = {2011}
}

@inproceedings{Maxwell2011Legal,
    abstract = {{Companies must ensure their software complies with relevant laws and regulations to avoid the risk of costly penalties, lost reputation, and brand damage resulting from noncompliance. Laws and regulations contain internal cross-references to portions of the same legal text, as well as cross-references to external legal texts. These cross-references introduce ambiguities, exceptions, as well as other challenges to regulatory compliance. Requirements engineers need guidance as to how to address cross-references in order to comply with the requirements of the law. Herein, we analyze each external cross-reference within the U.S. Health Insurance Portability and Accountability Act (HIPAA) Privacy Rule to determine whether a cross-reference either: introduces a conflicting requirement, a conflicting definition, and/or refines an existing requirement. Herein, we propose a legal cross-reference taxonomy to aid requirements engineers in classifying cross-references as they specify . Analyzing cross-references enables us to address conflicting requirements that may otherwise thwart legal compliance. We identify five sets of conflicting compliance requirements and recommend strategies for resolving these conflicts.}},
    author = {Maxwell, J. C. and Anton, A. I. and Swire, P.},
    booktitle = {Requirements Engineering Conference (RE), 2011 19th IEEE International},
    citeulike-article-id = {10279664},
    citeulike-linkout-0 = {http://proxying.lib.ncsu.edu/index.php?url=http://dx.doi.org/10.1109\%2FRE.2011.6051647},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/re.2011.6051647},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6051647},
    comment = {(private-note)This paper contributes a taxonomy for classifying legal cross-references so they can be better understood by software requirements engineers.  The taxonomy consisting of the classifications of Constraint, Exception, Definition, Unrelated, Incorrect, and General are well defined and documented to enable application outside the specific legal requirements in HIPAA analyzed (validity of outside application is not claimed).  The strength of the paper is in the generality of the calssification and the documentation defining each classification.  Weaknesses are the narrow scope of the case study needing support from futher studies of other regulations.  The opportunities for further research include replicated studies on other areas and identifying sub-categories of increasing specificity.},
    doi = {10.1109/re.2011.6051647},
    institution = {Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA},
    isbn = {978-1-4577-0921-0},
    issn = {1090-705X},
    keywords = {re, re-w4},
    month = aug,
    pages = {197--206},
    posted-at = {2012-01-28 01:37:27},
    priority = {0},
    publisher = {IEEE},
    title = {{A legal cross-references taxonomy for identifying conflicting software requirements}},
    url = {http://proxying.lib.ncsu.edu/index.php?url=http://dx.doi.org/10.1109\%2FRE.2011.6051647},
    year = {2011}
}

@incollection{Mandic2010SAS,
    address = {Berlin, Heidelberg},
    author = {Mandi\'{c}, Vladimir and Oivo, Markku},
    chapter = {23},
    citeulike-article-id = {10277978},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/978-3-642-13792-1\_23},
    citeulike-linkout-1 = {http://www.springerlink.com/content/wr10g42578656637},
    doi = {10.1007/978-3-642-13792-1\_23},
    editor = {Ali Babar, M. and Vierimaa, Matias and Oivo, Markku},
    isbn = {978-3-642-13791-4},
    keywords = {ds},
    pages = {291--305},
    posted-at = {2012-01-27 16:34:57},
    priority = {3},
    publisher = {Springer Berlin / Heidelberg},
    series = {Lecture Notes in Computer Science},
    title = {{SAS: A Tool for the GQM\&lt;sup\&gt;+\&lt;/sup\&gt;Strategies Grid Derivation Process Product-Focused Software Process Improvement}},
    url = {http://dx.doi.org/10.1007/978-3-642-13792-1\_23},
    volume = {6156},
    year = {2010}
}

@article{Sauer2007Impact,
    abstract = {{Studying the factors influencing project risk.}},
    address = {New York, NY, USA},
    author = {Sauer, Chris and Gemino, Andrew and Reich, Blaize H.},
    citeulike-article-id = {3477484},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1297801},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1297797.1297801},
    comment = {(private-note)Describes a study that finds 67\% of software projects succeed in contrast to the often quoted Standish Group report that 67\% fail.  Their definition of success provides a little more leway allowing project to complete within a margin of error on budget and schedule.  From a project management/risk management perspective this makes sense.},
    doi = {10.1145/1297797.1297801},
    issn = {0001-0782},
    journal = {Commun. ACM},
    keywords = {cmds, re-change},
    month = nov,
    number = {11},
    pages = {79--84},
    posted-at = {2012-01-23 22:28:12},
    priority = {0},
    publisher = {ACM},
    title = {{The impact of size and volatility on IT project performance}},
    url = {http://dx.doi.org/10.1145/1297797.1297801},
    volume = {50},
    year = {2007}
}

@article{OzkayaDeveloping,
    author = {Ozkaya, Ipek},
    citeulike-article-id = {10257624},
    citeulike-linkout-0 = {http://blog.sei.cmu.edu/post.cfm/developing-an-architecture-focused-measurement-framework-for-managing-technical-debt},
    comment = {(private-note)We could use change history mining via Code Hot Spot combined with MORPHISIS architectural metrics to arrive at a similar evaluation of architecture dependency and propagation cost.},
    keywords = {td},
    posted-at = {2012-01-23 18:38:50},
    priority = {0},
    title = {{Developing an Architecture-Focused Measurement Framework for Managing Technical Debt » SEI Blog}},
    url = {http://blog.sei.cmu.edu/post.cfm/developing-an-architecture-focused-measurement-framework-for-managing-technical-debt}
}

@inproceedings{Robbes2007Approach,
    abstract = {{The analysis of the evolution of software systems is a useful source of information for a variety of activities, such as reverse engineering, maintenance, and predicting the future evolution of these systems. Current software evolution research is mainly based on the information contained in versioning systems such as CVS and SubVersion. But the evolutionary information contained therein is incomplete and of low quality, hence limiting the scope of evolution research. It is incomplete because the historical information is only recorded at the explicit request of the developers (a commit in the classical checkin/checkout model). It is of low quality because the file-based nature of versioning systems leads to a view of software as being a set of files. In this paper we present a novel approach to software evolution analysis which is based on the recording of all semantic changes performed on a system, such as refactorings. We describe our approach in detail, and demonstrate how it can be used to perform fine-grained software evolution analysis.}},
    address = {Berlin, Heidelberg},
    author = {Robbes, Romain and Lanza, Michele and Lungu, Mircea},
    booktitle = {Proceedings of the 10th international conference on Fundamental approaches to software engineering},
    citeulike-article-id = {7132927},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1759400},
    isbn = {978-3-540-71288-6},
    keywords = {chs},
    location = {Braga, Portugal},
    pages = {27--41},
    posted-at = {2012-01-23 01:17:54},
    priority = {2},
    publisher = {Springer-Verlag},
    series = {FASE'07},
    title = {{An approach to software evolution based on semantic change}},
    url = {http://portal.acm.org/citation.cfm?id=1759400},
    year = {2007}
}

@techreport{Software,
    abstract = {{This report presents guidelines for defining, recording, and reporting two frequently used measures of software size: physical source lines and logical source statements. We propose a general framework for constructing size definitions and use it to derive operational methods for reducing misunderstandings in measurement results. We show how the methods can be applied to address the information needs of different users while maintaining a common definition of software size.}},
    citeulike-article-id = {10247328},
    citeulike-linkout-0 = {http://www.sei.cmu.edu/library/abstracts/reports/92tr020.cfm},
    keywords = {cmds, size},
    posted-at = {2012-01-20 14:07:54},
    priority = {2},
    title = {{Software Size Measurement: A Framework for Counting Source Statements}},
    url = {http://www.sei.cmu.edu/library/abstracts/reports/92tr020.cfm}
}

@inproceedings{Gaohui2006Analysis,
    abstract = {{This paper first introduces the foundational of game theory, then makes qualitative and quantitative analysis on the game activities between enterprise and software house during the implementation process of software project, lastly gives several pieces of advice for achieving "double win" during the game between enterprise and software house. The process of enterprise's implementing software project is a kind of cooperative game and dynamic game happening between enterprise and software house. This paper analyses the specific game process of this project game and draws a conclusion that this game will achieve Nash equilibrium that they all reap their own maximal profit only if enterprise ascertains the goal of the project, presents enough requirement to software house, and software house gives enterprise correct project solution and product, and they cooperatives positively and actively}},
    author = {Gao-hui, Nie},
    booktitle = {Management Science and Engineering, 2006. ICMSE '06. 2006 International Conference on},
    citeulike-article-id = {10247303},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/icmse.2006.313873},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4104900},
    doi = {10.1109/icmse.2006.313873},
    institution = {Sch. of Inf. Manage., Jiangxi Univ. of Finance \& Econ.},
    isbn = {7-5603-2355-3},
    keywords = {cmds},
    month = oct,
    pages = {234--237},
    posted-at = {2012-01-20 13:43:46},
    priority = {2},
    publisher = {IEEE},
    title = {{Analysis on Enterprise's Software Project Management Based on Game Theory}},
    url = {http://dx.doi.org/10.1109/icmse.2006.313873},
    year = {2006}
}

@inproceedings{Goguen1993Techniques,
    abstract = {{The authors survey and evaluate techniques for eliciting
requirements of computer-based systems, paying particular attention to
dealing with social issues. The methods surveyed include introspection,
interviews, questionnaires, and protocol, conversation, interaction, and
discourse analyses. The last three techniques grew out of
ethnomethodology and sociolinguistics. They can elicit tacit knowledge
by observing actual interactions in the workplace, and can also be
applied to the system development process itself}},
    author = {Goguen, J. A. and Linde, C.},
    booktitle = {Requirements Engineering, 1993., Proceedings of IEEE International Symposium on},
    citeulike-article-id = {9275586},
    citeulike-linkout-0 = {http://proxying.lib.ncsu.edu/index.php?url=http://dx.doi.org/10.1109\%2FISRE.1993.324822},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/isre.1993.324822},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=324822},
    comment = {(private-note)This is a survey of social science based methods that apply for
eliciting requirements.  The methods, uses and drawbacks of each
technique are discussed in detail.  The focus is on the possible
outcomes when applying each technique in a requirements elicitation
settting.  This paper is strong at describing how communiation on
requriements can reach incorrect conclusions with each technique.  The
main weakness is the paper addresses each technique without
highlighting supporting data in the paper itself.  There are many
opportunities to empirically research conclusions on the techniques
presented in the paper in the conduct of requirements engineering.},
    doi = {10.1109/isre.1993.324822},
    institution = {Comput. Lab., Oxford Univ.},
    isbn = {0-8186-3120-1},
    keywords = {re-w3},
    location = {San Diego, CA, USA},
    month = jan,
    pages = {152--164},
    posted-at = {2012-01-18 14:34:25},
    priority = {2},
    publisher = {IEEE},
    title = {{Techniques for requirements elicitation}},
    url = {http://proxying.lib.ncsu.edu/index.php?url=http://dx.doi.org/10.1109\%2FISRE.1993.324822},
    year = {1993}
}

@inproceedings{Lloyd2002Effectiveness,
    abstract = {{Software development teams are often geographically distributed from their customers and end users. This creates significant communication and coordination challenges that impact the effectiveness of requirements engineering. Travel costs, and the local availability of quality technical staff increase the demand for effective distributed software development teams. This research reports an empirical study of how groupware can be used to aid distributed requirements engineering for a software development project. Six groups of seven to nine members were formed and divided into separate remote groups of customers and engineers. The engineers conducted a requirements analysis and produced a software requirements specification (SRS) document through distributed interaction with the remote customers. We present results and conclusions from the research including: an analysis of factors that affected the quality of the software requirements specification document written at the conclusion of the requirements process and the effectiveness of requirements elicitation techniques which were used in a distributed setting for requirements gathering.}},
    author = {Lloyd, W. J. and Rosson, M. B. and Arthur, J. D.},
    booktitle = {Requirements Engineering, 2002. Proceedings. IEEE Joint International Conference on},
    citeulike-article-id = {1044607},
    citeulike-linkout-0 = {http://proxying.lib.ncsu.edu/index.php?url=http://dx.doi.org/10.1109\%2FICRE.2002.1048544},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/icre.2002.1048544},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1048544},
    comment = {(private-note)The main contribution of this work is the design and execution of an experiment to determine the effectiveness of requirements engineering methods conducted in a distributed environment. The relative effectiveness of each method in the environment was evaluated based on the ratings of the participants and the evaluation of the produced requirements spec. A strength of this paper is that the description of the study design was thorough and supporting background discussion was comprehensive. The main weakness is that the study involved only students with only 6 groups total meaning that the correlatoin analysis would have wide confidence bounds. The opportunities for future work include repeating the study in an industrial setting and repeating the study with additional collaboration aids such as video conferencing. },
    doi = {10.1109/icre.2002.1048544},
    isbn = {0-7695-1465-0},
    issn = {1090-705X},
    journal = {Requirements Engineering, 2002. Proceedings. IEEE Joint International Conference on},
    keywords = {re, re-w3},
    pages = {311--318},
    posted-at = {2012-01-18 14:32:08},
    priority = {5},
    publisher = {IEEE},
    title = {{Effectiveness of elicitation techniques in distributed requirements engineering}},
    url = {http://proxying.lib.ncsu.edu/index.php?url=http://dx.doi.org/10.1109\%2FICRE.2002.1048544},
    year = {2002}
}

@inproceedings{Allenby2001Deriving,
    abstract = {{Elicitation of requirements for safety critical aero-engine
control systems is dependent on the capture of core design intent and
the systematic derivation of requirements addressing hazardous
deviations from that intent. Derivation of these requirements is
inextricably linked to the safety assessment process. Conventional civil
aerospace practice (as advocated by guidelines such as ARP4754 and
ARP4671) promotes the application of Functional Hazard Assessment (FHA)
to sets of statements of functional intent. Systematic hazard analysis
of scenario-based requirements representations is less well understood.
This paper discusses the principles and problems of hazard analysis and
proposes an approach to conducting hazard analysis on use case
requirements representations. Using the approach, it is possible to
justifiably derive hazard-mitigation use cases as first class
requirements from systematic hazard analysis of core design intent
scenarios. An industrial example is used to illustrate the technique}},
    author = {Allenby, K. and Kelly, T.},
    booktitle = {Requirements Engineering, 2001. Proceedings. Fifth IEEE International Symposium on},
    citeulike-article-id = {5382932},
    citeulike-linkout-0 = {http://proxying.lib.ncsu.edu/index.php?url=http://dx.doi.org/10.1109\%2FISRE.2001.948563},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/isre.2001.948563},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=948563},
    comment = {(private-note)This paper contributes a method for performing a safety critical hazard analysis that is integrated with requirements elicitation through use case scenarios. The scenarios are defined from use case style requirements using a functional hazard assessment (FHA) technique. The paper provides an example of how this is done and describes the benefits and importance of performing hazard analysis for safety crtical systems. Strengths in include a good review of hazard based anaysis and a complete walkthrough of a restricted example. Weakness of the paper is that it only walks thorugh one example with one method (FHA) although it discusses two safety analysis methods FHA and HAZOP. Opportunities for furhter research include exteding the method for multiple failure combinations or failures across multiple systems, and defining a graphical representation for hazard scenarios.

Questions:
What is the best practice approach for defining requirements for safety critical systems. Use cases seem to be described as better than natural language requirements, but are there even better techniques that incorporate hazard analysis?

How do use cases and scenarios differ?  Scenarios describe states and flows of events of the system (like use cases) but include exceptions to the normal flow of events.  (seems like this could still be handled in use cases)?                    },
    doi = {10.1109/isre.2001.948563},
    institution = {Dept. of Comput. Sci., York Univ.},
    isbn = {0-7695-1125-2},
    journal = {Requirements Engineering, 2001. Proceedings. Fifth IEEE International Symposium on},
    keywords = {re, re-w3},
    pages = {228--235},
    posted-at = {2012-01-18 14:28:00},
    priority = {5},
    publisher = {IEEE},
    title = {{Deriving safety requirements using scenarios}},
    url = {http://proxying.lib.ncsu.edu/index.php?url=http://dx.doi.org/10.1109\%2FISRE.2001.948563},
    year = {2001}
}

@manual{BurnsR,
    author = {Burns, Patrick},
    citeulike-article-id = {10236437},
    citeulike-linkout-0 = {http://www.burns-stat.com/pages/Tutor/R\_inferno.pdf},
    keywords = {stat},
    posted-at = {2012-01-17 17:14:56},
    priority = {2},
    title = {{The R Inferno}},
    url = {http://www.burns-stat.com/pages/Tutor/R\_inferno.pdf}
}

@inproceedings{Easterbrook1995Managing,
    abstract = {{In an evolving specification, considerable effort is spent handling recurrent inconsistencies. Detecting and resolving inconsistencies is only part of the problem: a resolved inconsistency might not stay resolved. Frameworks in which inconsistency is tolerated help by allowing resolution to be delayed. However, evolution of a specification may affect both resolved and unresolved inconsistencies. We address these problems by explicitly recording relationships between partial specifications (ViewPoints), representing both resolved and unresolved inconsistencies. We assume that ViewPoints will often be inconsistent with one another, and we ensure that a complete work record is kept, detailing any inconsistencies that have been detected, and what actions, if any, have been taken to resolve them. The work record is then used to reason about the effects of subsequent changes to ViewPoints, without constraining the development process.}},
    author = {Easterbrook, S. and Nuseibeh, B.},
    booktitle = {Requirements Engineering, 1995., Proceedings of the Second IEEE International Symposium on},
    citeulike-article-id = {261800},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/isre.1995.512545},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=512545},
    doi = {10.1109/isre.1995.512545},
    institution = {Sch. of Cognitive \& Comput. Sci., Sussex Univ., Brighton, UK},
    isbn = {0-8186-7017-7},
    journal = {Requirements Engineering, 1995., Proceedings of the Second IEEE International Symposium on},
    keywords = {re-w12},
    month = mar,
    pages = {48--55},
    posted-at = {2012-01-13 02:52:55},
    priority = {4},
    publisher = {IEEE},
    title = {{Managing inconsistencies in an evolving specification}},
    url = {http://dx.doi.org/10.1109/isre.1995.512545},
    year = {1995}
}

@article{Potts1994Inquirybased,
    abstract = {{This approach emphasizes pinpointing where and when information needs occur; at its core is the inquiry cycle model, a structure for describing and supporting discussions about system requirements. The authors use a case study to describe the model's conversation metaphor, which follows analysis activities from requirements elicitation and documentation through refinement.>}},
    author = {Potts, C. and Takahashi, K. and Anton, A. I.},
    citeulike-article-id = {256356},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/52.268952},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=268952},
    doi = {10.1109/52.268952},
    institution = {Georgia Inst. of Technol., Atlanta, GA, USA},
    issn = {0740-7459},
    journal = {IEEE Software},
    keywords = {re-w7},
    month = mar,
    number = {2},
    pages = {21--32},
    posted-at = {2012-01-13 02:21:37},
    priority = {2},
    publisher = {IEEE},
    title = {{Inquiry-based requirements analysis}},
    url = {http://dx.doi.org/10.1109/52.268952},
    volume = {11},
    year = {1994}
}

@article{Damian2006Empirical,
    abstract = {{Requirements engineering is an important component of effective software engineering, yet more research is needed to demonstrate the benefits to development organizations. While the existing literature suggests that effective requirements engineering can lead to improved productivity, quality, and risk management, there is little evidence to support this. We present empirical evidence showing how requirements engineering practice relates to these claims. This evidence was collected over the course of a 30-month case study of a large software development project undergoing requirements process improvement. Our findings add to the scarce evidence on RE payoffs and, more importantly, represent an in-depth explanation of the role of requirements engineering processes in contributing to these benefits. In particular, the results of our case study show that an effective requirements process at the beginning of the project had positive outcomes throughout the project lifecycle, improving the efficacy of other project processes, ultimately leading to improvements in project negotiation, project planning, and managing feature creep, testing, defects, rework, and product quality. Finally, we consider the role collaboration had in producing the effects we observed and the implications of this work to both research and practice}},
    author = {Damian, D. and Chisan, J.},
    citeulike-article-id = {812619},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/tse.2006.61},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1677531},
    doi = {10.1109/tse.2006.61},
    institution = {Dept. of Comput. Sci., Victoria Univ., BC},
    issn = {0098-5589},
    journal = {IEEE Transactions on Software Engineering},
    keywords = {re-w14},
    month = jul,
    number = {7},
    pages = {433--453},
    posted-at = {2012-01-13 02:11:13},
    priority = {5},
    publisher = {IEEE},
    title = {{An Empirical Study of the Complex Relationships between Requirements Engineering Processes and Other Processes that Lead to Payoffs in Productivity, Quality, and Risk Management}},
    url = {http://dx.doi.org/10.1109/tse.2006.61},
    volume = {32},
    year = {2006}
}

@article{Anton2003Functional,
    abstract = {{It has long been accepted that requirements analysis should precede architectural design and implementation, but in software evolution and reverse engineering this concern with black-box analysis of function has necessarily been de-emphasized in favor of code-based analysis and designer-oriented interpretation. In this paper, we redress this balance by describing "functional paleontology," an approach to analyzing the evolution of user-visible features or services independent of architecture and design intent. We classify the benefits and burdens of interpersonal communication services into core and peripheral categories and investigate the telephony services available to domestic subscribers over a 50-year period. We report that services were introduced in discrete bursts, each of which emphasized different benefits and burdens. We discuss the general patterns of functional evolution that this "fossil record" illustrates and conclude by discussing their implications for forward engineering of software products.}},
    address = {Piscataway, NJ, USA},
    author = {Anton, A. I. and Potts, C.},
    citeulike-article-id = {846068},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=642976},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/tse.2003.1178053},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1178053},
    doi = {10.1109/tse.2003.1178053},
    institution = {Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA},
    issn = {0098-5589},
    journal = {Software Engineering, IEEE Transactions on},
    keywords = {re, relead},
    month = feb,
    number = {2},
    pages = {151--166},
    posted-at = {2012-01-13 02:05:00},
    priority = {5},
    publisher = {IEEE},
    title = {{Functional paleontology: the evolution of user-visible system services}},
    url = {http://dx.doi.org/10.1109/tse.2003.1178053},
    volume = {29},
    year = {2003}
}

@inproceedings{Wieringa2009How,
    abstract = {{Scientific evaluation papers investigate existing problem situations or validate proposed solutions with scientific means, such as by experiment or case study. There is a growing amount of literature about how to report about empirical research in software engineering, but there is still some confusion about the difference between a scientific evaluation paper and other kinds of research papers. This is related to lack of clarity about the relation between empirical research, engineering, and industrial practice. In this minitutorial we give a brief rundown on how to structure a scientific evaluation papers as a special kind of research paper, using experiment reports and case study reports as examples. We give checklists of items that a reader should be able to find in these papers, and sketch the dilemmas that writers and readers of these papers face when applying these checklists.}},
    author = {Wieringa, R. and Heerkens, H. and Regnell, B.},
    booktitle = {Requirements Engineering Conference, 2009. RE '09. 17th IEEE International},
    citeulike-article-id = {6614568},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/re.2009.17},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5328494},
    day = {17},
    doi = {10.1109/re.2009.17},
    institution = {Dept. of Comput. Sci., Univ. of Twente, Enschede, Netherlands},
    isbn = {978-0-7695-3761-0},
    issn = {1090-705X},
    journal = {Requirements Engineering Conference, 2009. RE '09. 17th IEEE International},
    keywords = {re},
    month = aug,
    pages = {361--364},
    posted-at = {2012-01-11 03:21:33},
    priority = {5},
    publisher = {IEEE},
    title = {{How to Write and Read a Scientific Evaluation Paper}},
    url = {http://dx.doi.org/10.1109/re.2009.17},
    year = {2009}
}

@inproceedings{Gotel2009How,
    abstract = {{This mini-tutorial will provide high-level guidance on designing a requirements management solution and selecting a requirements management tool. The guidance will focus on first understanding the context, stakeholders and tasks, so on articulating the problems that need to be addressed and the constraints that shape viable options. The guidance will also attend to the equally critical issue of managing the high expectations that are typically associated with requirements management tool adoption and use. Through examining the minimum and desirable requirements, based upon the type and size of project, the question of whether a requirements management tool is needed at all can also be considered.}},
    author = {Gotel, O. and Mader, P.},
    booktitle = {Requirements Engineering Conference, 2009. RE '09. 17th IEEE International},
    citeulike-article-id = {10212923},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/re.2009.49},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5328495},
    comment = {(private-note)This paper introduces a tutorial session on how to choose a
requirements tool for an organization.  It does a good job of
introducing the tutorial, however, leaves out some of the particular
advice and criteria that are discussed in the tutorial.  The
references cited by the paper are more likely to provide specific
guidance and criteria.  A key message of the papers is that
requirements tools must be selected based upon the characteristics of
the organization and problem domain where they will be used.  There
are opportunities for further discussion of key criteria and decision
aids that could help practitioners refine their methods for selecting
a requirements tool.},
    doi = {10.1109/re.2009.49},
    institution = {Dept. of Comput. Sci., Pace Univ., New York, NY, USA},
    isbn = {978-0-7695-3761-0},
    issn = {1090-705X},
    keywords = {re},
    month = aug,
    pages = {365--367},
    posted-at = {2012-01-11 03:03:14},
    priority = {4},
    publisher = {IEEE},
    title = {{How to Select a Requirements Management Tool: Initial Steps}},
    url = {http://dx.doi.org/10.1109/re.2009.49},
    year = {2009}
}

@article{Zave1997Four,
    abstract = {{This article
shines some light in the \&quot;four dark corners,\&quot; exposing problems and proposing solutions. We
show that all descriptions involved in requirements engineering should be descriptions of the
environment. We show that certain control information is necessary for sound requirements
engineering, and we explain the close association between domain knowledge and refinement
of requirements. Together these conclusions explain the precise nature of requirements,
specifications, and domain...}},
    author = {Zave, Pamela and Jackson, Michael},
    citeulike-article-id = {1225202},
    citeulike-linkout-0 = {http://dx.doi.org/10.1145/237432.237434},
    citeulike-linkout-1 = {http://dl.acm.org.prox.lib.ncsu.edu/citation.cfm?id=237432\&\#38;CFID=189564\&\#38;CFTOKEN=39724621},
    citeulike-linkout-2 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.26.4161},
    comment = {(private-note)The article contributes the motivation for and the definition of requirements as a formal description of the environment a system operates within and upon.  The four corners discussed include a focus on grounding requiremets in the scope of the environment, that they do not describe the machie itself but rather its interaction with the environment, that the actions of the environment and the machiene should be separately described, and the tranlation of requirements into a formal specification.  Zave and Jackson define and provide examples for 10 key formal mechaisms that provide a framework for completely defining requirements.  A designation and a defintion distinguish the declaration of a formal term within the environment from derived definition of a term that uses designations.  They separate statements that describe given conditions of the environment (indicative) from statements that describe how the environment is transformed (optative) by the system.  Actions are divided into quandrants of shared or unshared and environment-controlled and machine-controlled.  They declara a requirement as an optative property of the intended system, a domain assumtion as a indicative property of the environment, and a specification as an optative property that translates bewteen requirements and the implementation in code and hardware of the system.  Zave and Jackson finally define 5 criteria for the completion of requirements engineering activity for a project.  Opportunities for further research could be creating derived notations that implement the framework and provide traceability between the elments.  Empirical stuides of the effectivenes of the framework could be conducted.},
    doi = {10.1145/237432.237434},
    journal = {ACM Transactions on Software Engineering and Methodology},
    keywords = {re},
    number = {1},
    pages = {1--30},
    posted-at = {2012-01-11 02:58:23},
    priority = {0},
    title = {{Four dark corners of requirements engineering}},
    url = {http://dl.acm.org.prox.lib.ncsu.edu/citation.cfm?id=237432\&\#38;CFID=189564\&\#38;CFTOKEN=39724621},
    volume = {6},
    year = {1997}
}

@article{Jackson1997Meaning,
    abstract = {{We use the term requirements to denote what are often called functional requirements. Requirements are located in the environment, which is distinguished from the machine to be built. A requirement is a condition over phenomena of the environment. A specification is a restricted form of requirement, providing enough information for the implementer to build the machine (by programming it) without further environment knowledge. To describe requirements appropriately we must fit our descriptions into an appropriate structure. This structure must respect the distinction between the machine and the environment, and the distinction between those environment properties that are given (indicative descriptions) and those that must be achieved by the machine (optative descriptions). Formalisation is a fundamental problem of requirements engineering. Since most environments are parts of the physical world, and therefore informal, the formalisation task is inescapable. Some techniques are discussed for tackling this task. In particular, the use of designations is explained, and the distinction between definition and assertion. By using the smallest possible set of designated terms, augmented by appropriate definitions, the developer can create a narrow bridge between the environment and its descriptions in the requirements. In this way a sufficiently faithful approximation to the informal reality can be obtained.}},
    author = {Jackson, Michael},
    citeulike-article-id = {2459970},
    citeulike-linkout-0 = {http://www.springerlink.com.prox.lib.ncsu.edu/content/l1p7280738531t21/fulltext.pdf},
    citeulike-linkout-1 = {http://dx.doi.org/10.1023/a:1018990005598},
    comment = {(private-note)This paper contributes a method for formal treatment of requirements and specifications so that requirements can be written less ambiguously and translated into a formal specification. The scope of requirements is defined to describe the customer's desired behavior of the machine within the environment it operates to achieve a specific outcome. Jackson makes a clear distinction between the operative statement of requirements that specify a change to the environment made by the machine from the indicative statement that asserts a property of the environment. Jackson discusses the need for a formal specification that bridges the chasm between natural language descriptions of the environment and desired system influence upon it and the rigorous nature of a computer program. Attributes of a specification include ground terms that unambiguously designate an attribute of the environment, and a definition that describes attributes related to the designated attributes such as the number of them available. Using these two constructs the author asserts that formal specifications are possible for any particular problem and solution. There is no specific example given of where these formalisms have been successfully applied though several derived examples are given. Additional work could test these generalized formalisms in challenging environments to determine their completeness.},
    day = {1},
    doi = {10.1023/a:1018990005598},
    journal = {Annals of Software Engineering},
    keywords = {re},
    month = jan,
    number = {0},
    pages = {5--21},
    posted-at = {2012-01-11 02:57:28},
    priority = {0},
    title = {{The meaning of requirements}},
    url = {http://www.springerlink.com.prox.lib.ncsu.edu/content/l1p7280738531t21/fulltext.pdf},
    volume = {3},
    year = {1997}
}

@article{Anton2003Successful,
    abstract = {{A project will likely fail without a plan. We must understand a problem before we can express the requirements for a correct Solution. Otherwise, we will develop software that fails to provide customer satisfaction. Expressing a set of complete, consistent, and correct requirements is conceptually complex, but essential in our quest to develop high-quality, useful software. Although small projects can succeed without formal requirements engineering, any project of considerable size and complexity requires it.}},
    author = {Anton, A. I.},
    booktitle = {Software, IEEE},
    citeulike-article-id = {2894825},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/ms.2003.1196319},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1196319},
    comment = {(private-note)The article provides strong motivation for placing a focus on up-front requirements analysis then highlights general principles that provide a good requirements foundation.  Anton recommends that a preliminary understanding of the problem be established before beginning requirements elicitation.  Other principles include involvement of stakeholders, identifing critical requirements, and paying attention to non-functional and quality requirements.  Opportunities for research include methods for eliciting or discovering non-functional requirements and methods for descending or ascending the needs hierarchy (ie. goal oriented requirements).},
    doi = {10.1109/ms.2003.1196319},
    institution = {Coll. of Eng., North Carolina State Univ., Raleigh, NC, USA},
    issn = {0740-7459},
    journal = {Software, IEEE},
    keywords = {re, re-change},
    month = may,
    number = {3},
    pages = {44, 46+},
    posted-at = {2012-01-11 02:56:21},
    priority = {0},
    publisher = {IEEE},
    title = {{Successful software projects need requirements planning}},
    url = {http://dx.doi.org/10.1109/ms.2003.1196319},
    volume = {20},
    year = {2003}
}

@inproceedings{vanLamsweerde2000Requirements,
    abstract = {{Requirements engineering (RE) is concerned with the identification

of the goals to be achieved by the envisioned system,

the operationalization of such goals into services and

constraints, and the assignment of responsibilities for the

resulting requirements to agents such as humans, devices,

and software. The processes involved in RE include

domain analysis, elicitation, specification, assessment,

negotiation, documentation, and evolution. Getting highquality

requirements is difficult...}},
    author = {van Lamsweerde, Axel},
    booktitle = {International Conference on Software Engineering},
    citeulike-article-id = {1007451},
    citeulike-linkout-0 = {http://ieeexplore.ieee.org.prox.lib.ncsu.edu/stamp/stamp.jsp?tp=\&\#38;arnumber=870392},
    citeulike-linkout-1 = {http://citeseer.ist.psu.edu/vanlamsweerde00requirements.html},
    citeulike-linkout-2 = {http://citeseer.lcs.mit.edu/vanlamsweerde00requirements.html},
    citeulike-linkout-3 = {http://citeseer.ifi.unizh.ch/vanlamsweerde00requirements.html},
    citeulike-linkout-4 = {http://citeseer.comp.nus.edu.sg/vanlamsweerde00requirements.html},
    comment = {(private-note)This paper provides a comprehensive review of the state of requrements
research progress in the year 2000.    The paper contributes a
thorouogh discussion of the state of the art in goal based elicitation
where goals and sub-goals are derived for systems such as TRACS and
BART transportation management systems.  A discussion of requirements
modeling techniques highlights that at this time UML based methods are
dominant.  There are several good examples of requirements engineering
failures that motivate the importance of the area. If there is a
weakness it could be stitching together a message of how all the areas
of research fit together.  The paper highlights many opportunities for
further research particularly the intersection of formal
specifications with software architecture such that architectures can
be more directly derived.},
    keywords = {re},
    pages = {5--19},
    posted-at = {2012-01-11 02:55:29},
    priority = {0},
    title = {{Requirements engineering in the year 00: a research perspective}},
    url = {http://ieeexplore.ieee.org.prox.lib.ncsu.edu/stamp/stamp.jsp?tp=\&\#38;arnumber=870392},
    year = {2000}
}

@inproceedings{nuseibeh00,
    abstract = {{An abstract is not available.}},
    address = {New York, NY, USA},
    author = {Nuseibeh, Bashar and Easterbrook, Steve},
    booktitle = {Proceedings of the Conference on The Future of Software Engineering},
    citeulike-article-id = {486087},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=336523},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/336512.336523},
    comment = {(private-note)The key topics of this article formally define requirements engineering, discuss its key practices and research areas, and provide a motivation for future research through a roadmap.  RE connects several social sciences to the computer science domain namely Sociology, Cognitive Psychology, Anthropology and Linguistics.  It provides a high level overview of the key practices for eliciting requirements, modeling requriements, communicating requirements, and evolving requirements.  The roadmap provides analysis of the current state of RE research and opprotunities for future research directions.  The paper is strong at providing an exposure to the scope of requirements engineering.  The weaknesses are lack of motivational support and very light treatment of how RE fits into the process models and relates to other areas of software engineering.},
    doi = {10.1145/336512.336523},
    isbn = {1-58113-253-0},
    keywords = {re},
    location = {Limerick, Ireland},
    month = may,
    pages = {35--46},
    posted-at = {2012-01-11 02:53:53},
    priority = {0},
    publisher = {ACM},
    series = {ICSE '00},
    title = {{Requirements engineering: a roadmap}},
    url = {http://dx.doi.org/10.1145/336512.336523},
    year = {2000}
}

@article{Green2011Measuring,
    abstract = {{Over the past several years scrum has grown to become the most commonly used product development method at Adobe Systems. Large desktop software products like Premiere Pro and After Effects, Platform tools like Adobe AIR, and Software as a Service products like Acrobat Connect and Omniture SiteCatalyst are using scrum to become more effective at delivering the right solutions to customers with higher quality. This paper discusses the methods Adobe Systems is putting in place to measure the impact scrum has had on these teams.}},
    address = {Los Alamitos, CA, USA},
    author = {Green, Peter},
    citeulike-article-id = {10212077},
    citeulike-linkout-0 = {http://doi.ieeecomputersociety.org/10.1109/HICSS.2011.306},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/hicss.2011.306},
    comment = {(private-note)We have found that deferred defects tend to be very
strongly correlated to defects found by customers in the field, so this particular metric is a good predictor
of the perceived quality of the software we release.},
    doi = {10.1109/hicss.2011.306},
    isbn = {978-0-7695-4282-9},
    journal = {Hawaii International Conference on System Sciences},
    keywords = {cmds, prediction, reliability, td},
    pages = {1--10},
    posted-at = {2012-01-10 16:46:55},
    priority = {3},
    publisher = {IEEE Computer Society},
    title = {{Measuring the Impact of Scrum on Product Development at Adobe Systems}},
    url = {http://dx.doi.org/10.1109/hicss.2011.306},
    volume = {0},
    year = {2011}
}

@inproceedings{Wagner2007Using,
    abstract = {{The economics and cost of software quality have been discussed in software engineering for decades now. There is clearly a relationship and a need to manage cost and quality in combination. Moreover, economics should be the basis of any quality analysis. However, this implies several issues that have not been addressed to an extent so that managing the economics of software quality is common practice. This paper discusses these issues, possible solutions, and research directions.}},
    author = {Wagner, S.},
    booktitle = {Economics of Software and Computation, 2007. ESC '07. First International Workshop on the},
    citeulike-article-id = {10212050},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/esc.2007.10},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4273048},
    comment = {(private-note)Wenger discusses the economics of software quality and defines a model that considers economic influences and concsequences on the environment the software operates within and the organization responsible for product development and maintenance.  "The quality of the software is then how it influences the activities and its environment in incurring costs and generating benefits."  Research opportunities highlighted include empirical studies of the economics of software quality.},
    doi = {10.1109/esc.2007.10},
    institution = {Tech. Univ. Munchen, Garching},
    isbn = {0-7695-2955-0},
    keywords = {cmds, td},
    month = may,
    pages = {2},
    posted-at = {2012-01-10 16:34:05},
    priority = {0},
    publisher = {IEEE},
    title = {{Using Economics as Basis for Modelling and Evaluating Software Quality}},
    url = {http://dx.doi.org/10.1109/esc.2007.10},
    year = {2007}
}

@book{Humphrey1995Discipline,
    author = {Humphrey, Watts S.},
    booktitle = {A Discipline for Software Engineering},
    citeulike-article-id = {10211940},
    isbn = {0201546108},
    keywords = {cmds},
    location = {Reading, Mass.},
    posted-at = {2012-01-10 15:32:47},
    priority = {2},
    publisher = {Addison-Wesley},
    title = {{A Discipline for Software Engineering}},
    year = {1995}
}

@article{Kitchenham1999Towards,
    abstract = {{We suggest that empirical studies of maintenance are difficult to understand unless the context of the study is fully defined. We developed a preliminary ontology to identify a number of factors that influence maintenance. The purpose of the ontology is to identify factors that would affect the results of empirical studies. We present the ontology in the form of a UML model. Using the maintenance factors included in the ontology, we define two common maintenance scenarios and consider the industrial issues associated with them. Copyright {\copyright} 1999 John Wiley \& Sons, Ltd.}},
    author = {Kitchenham, Barbara A. and Travassos, Guilherme H. and von Mayrhauser, Anneliese and Niessink, Frank and Schneidewind, Norman F. and Singer, Janice and Takada, Shingo and Vehvilainen, Risto and Yang, Hongji},
    citeulike-article-id = {378462},
    citeulike-linkout-0 = {http://dx.doi.org/10.1002/(sici)1096-908x(199911/12)11:6\%3C365::aid-smr200\%3E3.0.co;2-w},
    citeulike-linkout-1 = {http://www3.interscience.wiley.com/cgi-bin/abstract/68502729/ABSTRACT},
    comment = {(private-note)In addition, there needs to be a management process for authorising or rejecting modification
activities after initial investigation of the trigger event. This is usually the responsibility of a change
control board. The authorisation process may also include a process of negotiation with the client
about contractual arrangements for implementing a required modification (e.g., budgets/price and
time-scales). Only after a proposed modification activity is approved by the change control board
and any necessary contractual arrangements are agreed with the client (which, for applications like
operating systems or self-standing products, may be the marketing department), will the proposed
modification activity be scheduled. A change control board can be organised as a formal process
involving meetings between users and customers/clients and maintenance managers, or as a simple
working procedure. The level of formality can affect quality and efficiency. Formal change control
boards are likely to slow the},
    day = {21},
    doi = {10.1002/(sici)1096-908x(199911/12)11:6\%3C365::aid-smr200\%3E3.0.co;2-w},
    issn = {1096-908X},
    journal = {J. Softw. Maint: Res. Pract.},
    keywords = {cmds},
    month = dec,
    number = {6},
    pages = {365--389},
    posted-at = {2012-01-10 13:52:14},
    priority = {2},
    publisher = {John Wiley \& Sons, Ltd.},
    title = {{Towards an ontology of software maintenance}},
    url = {http://dx.doi.org/10.1002/(sici)1096-908x(199911/12)11:6\%3C365::aid-smr200\%3E3.0.co;2-w},
    volume = {11},
    year = {1999}
}

@article{Jones1996Strategies,
    abstract = {{One of the most chronic problems in software development is the
fact that application requirements are almost never stable and fixed.
Frequent changes in requirements are not always caused by capricious
clients (although sometimes they are). The root cause of requirements
volatility is that many applications are attempting to automate domains
that are only partly understood. As software design and development
proceeds, the process of automation begins to expose these ill-defined
situations. Therefore, although creeping requirements are troublesome,
they are often a technical necessity. Several threads of research and
some emerging technologies are aimed at either clarifying requirements
earlier in development or minimizing the disruptive effect of changing
requirements later}},
    author = {Jones, C.},
    citeulike-article-id = {10211699},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/2.507640},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=507640},
    comment = {(private-note)A change-control board
is a group of managers, client representatives, and technical
personnel who decide which change to accept and
which to reject. },
    doi = {10.1109/2.507640},
    institution = {Software Productivity Res. Inc., Burlington, MA, USA},
    issn = {0018-9162},
    journal = {Computer},
    keywords = {re-change, td},
    month = jun,
    number = {6},
    pages = {92--94},
    posted-at = {2012-01-10 13:48:52},
    priority = {2},
    publisher = {IEEE},
    title = {{Strategies for managing requirements creep}},
    url = {http://dx.doi.org/10.1109/2.507640},
    volume = {29},
    year = {1996}
}

@article{KajkoMattsson2000Software,
    abstract = {{A well-functioning process for reporting, analysing and resolving software problems is an important vehicle for establishing and retaining control over the development and maintenance of software products. In this paper we present such a process, its state of practice and its role within corrective software maintenance. This process is utilized at ABB Robotics AB and is called the System Progress Report process (SPR). The SPR process is the result of our 20 years of work and experience. This paper concludes with our lessons learned and plans for future improvements. Copyright {\copyright} 2000 John Wiley \& Sons, Ltd.}},
    author = {Kajko-Mattsson, Mira and Forssander, Stefan and Andersson, Gunnar},
    citeulike-article-id = {10210633},
    citeulike-linkout-0 = {http://dx.doi.org/10.1002/1096-908x(200009/10)12:5\%3C255::aid-smr216\%3E3.0.co;2-l},
    doi = {10.1002/1096-908x(200009/10)12:5\%3C255::aid-smr216\%3E3.0.co;2-l},
    journal = {J. Softw. Maint: Res. Pract.},
    keywords = {cmds, td},
    number = {5},
    pages = {255--285},
    posted-at = {2012-01-09 22:34:22},
    priority = {2},
    publisher = {John Wiley \& Sons, Ltd.},
    title = {{Software problem reporting and resolution process at ABB Robotics AB: state of practice}},
    url = {http://dx.doi.org/10.1002/1096-908x(200009/10)12:5\%3C255::aid-smr216\%3E3.0.co;2-l},
    volume = {12},
    year = {2000}
}

@article{Humphrey1991Software,
    abstract = {{In 1987 and 1990, the Software Engineering Institute conducted process assessments of the Software Engineering Division (SED) of Hughes Aircraft in Fullerton, CA. The first assessment found the SED to be a level two organization, based on the SEI's process-maturity scale of one to five, where one is worst and five is best. The first assessment identified the strengths and weaknesses of the SED, and the SEI made recommendations for process improvement. Hughes then established and implemented an action plan in accordance with these recommendations. The second assessment found the SEP to be a strong level three organization. The authors outline the assessment method used, the findings and recommendations from the initial assessment, the actions taken by Hughes, the lessons learned, and the business and product consequences.>}},
    author = {Humphrey, W. S. and Snyder, T. R. and Willis, R. R.},
    citeulike-article-id = {10210629},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/52.300031},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=300031},
    doi = {10.1109/52.300031},
    institution = {Software Eng. Inst., Pittsburgh, PA, USA},
    issn = {0740-7459},
    journal = {Software, IEEE},
    keywords = {cmds, td},
    month = jul,
    number = {4},
    pages = {11--23},
    posted-at = {2012-01-09 22:29:52},
    priority = {2},
    publisher = {IEEE},
    title = {{Software process improvement at Hughes Aircraft}},
    url = {http://dx.doi.org/10.1109/52.300031},
    volume = {8},
    year = {1991}
}

@article{Clavadetscher1998User,
    abstract = {{The evidence is voluminous, consistent, and incontrovertible. It
applies to corporate, government agency, and military software
development. Quite simply, the software we build does not meet our
customers' needs: those of us who build large software programs fail
miserably-90 percent of the time-to deliver what customers want, when
they want it, at the agreed upon price; we fail to adequately manage the
software development process, user-developer communication breaks down,
the requirements control process breaks down, we have runaway
requirements, budgets, schedules, and  ” death march”
projects. The Best Practices Framework of the Software Program Manager's
Network outlines some solutions. First, we must identify what can go
wrong. Precedents give ample hints regarding risks. We need to manage
the development process with more attention, particularly to what might
go wrong. Second, we must manage the most fundamental part of our task:
defining our goal. We fail to use requirements management to surface
(early) errors or problems, to baseline and track changes, and to
improve user-developer communication}},
    author = {Clavadetscher, C.},
    citeulike-article-id = {10210626},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/52.663781},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=663781},
    doi = {10.1109/52.663781},
    institution = {Nat. Defense Univ., USA},
    issn = {0740-7459},
    journal = {Software, IEEE},
    keywords = {td},
    month = mar,
    number = {2},
    pages = {30, 32+},
    posted-at = {2012-01-09 22:23:17},
    priority = {2},
    publisher = {IEEE},
    title = {{User involvement: key to success}},
    url = {http://dx.doi.org/10.1109/52.663781},
    volume = {15},
    year = {1998}
}

@electronic{CohnShould,
    author = {Cohn, Mike},
    citeulike-article-id = {10107511},
    citeulike-linkout-0 = {http://blog.mountaingoatsoftware.com/should-story-points-be-assigned-to-a-bug-fixing-story},
    keywords = {td},
    posted-at = {2011-12-08 18:58:39},
    priority = {0},
    title = {{Should Story Points Be Assigned to A Bug-Fixing Story | Mike Cohn's Blog - Succeeding With Agile®}},
    url = {http://blog.mountaingoatsoftware.com/should-story-points-be-assigned-to-a-bug-fixing-story}
}

@electronic{Whats,
    citeulike-article-id = {10107485},
    citeulike-linkout-0 = {http://www.thetestingplanet.com/2011/04/what\%E2\%80\%99s-a-defect-worth/},
    journal = {The Testing Planet},
    keywords = {td},
    posted-at = {2011-12-08 18:44:13},
    priority = {0},
    title = {{What's a defect worth?}},
    url = {http://www.thetestingplanet.com/2011/04/what\%E2\%80\%99s-a-defect-worth/}
}

@inproceedings{NicoZazworkaComparing,
    abstract = {{Background:     Software     systems     accumulate 
technical   debt   (TD)   when   short-term   goals   in   software 
development  are  traded  for  long  term  goals  (e.g.,  quick-and-
dirty  implementation  to  reach  a  release  date  vs.  a  well-
refactored  implementation  that supports the  long term  health 
of  the  project).  The  code-level  debt  accumulated  by  a  system 
can   be   identified   through   different   source   code   analysis 
techniques;  what  has  not  been  studied  is  how  a  larger  set  of 
techniques   overlap   with   respect   to   flagging   source   code 
components,   and   how   the   use   of   these   techniques   helps 
identifying code components that are in debt. 
Aims:  Comparing  the  results  of  different  TD  identification 
approaches to understand their commonalities and differences 
and to evaluate their relationship to software quality.  
Method:    We    selected    four    different    TD    identification 
techniques (code smells, automatic static analysis (ASA) issues, 
grime buildup, and modularity violations) and applied them to 
13  versions  of  the  Apache  Hadoop  open  source  software 
project.  We  collected  and  aggregated  statistical  measures  to 
investigate  whether  the  different  techniques  identified  TD 
indicators  in  the  same  classes  and  whether  those  classes 
exposed high defect and change proneness. 
Results: The four approaches have very little overlap and are 
therefore   pointing   to   different   problems   in   source   code. 
Dispersed  coupling  and  modularity  violations  were  co-located 
in classes with higher defect proneness, and we also observed a 
strong  relationship  between  modularity  violations  and  change 
proneness. 
Conclusions: Our main contribution is an initial TD landscape 
figure  that  shows  that  different  TD  techniques  and  related 
indicators are loosely coupled and therefore indicate problems 
in  different  locations  of  the  source  code.  Moreover,  only  a 
small subset of TD indicators is strongly and directly related to 
quality indicators.}},
    author = {Nico Zazworka, Antonio V. and Yuangfang Cai, Carolyn S.},
    citeulike-article-id = {10104326},
    citeulike-linkout-0 = {http://www.google.com/search?q=Comparing+Four+Approaches+for+Technical+Debt+Identification\&\#38;sourceid=ie7\&\#38;rls=com.microsoft:en-us:IE-Address\&\#38;ie=\&\#38;oe=\&\#38;rlz=1I7GGHP\_en},
    keywords = {td},
    posted-at = {2011-12-07 21:00:09},
    priority = {3},
    title = {{Comparing Four Approaches for Technical Debt Identification - Google Search}},
    url = {http://www.google.com/search?q=Comparing+Four+Approaches+for+Technical+Debt+Identification\&\#38;sourceid=ie7\&\#38;rls=com.microsoft:en-us:IE-Address\&\#38;ie=\&\#38;oe=\&\#38;rlz=1I7GGHP\_en}
}

@inproceedings{DAmbros2010Impact,
    abstract = {{The presence of design flaws in a software system has a negative impact on the quality of the software, as they indicate violations of design practices and principles, which make a software system harder to understand, maintain, and evolve. Software defects are tangible effects of poor software quality. In this paper we study the relationship between software defects and a number of design flaws. We found that, while some design flaws are more frequent, none of them can be considered more harmful with respect to software defects. We also analyzed the correlation between the introduction of new flaws and the generation of defects.}},
    author = {D'Ambros, M. and Bacchelli, A. and Lanza, M.},
    booktitle = {Quality Software (QSIC), 2010 10th International Conference on},
    citeulike-article-id = {10104321},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/qsic.2010.58},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5562941},
    doi = {10.1109/qsic.2010.58},
    institution = {Fac. of Inf., REVEAL, Univ. of Lugano, Lugano, Switzerland},
    isbn = {978-1-4244-8078-4},
    issn = {1550-6002},
    keywords = {td},
    month = jul,
    pages = {23--31},
    posted-at = {2011-12-07 20:55:33},
    priority = {4},
    publisher = {IEEE},
    title = {{On the Impact of Design Flaws on Software Defects}},
    url = {http://dx.doi.org/10.1109/qsic.2010.58},
    year = {2010}
}

@article{Sarkar2007APIBased,
    abstract = {{We present in this paper a new set of metrics that measure the quality of modularization of a non-object-oriented software system. We have proposed a set of design principles to capture the notion of modularity and defined metrics centered around these principles. These metrics characterize the software from a variety of perspectives: structural, architectural, and notions such as the similarity of purpose and commonality of goals. (By structural, we are referring to intermodule coupling-based notions, and by architectural, we mean the horizontal layering of modules in large software systems.) We employ the notion of API (application programming interface) as the basis for our structural metrics. The rest of the metrics we present are in support of those that are based on API. Some of the important support metrics include those that characterize each module on the basis of the similarity of purpose of the services offered by the module. These metrics are based on information-theoretic principles. We tested our metrics on some popular open-source systems and some large legacy-code business applications. To validate the metrics, we compared the results obtained on human-modularized versions of the software (as created by the developers of the software) with those obtained on randomized versions of the code. For randomized versions, the assignment of the individual functions to modules was randomized}},
    author = {Sarkar, S. and Rama, G. M. and Kak, A. C.},
    booktitle = {Software Engineering, IEEE Transactions on},
    citeulike-article-id = {2635742},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/tse.2007.256942},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4027146},
    doi = {10.1109/tse.2007.256942},
    institution = {SETLabs, Infosys Technol. Ltd., Bangalore},
    issn = {0098-5589},
    journal = {Software Engineering, IEEE Transactions on},
    keywords = {metrics},
    month = jan,
    number = {1},
    pages = {14--32},
    posted-at = {2011-11-30 16:49:03},
    priority = {0},
    publisher = {IEEE},
    title = {{API-Based and Information-Theoretic Metrics for Measuring the Quality of Software Modularization}},
    url = {http://dx.doi.org/10.1109/tse.2007.256942},
    volume = {33},
    year = {2007}
}

@article{Hayes2005Maintainability,
    abstract = {{In order to build predictors of the maintainability of evolving software, we first need a means for measuring maintainability as well as a training set of software modules for which the actual maintainability is known. This paper describes our success at building such a predictor. Numerous candidate measures for maintainability were examined, including a new compound measure. Two datasets were evaluated and used to build a maintainability predictor. The resulting model, Maintainability Prediction Model (MainPredMo), was validated against three held-out datasets. We found that the model possesses predictive accuracy of 83\% (accurately predicts the maintainability of 83\% of the modules). A variant of MainPredMo, also with accuracy of 83\%, is offered for interested researchers.}},
    author = {Hayes, J. H. and Zhao, L.},
    booktitle = {Software Maintenance, 2005. ICSM'05. Proceedings of the 21st IEEE International Conference on},
    citeulike-article-id = {2948590},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/icsm.2005.59},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1510154},
    doi = {10.1109/icsm.2005.59},
    isbn = {0-7695-2368-4},
    issn = {1063-6773},
    journal = {Software Maintenance, 2005. ICSM'05. Proceedings of the 21st IEEE International Conference on},
    keywords = {td},
    pages = {601--604},
    posted-at = {2011-11-17 23:28:10},
    priority = {4},
    publisher = {IEEE},
    title = {{Maintainability prediction: a regression analysis of measures of evolving systems}},
    url = {http://dx.doi.org/10.1109/icsm.2005.59},
    year = {2005}
}

@inproceedings{Hayes2004Metricsbased,
    abstract = {{We derive a model for estimating adaptive software maintenance effort in person hours, the adaptive maintenance effort model (AMEffMo). A number of metrics such as lines of code changed and number of operators changed were found to be strongly correlated to maintenance effort. The regression models performed well in predicting adaptive maintenance effort as well as provide useful information for managers and maintainers.}},
    author = {Hayes, J. H. and Patel, S. C. and Zhao, Liming},
    booktitle = {Eighth European Conference on Software Maintenance and Reengineering, 2004. CSMR 2004. Proceedings.},
    citeulike-article-id = {10037371},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/csmr.2004.1281427},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1281427},
    doi = {10.1109/csmr.2004.1281427},
    isbn = {0-7695-2107-X},
    issn = {1534-5351},
    keywords = {chs-fse2012, ds, td},
    location = {Tampere, Finland},
    month = mar,
    pages = {254--258},
    posted-at = {2011-11-17 23:25:10},
    priority = {4},
    publisher = {IEEE},
    title = {{A metrics-based software maintenance effort model}},
    url = {http://dx.doi.org/10.1109/csmr.2004.1281427},
    year = {2004}
}

@inproceedings{Robinson2008Defectdriven,
    abstract = {{Software quality improvement initiatives are frequently attempted inside major software development companies. These initiatives often face difficulty motivating managers and developers. Yet the success of these improvement initiatives is directly related to receiving support from them. A defect-driven improvement process is proposed, aiming to improve the adoption of techniques by these key groups. The process is based on using defect data to identify issues in the current development process. ABB has used this process to focus and motivate change in three major development organizations around the world. Results show that this process yields measurable improvements in organizations that have been reluctant to change in the past.}},
    address = {New York, NY, USA},
    author = {Robinson, Brian and Francis, Patrick and Ekdahl, Fredrik},
    booktitle = {Proceedings of the Second ACM-IEEE international symposium on Empirical software engineering and measurement},
    citeulike-article-id = {4079732},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1414004.1414072},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1414004.1414072},
    doi = {10.1145/1414004.1414072},
    isbn = {978-1-59593-971-5},
    keywords = {chs},
    location = {Kaiserslautern, Germany},
    pages = {333--335},
    posted-at = {2011-07-01 22:44:11},
    priority = {0},
    publisher = {ACM},
    series = {ESEM '08},
    title = {{A defect-driven process for software quality improvement}},
    url = {http://dx.doi.org/10.1145/1414004.1414072},
    year = {2008}
}

@article{Eick2001Does,
    abstract = {{A central feature of the evolution of large software systems is that change-which is necessary to add new functionality, accommodate new hardware, and repair faults-becomes increasingly difficult over time. We approach this phenomenon, which we term code decay, scientifically and statistically. We define code decay and propose a number of measurements (code decay indices) on software and on the organizations that produce it, that serve as symptoms, risk factors, and predictors of decay. Using an unusually rich data set (the fifteen-plus year change history of the millions of lines of software for a telephone switching system), we find mixed, but on the whole persuasive, statistical evidence of code decay, which is corroborated by developers of the code. Suggestive indications that perfective maintenance can retard code decay are also discussed}},
    address = {Los Alamitos, CA, USA},
    author = {Eick, S. G. and Graves, T. L. and Karr, A. F. and Marron, J. S. and Mockus, A.},
    citeulike-article-id = {356159},
    citeulike-linkout-0 = {http://doi.ieeecomputersociety.org/10.1109/32.895984},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/32.895984},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=895984},
    doi = {10.1109/32.895984},
    issn = {00985589},
    journal = {IEEE Transactions on Software Engineering},
    keywords = {chs},
    month = jan,
    number = {1},
    pages = {1--12},
    posted-at = {2011-06-07 21:46:53},
    priority = {2},
    publisher = {IEEE},
    title = {{Does code decay? Assessing the evidence from change management data}},
    url = {http://dx.doi.org/10.1109/32.895984},
    volume = {27},
    year = {2001}
}

@article{Raccoon1996Learning,
    abstract = {{I believe that many software engineers have heard of learning curves. We know that new skills take time to learn, that developers take a while to ramp up on a new project, and that improvement slows as time goes on. I also believe that we share many misconceptions about learning curves. We don't assume that learning curves are relevant to our software projects and we don't use them in our processes. In this paper, I want to raise the level of understanding of learning curves within the Software Engineering community. This paper is for managers and developers who want to better understand learning curves.Learning curves are much more than a hurdle to full productivity. They are both a metaphor and a specific set of equations that describe the most common patterns of improvement within stable processes. Learning curves explain why the productivity of a stable process changes the way that it does, why productivity is lowest at the start of a project and highest at the end of a project. Learning curves explain that though the improvements diminish throughout the process, the improvements continue adding up. And, we can use learning curves to predict future productivity.This paper is laid out as follows. In the first section, I define learning curves, describe their history, and argue that they apply to Software Engineering. In the second section, I describe the need to both stabilize and improve a process. Our concept of process influences how we try to improve it, so I discuss the implications of two different concepts of process. Learning curves denote the relationship between stability and improvement. In the third section, I discuss the implications of learning curves on staffing a project. Specifically, I show that Brooks's observations about man-months can be explained in terms of learning curves and that we need to keep teams together on long-term projects. And in the fourth section, I comment on several technical issues one might encounter when applying learning curves to software development. I describe the equations one might use to model a process as well as the affect learning curves typically have on software engineering projects.}},
    address = {New York, NY, USA},
    author = {Raccoon},
    citeulike-article-id = {673486},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=381805},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/381790.381805},
    doi = {10.1145/381790.381805},
    issn = {0163-5948},
    journal = {SIGSOFT Softw. Eng. Notes},
    keywords = {lcm},
    month = jan,
    number = {1},
    pages = {77--86},
    posted-at = {2011-05-06 02:25:35},
    priority = {4},
    publisher = {ACM},
    title = {{A learning curve primer for software engineers}},
    url = {http://dx.doi.org/10.1145/381790.381805},
    volume = {21},
    year = {1996}
}

@article{Godfrey2009Future,
    abstract = {{When mining software archives, we want to learn from the past to shape the future. But what does the research so far tell us about the future of the field itself? For this special issue, we invited and collected statements from nine research leaders in the field. These statements show opportunities for data collection and exploitation (Michael Godfrey, Ahmad Hassan, and James Herbsleb), enhancing programmer productivity (Gail Murphy and Martin Robillard), examining the role of social networking (Prem Devanbu), leveraging data for industry (Audris Mockus), and answering open research questions (Dewayne Perry). David Notkin, though, warns against too much enthusiasm: \~{A}'\^{A}¿Let us not mine for fool\~{A}'\^{A}¿s gold.\~{A}'\^{A}¿ Enjoy! \~{A}'\^{A}¿Nachiappan Nagappan, Andreas Zeller, and Thomas Zimmermann, Guest Editors}},
    address = {Los Alamitos, CA, USA},
    author = {Godfrey, Michael W. and Hassan, Ahmed E. and Herbsleb, James and Murphy, Gail C. and Robillard, Martin and Devanbu, Prem and Mockus, Audris and Perry, Dewayne E. and Notkin, David},
    citeulike-article-id = {7549051},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1495958},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/ms.2009.10},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4721185},
    comment = {(private-note)There are some good ideas in here including looking for changes that occur simultaneously, some social network ideas, and ideas for questions that need answering.  Targeting an end use of the data is mentioned by only one person.

Think about the intersection of project data and software metrics.  Is there an opportunity there to say something about projects.},
    doi = {10.1109/ms.2009.10},
    institution = {University of Waterloo},
    issn = {0740-7459},
    journal = {Software, IEEE},
    keywords = {chs},
    month = jan,
    number = {1},
    pages = {67--70},
    posted-at = {2011-04-13 17:41:03},
    priority = {0},
    publisher = {IEEE},
    title = {{Future of Mining Software Archives: A Roundtable}},
    url = {http://dx.doi.org/10.1109/ms.2009.10},
    volume = {26},
    year = {2009}
}

@article{MinkiewiczEvolution,
    author = {Minkiewicz, Arlene F.},
    citeulike-article-id = {8977545},
    citeulike-linkout-0 = {http://www.crosstalkonline.org/storage/issue-archives/2009/200903/200903-Minkiewicz.pdf},
    journal = {Crosstalk},
    keywords = {benchmarking},
    posted-at = {2011-03-10 22:06:04},
    priority = {4},
    title = {{The Evolution of Software Size: A Search for Value}},
    url = {http://www.crosstalkonline.org/storage/issue-archives/2009/200903/200903-Minkiewicz.pdf}
}

@misc{CagleyAgile,
    author = {Cagley, Thomas},
    citeulike-article-id = {8977540},
    citeulike-linkout-0 = {http://www.davidconsultinggroup.com/estimationservices/Agile\%20Estimation\%20Using\%20Functional\%20Metrics.pdf},
    howpublished = {web site},
    posted-at = {2011-03-10 22:02:46},
    priority = {4},
    title = {{Agile Estimation Using Functional Metrics}},
    url = {http://www.davidconsultinggroup.com/estimationservices/Agile\%20Estimation\%20Using\%20Functional\%20Metrics.pdf}
}

@inproceedings{Redwine1985Software,
    abstract = {{We have reviewed the growth and propagation of a variety of software technologies in an attempt to discover natural characteristics of the process as well as principles and techniques useful in transitioning modern software technology into widespread use. What we have looked at is the technology maturation process, the process by which a piece of technology is first conceived, then shaped into something usable, and finally  ” marketed” to the point that it is found in the repertoire of a majority of professionals.A major interest is the time required for technology maturation — and our conclusion is that technology maturation generally takes much longer than popularly thought, especially for major technology areas. But our prime interest is in determining what actions, if any, can accelerate the maturation of technology, in particular that part of maturation that has to do with transitioning the technology into widespread use. Our observations concerning maturation facilitators and inhibitors are the major subject of this paper.}},
    address = {Los Alamitos, CA, USA},
    author = {Redwine, Samuel T. and Riddle, William E.},
    booktitle = {Proceedings of the 8th international conference on Software engineering},
    citeulike-article-id = {742911},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=319568.319624},
    comment = {(private-note)Provides some good theory for mapping the maturity of software technologies.  The data studied is out of date however.  There is a chasm between classes of software technologies now because of the internet, therefore adoption rates should differ significantly between classes of technologies such as operating systems and web-based tools.  },
    isbn = {0-8186-0620-7},
    keywords = {lcm},
    location = {London, England},
    pages = {189--200},
    posted-at = {2011-03-02 16:57:17},
    priority = {0},
    publisher = {IEEE Computer Society Press},
    series = {ICSE '85},
    title = {{Software technology maturation}},
    url = {http://portal.acm.org/citation.cfm?id=319568.319624},
    year = {1985}
}

@article{Boehm2005Management,
    abstract = {{Discussions with traditional developers and managers concerning agile software development practices nearly always contain two somewhat contradictory ideas. They find that on small, stand-alone projects, agile practices are less burdensome and more in tune with the software industry's increasing needs for rapid development and coping with continuous change. Managers face several barriers, real and perceived, when they try to bring agile approaches into traditional organizations. They categorized the barriers either as problems only in terms of scope or scale, or as significant general issues needing resolution. From these two categories, we've identified three areas - development process conflicts, business process conflicts, and people conflicts - that we believe are the critical challenges to software managers of large organizations in bringing agile approaches to bear in their projects.}},
    address = {Los Alamitos, CA, USA},
    author = {Boehm, B. and Turner, R.},
    citeulike-article-id = {6252649},
    citeulike-linkout-0 = {http://doi.ieeecomputersociety.org/10.1109/MS.2005.129},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/ms.2005.129},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1504661},
    comment = {(private-note)Discusses general issues for tranistioning a waterfall oriented development organization to agile development model.  Says cost estimation is a significant issue.},
    doi = {10.1109/ms.2005.129},
    institution = {Center for Software Eng., Univ. of Southern California, Los Angeles, CA, USA},
    issn = {0740-7459},
    journal = {Software, IEEE},
    keywords = {lcm},
    month = sep,
    number = {5},
    pages = {30--39},
    posted-at = {2011-02-25 20:30:24},
    priority = {0},
    publisher = {IEEE},
    title = {{Management challenges to implementing agile processes in traditional development organizations}},
    url = {http://dx.doi.org/10.1109/ms.2005.129},
    volume = {22},
    year = {2005}
}

@inproceedings{DAmbros2009Relationship,
    abstract = {{Change coupling is the implicit relationship between two or more software artifacts that have been observed to frequently change together during the evolution of a software system. Researchers have studied this dependency and have observed that it points to design issues such as architectural decay. It is still unknown whether change coupling correlates with a tangible effect of design issues, i.e., software defects.In this paper we analyze the relationship between change coupling and software defects on three large software systems. We investigate whether change coupling correlates with defects, and if the performance of bug prediction models based on software metrics can be improved with change coupling information.}},
    address = {Los Alamitos, CA, USA},
    author = {D'Ambros, M. and Lanza, M. and Robbes, R.},
    booktitle = {Reverse Engineering, 2009. WCRE \&\#039;09. 16th Working Conference on},
    citeulike-article-id = {8802775},
    citeulike-linkout-0 = {http://doi.ieeecomputersociety.org/10.1109/WCRE.2009.19},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/wcre.2009.19},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5328803},
    doi = {10.1109/wcre.2009.19},
    institution = {Fac. of Inf., REVEAL, Univ. of Lugano, Lugano, Switzerland},
    isbn = {978-0-7695-3867-9},
    issn = {1095-1350},
    journal = {Reverse Engineering, Working Conference on},
    keywords = {chs, impact\_analysis},
    location = {Lille, France},
    month = oct,
    pages = {135--144},
    posted-at = {2011-02-10 21:24:52},
    priority = {3},
    publisher = {IEEE},
    title = {{On the Relationship Between Change Coupling and Software Defects}},
    url = {http://dx.doi.org/10.1109/wcre.2009.19},
    volume = {0},
    year = {2009}
}

@inproceedings{albrecht79:_fpa,
    author = {Albrecht, Aj},
    booktitle = {IBM Application Development Symp.},
    citeulike-article-id = {387510},
    editor = {Press, I. B. M.},
    journal = {Proc. of IBM Application Development Symp.},
    keywords = {lcm},
    month = oct,
    pages = {83--92},
    posted-at = {2011-02-10 21:23:16},
    priority = {2},
    title = {{Measuring Application Development Productivity}},
    year = {1979}
}

@article{Benaroch2010Financial,
    abstract = {{The ability to price (monetize) software development risks can benefit various aspects of software development. Cost estimators predict project cost by adjusting a project's nominal cost on the basis of risk factors' (cost drivers') expected values, but the predicted cost is often inaccurate because risk factors' actual values normally deviate from expectations. Because variability is a widely used risk measure in finance, this risk-pricing method relates risk factor variability to project cost variability. The method estimates two parameters for each risk factor: extra cost incurred per unit exposure and project sensitivity. Several areas can benefit from the benchmark risk-pricing parameters obtained when applying this method with a cost estimator such as Cocomo.}},
    author = {Benaroch, Michel and Appari, Ajit},
    citeulike-article-id = {8777962},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/ms.2010.28},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5396317},
    comment = {(private-note)This looks promising for LCM.  Using a risk based approach with monte-carlo simulation or simple pert techniques could form the basis for a LCM.  Rolling up risk costs by project to the portfolio level is a good idea.  Risk data are hard to estimate and harder to verify as there are many decisions along each project.  LCM also is supposed to focus on the long term viability of the product so that it does not devolve into the more maintenance cost than revenue phase.  Perhaps this is unavoidable given market realities of competition and commoditization for software systems.  So we need a way to quantify architecture risk across multiple releases in both a backward looking and forward looking way.  Certain factors from projects may be useful for that, however, we may need additional factors to quantify the architecture risk whithin a system as it evolves.  Perhaps some metrics would help to aggregate program complexity pr program dependency data to the system level where it could be used to assess systematic risk.  Evaluating measures over time could be enlightening.},
    doi = {10.1109/ms.2010.28},
    issn = {0740-7459},
    journal = {IEEE Software},
    keywords = {lcm},
    month = sep,
    number = {5},
    pages = {65--73},
    posted-at = {2011-02-07 15:32:48},
    priority = {5},
    title = {{Financial Pricing of Software Development Risk Factors}},
    url = {http://dx.doi.org/10.1109/ms.2010.28},
    volume = {27},
    year = {2010}
}

@article{Pan2006Bug,
    abstract = {{In this paper, we introduce 13 program slicing metrics for C language programs. These metrics use program slice information to measure the size, complexity, coupling, and cohesion properties of programs. Compared with traditional code metrics based on code statements or code structure, program slicing metrics involve measures for program behaviors. To evaluate the program slicing metrics, we compare them with the Understand for C++ suite of metrics, a set of widely-used traditional code metrics, in a series of bug classification experiments. We used the program slicing and the Understand for C++ metrics computed for 887 revisions of the Apache HTTP project and 76 revisions of the Latex2rtf project to classify source code files or functions as either buggy or bug-free. We then compared their classification prediction accuracy. Program slicing metrics have slightly better performance than the Understand for C++ metrics in classifying buggy/bug-free source code. Program slicing metrics have an overall 82.6\% (Apache) and 92\% (Latex2rtf) accuracy at the file level, better than the Understand for C++ metrics with an overall 80.4\% (Apache) and 88\% (Latex2rtf) accuracy. The experiments illustrate that the program slicing metrics have at least the same bug classification performance as the Understand for C++ metrics.}},
    address = {Washington, DC, USA},
    author = {Pan, Kai and Kim, S. and Whitehead, E. J.},
    booktitle = {Source Code Analysis and Manipulation, 2006. SCAM '06. Sixth IEEE International Workshop on},
    citeulike-article-id = {5343984},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1173699.1174125},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/scam.2006.6},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4026853},
    comment = {(private-note)This paper performs a comparative analysis of the ability of program slicing metrics and understand complexity metrics to classify files and functions as bug prone.  The bug proneness of files or functions is based on analysis of change history and is a binary classification.  They use CodeSurfer to compute program slice metrics at the function level then aggregate those via summation or average to the file level.  They apply the technique to Apache HTTP and Latex2rtf open source projects.  The results for files and functions have similar accuracy improvement over the Understand metrics.  },
    doi = {10.1109/scam.2006.6},
    isbn = {0-7695-2353-6},
    keywords = {impact\_analysis},
    location = {Philadelphia, PA, USA},
    month = sep,
    pages = {31--42},
    posted-at = {2011-02-04 16:28:39},
    priority = {0},
    publisher = {IEEE},
    title = {{Bug Classification Using Program Slicing Metrics}},
    url = {http://dx.doi.org/10.1109/scam.2006.6},
    year = {2006}
}

@article{Binkley2003Largescale,
    abstract = {{A large-scale study of 43 C programs totaling just over 1 million lines of code is presented. The study includes the forward and backward static slice on every executable statement. In total 2353598 slices were constructed, with an average slice size being just under 30\% of the original program. The results also show that ignoring calling-context led to a 50\% increase in average slice size and, in contrast to previous results, a 66-77\% increase in computation time (due to the increased size). Though not the principal focus of the study, the results also show an average pace for the slicing engine, on a standard PC, of 3 million lines of code per second thereby providing additional evidence for static slicing's practicability.}},
    address = {Los Alamitos, CA, USA},
    author = {Binkley, D. and Harman, M.},
    booktitle = {Software Maintenance, 2003. ICSM 2003. Proceedings. International Conference on},
    citeulike-article-id = {7468092},
    citeulike-linkout-0 = {http://doi.ieeecomputersociety.org/10.1109/ICSM.2003.1235405},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/icsm.2003.1235405},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1235405},
    doi = {10.1109/icsm.2003.1235405},
    isbn = {0-7695-1905-9},
    issn = {1063-6773},
    journal = {Software Maintenance, IEEE International Conference on},
    keywords = {impact\_analysis},
    location = {Amsterdam, Netherlands},
    pages = {44--53},
    posted-at = {2011-02-04 16:24:25},
    priority = {3},
    publisher = {IEEE},
    title = {{A large-scale empirical study of forward and backward static slice size and context sensitivity}},
    url = {http://dx.doi.org/10.1109/icsm.2003.1235405},
    volume = {0},
    year = {2003}
}

@inproceedings{Kim2006Automatic,
    abstract = {{Bug-fixes are widely used for predicting bugs or finding risky parts of software. However, a bug-fix does not contain information about the change that initially introduced a bug. Such bug-introducing changes can help identify important properties of software bugs such as correlated factors or causalities. For example, they reveal which developers or what kinds of source code changes introduce more bugs. In contrast to bug-fixes that are relatively easy to obtain, the extraction of bugintroducing changes is challenging. In this paper, we present algorithms to automatically and accurately identify bug-introducing changes. We remove false positives and false negatives by using annotation graphs, by ignoring non-semantic source code changes, and outlier fixes. Additionally, we validated that the fixes we used are true fixes by a manual inspection. Altogether, our algorithms can remove about 38\%\~{}51\% of false positives and 14\%\~{}15\% of false negatives compared to the previous algorithm. Finally, we show applications of bug-introducing changes that demonstrate their value for research.}},
    address = {Washington, DC, USA},
    author = {Kim, Sunghun and Zimmermann, Thomas and Pan, Kai and James Jr},
    booktitle = {Proceedings of the 21st IEEE/ACM International Conference on Automated Software Engineering},
    citeulike-article-id = {4594914},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1169308},
    citeulike-linkout-1 = {http://doi.ieeecomputersociety.org/10.1109/ASE.2006.23},
    citeulike-linkout-2 = {http://dx.doi.org/10.1109/ase.2006.23},
    citeulike-linkout-3 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4019564},
    comment = {(private-note)This paper describes in detail how to perform the analysis to automatically find a bug-introducing change.  First the bug fix is identified, then the change history of that portion (hunk) of the program is examined using HATARI that follows three key steps.  It first eliminates changes that were to white-space or comments, then eliminates changes only to formatting, then eliminates change records that have a high number of affected change hunks.  This increases the accuracy of identifying by automated means the bug introducing change as compared with identifying them by manual means.  },
    doi = {10.1109/ase.2006.23},
    isbn = {0-7695-2579-2},
    issn = {1527-1366},
    journal = {Automated Software Engineering, International Conference on},
    keywords = {chs, chs-fse2012, impact\_analysis},
    location = {Tokyo},
    pages = {81--90},
    posted-at = {2011-02-04 16:16:17},
    priority = {0},
    publisher = {IEEE Computer Society},
    title = {{Automatic Identification of Bug-Introducing Changes}},
    url = {http://dx.doi.org/10.1109/ase.2006.23},
    volume = {0},
    year = {2006}
}

@inproceedings{Sherriff2008Empirical,
    abstract = {{Verification and validation techniques often generate various forms of software development artifacts. Change records created from verification and validation efforts show how files in the system tend to change together in response to fixes for identified faults and failures. We propose a methodology for determining the impact of a new system modification by analyzing software change records through singular value decomposition. This methodology generates clusters of files that historically tend to change together to address faults and failures found in the code base. We performed a post hoc case study using this technique on five open source software systems. We determined that our technique was effective in identifying impacted files in a system from an introduced change when the developers tended to make small, targeted updates to the source system regularly. We further compared our technique against two other impact analysis techniques (Pathlmpact and Coveragelmpact) and found that our technique provided comparable results, while also identifying non-source files that could be impacted by the change.}},
    author = {Sherriff, Mark and Williams, Laurie},
    citeulike-article-id = {8766913},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/icst.2008.25},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4539554},
    comment = {(private-note)Uses singular value decomposition to identify the files that change together in a version history archive. },
    doi = {10.1109/icst.2008.25},
    keywords = {impact\_analysis},
    location = {Lillehammer, Norway},
    month = apr,
    pages = {268--277},
    posted-at = {2011-02-04 16:06:19},
    priority = {0},
    title = {{Empirical Software Change Impact Analysis using Singular Value Decomposition}},
    url = {http://dx.doi.org/10.1109/icst.2008.25},
    year = {2008}
}

@electronic{Observational1998Parallel,
    abstract = {{di erences between this journal submission for the ICSE98 special issue and the ICSE98 conference paper are as follows: Overall the paper is about 50 \% longer than the conference version Our understanding of the details of 5ess process has changed- we have corrected section 3.2 We added section 5 which deals with the e ects of parallel changes, correlating the increased parallelism with increased faults. The other parts of the paper forecasting and summarizing re ect this addition. We moved the interfering changes subsection to this section and expanded somewhat We added further analysis and data extending our understanding of parallel change phenomena, particularly gure 8 and reorganized the section moving from higher to lower levels. We added the appropriate information to the validity section about the quality correlations We expanded the summary and evaluation section: added sections 7.3 and 7.4 Section 7.5 has been updated to re ect the current plans}},
    author = {Observational, An and Perry, Dewayne E. and Siy, Harvey P. and Votta, Lawrence G.},
    citeulike-article-id = {8766896},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.3449},
    keywords = {impact\_analysis},
    posted-at = {2011-02-04 16:00:57},
    priority = {2},
    title = {{Parallel Changes in Large Scale Software Development:}},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.3449},
    year = {1998}
}

@inproceedings{Queille1994Impact,
    abstract = {{If better tools are to be developed to support the impact analysis
task, greater clarity is needed about the exact nature of that task.
This paper presents a definition of impact analysis and distinguishes
between impact analysis and program understanding. Impact analysis is a
necessarily approximate technique which must focus on the cost-effective
minimization of unwanted side-effects. A key to effectiveness would be a
way of precisely describing the semantics of each software change. A
general model of software impacts is presented that uses flexible
declarative propagation rules to describe the way software objects
affect each other. Preliminary versions of the model have been
implemented in an impact analysis system. A small case study was
performed to gain experience in applying the model as part of a software
change process. Recommendations are made for impact analysis tool
support and for further research directions}},
    author = {Queille and Voidrot and Wilde and Munro},
    citeulike-article-id = {707550},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/icsm.1994.336771},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=336771},
    comment = {(private-note)This paper describes a framework for performing impact analysis on code elements.  The Impact analsis traces dependencies of specfic types between certain objects that are incurred during a change cycle.  The framework form includes a source object,  a modification type, a link/dependency type, a destintion object, and the modification type of the destination object.  

They have the following form: 
"impact  (01,  M1, D, 02, M2) 
which means that if  an object 01, of  type 01, is affected 
by  a modification of type M1, then all objects of  type 02 
which depend on 01 through a dependency link of type D 
are impacted, and consequently affected by  a modification 
of  type M2 (which will be propagated by itself). The framework is simply structured and should cover a very generic view of the change impact analysis."},
    doi = {10.1109/icsm.1994.336771},
    journal = {Software Maintenance, 1994. Proceedings., International Conference on},
    keywords = {impact\_analysis},
    location = {Victoria, BC, Canada},
    pages = {234--242},
    posted-at = {2011-02-03 20:20:22},
    priority = {0},
    title = {{The impact analysis task in software maintenance: a model and a case study}},
    url = {http://dx.doi.org/10.1109/icsm.1994.336771},
    year = {1994}
}

@inproceedings{Parnin2008Improving,
    abstract = {{Software archives are one of the best sources available to researchers for understanding the software development process. However, much detective work is still necessary in order to unravel the software development story. During this process, researchers must isolate changes and follow their trails over time. In support of this analysis, several research tools have provided different representations for connecting the many changes extracted from software archives. Most of these tools are based on textual analysis of source code and use line-based differencing between software versions. This approach limits the ability to process changes structurally resulting in less concise and comparable items. Adoption of structure-based approaches have been hampered by complex implementations and overly verbose change descriptions. We present a technique for expressing changes that is fine-grained but preserves some structural aspects. The structural information itself may not have changed, but instead provides a context for interpreting the change. This in turn, enables more relevant and concise descriptions in terms of software types and programming activities. We apply our technique to common challenges that researchers face, and then we discuss and compare our results with other techniques.}},
    address = {New York, NY, USA},
    author = {Parnin, Chris and G\"{o}rg, Carsten},
    booktitle = {Proceedings of the 2008 international working conference on Mining software repositories},
    citeulike-article-id = {7132923},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1370765},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1370750.1370765},
    doi = {10.1145/1370750.1370765},
    isbn = {978-1-60558-024-1},
    keywords = {chs},
    location = {Leipzig, Germany},
    pages = {51--60},
    posted-at = {2011-01-20 20:08:05},
    priority = {2},
    publisher = {ACM},
    series = {MSR '08},
    title = {{Improving change descriptions with change contexts}},
    url = {http://dx.doi.org/10.1145/1370750.1370765},
    year = {2008}
}

@inproceedings{Zimmermann2009Crossproject,
    abstract = {{Prediction of software defects works well within projects as long as there is a sufficient amount of data available to train any models. However, this is rarely the case for new software projects and for many companies. So far, only a few have studies focused on transferring prediction models from one project to another. In this paper, we study cross-project defect prediction models on a large scale. For 12 real-world applications, we ran 622 cross-project predictions. Our results indicate that cross-project prediction is a serious challenge, i.e., simply using models from projects in the same domain or with the same process does not lead to accurate predictions. To help software engineers choose models wisely, we identified factors that do influence the success of cross-project predictions. We also derived decision trees that can provide early estimates for precision, recall, and accuracy before a prediction is attempted.}},
    address = {New York, NY, USA},
    author = {Zimmermann, Thomas and Nagappan, Nachiappan and Gall, Harald and Giger, Emanuel and Murphy, Brendan},
    booktitle = {Proceedings of the the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
    citeulike-article-id = {6055293},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1595696.1595713},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1595696.1595713},
    doi = {10.1145/1595696.1595713},
    isbn = {978-1-60558-001-2},
    keywords = {lcm},
    location = {Amsterdam, The Netherlands},
    pages = {91--100},
    posted-at = {2011-01-19 21:08:27},
    priority = {2},
    publisher = {ACM},
    series = {ESEC/FSE '09},
    title = {{Cross-project Defect Prediction: A Large Scale Experiment on Data vs. Domain vs. Process}},
    url = {http://dx.doi.org/10.1145/1595696.1595713},
    year = {2009}
}

@inproceedings{Hassan2009Predicting,
    abstract = {{Predicting the incidence of faults in code has been commonly associated with measuring complexity. In this paper, we propose complexity metrics that are based on the code change process instead of on the code. We conjecture that a complex code change process negatively affects its product, i.e., the software system. We validate our hypothesis empirically through a case study using data derived from the change history for six large open source projects. Our case study shows that our change complexity metrics are better predictors of fault potential in comparison to other well-known historical predictors of faults, i.e., prior modifications and prior faults.}},
    address = {Washington, DC, USA},
    author = {Hassan, A. E.},
    booktitle = {Software Engineering, 2009. ICSE 2009. IEEE 31st International Conference on},
    citeulike-article-id = {6066157},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1555024},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/icse.2009.5070510},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5070510},
    doi = {10.1109/icse.2009.5070510},
    institution = {Software Anal. \& Intell. Lab. (SAIL), Queen''s Univ., Kingston, ON},
    isbn = {978-1-4244-3453-4},
    issn = {0270-5257},
    keywords = {chs},
    location = {Vancouver, BC, Canada},
    month = may,
    pages = {78--88},
    posted-at = {2011-01-19 21:04:06},
    priority = {4},
    publisher = {IEEE},
    series = {ICSE '09},
    title = {{Predicting faults using the complexity of code changes}},
    url = {http://dx.doi.org/10.1109/icse.2009.5070510},
    year = {2009}
}

@inproceedings{Pinzger2008Can,
    abstract = {{Software teams should follow a well defined goal and keep their work focused. Work fragmentation is bad for efficiency and quality. In this paper we empirically investigate the relationship between the fragmentation of developer contributions and the number of post-release failures. Our approach is to represent developer contributions with a developer-module network that we call contribution network. We use network centrality measures to measure the degree of fragmentation of developer contributions. Fragmentation is determined by the centrality of software modules in the contribution network. Our claim is that central software modules are more likely to be failure-prone than modules located in surrounding areas of the network. We analyze this hypothesis by exploring the network centrality of Microsoft Windows Vista binaries using several network centrality measures as well as linear and logistic regression analysis. In particular, we investigate which centrality measures are significant to predict the probability and number of post-release failures. Results of our experiments show that central modules are more failure-prone than modules located in surrounding areas of the network. Results further confirm that number of authors and number of commits are significant predictors for the probability of post-release failures. For predicting the number of post-release failures the closeness centrality measure is most significant.}},
    address = {New York, NY, USA},
    author = {Pinzger, Martin and Nagappan, Nachiappan and Murphy, Brendan},
    booktitle = {Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of software engineering},
    citeulike-article-id = {6662136},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1453101.1453105},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1453101.1453105},
    doi = {10.1145/1453101.1453105},
    isbn = {978-1-59593-995-1},
    keywords = {chs, chs-fse2012},
    location = {Atlanta, Georgia},
    pages = {2--12},
    posted-at = {2011-01-19 20:56:42},
    priority = {3},
    publisher = {ACM},
    series = {SIGSOFT '08/FSE-16},
    title = {{Can developer-module networks predict failures?}},
    url = {http://dx.doi.org/10.1145/1453101.1453105},
    year = {2008}
}

@article{Jorgensen1995Experience,
    abstract = {{The paper reports experience from the development and use of
eleven different software maintenance effort prediction models. The
models were developed applying regression analysis, neural networks and
pattern recognition and the prediction accuracy was measured and
compared for each model type. The most accurate predictions were
achieved applying models based on multiple regression and on pattern
recognition. We suggest the use of prediction models as instruments to
support the expert estimates and to analyse the impact of the
maintenance variables on the maintenance process and product. We believe
that the pattern recognition based models evaluated, i.e., the
prediction models based on the Optimized Set Reduction method, show
potential for such use}},
    author = {Jorgensen, M.},
    booktitle = {Software Engineering, IEEE Transactions on},
    citeulike-article-id = {2483658},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/32.403791},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=403791},
    comment = {not based on standard industry models but own developed models.  Applies to maintenance efforts on some projects.  Factors of interest: programmer knowledge of task, education, experience, type of task, task size, type of code changes, information sources, age and size of application.  Size includes deleted code.  It uses the actual size to estimate model accuracy.  Estimating size would seem like a key problem to solve.  

The included variable, together with the  categories  significantly predicting the productivity were: 
0  Cause: Corrective maintenance = 0, otherwise = 1 
0  Change: More  than  50\% of the  effort is believed to  be 
spent on updating of code compared to inserting and de- 
leting the code = 0, otherwise = l 
Mode:  More  than  50\%  of  the  effort  is  believed  to  be 
spent  on  development  of  new  modules  (New  module 
mode) = 0, otherwise (Embedded mode) = 1 
Confidence:  The maintainer believes he/she knows  how 
to solve the task when the task specification is readheard 
the  first  time  =  0  (High  confidence),  otherwise  =  1 
(Medium or low confidence). },
    doi = {10.1109/32.403791},
    issn = {00985589},
    journal = {IEEE Transactions on Software Engineering},
    keywords = {lcm},
    month = aug,
    number = {8},
    pages = {674--681},
    posted-at = {2011-01-07 19:02:06},
    priority = {0},
    title = {{Experience with the accuracy of software maintenance task effort prediction models}},
    url = {http://dx.doi.org/10.1109/32.403791},
    volume = {21},
    year = {1995}
}

@article{Boehm1984Software,
    abstract = {{This paper summarizes the current state of the art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation.}},
    author = {Boehm, B. W.},
    citeulike-article-id = {6845135},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/tse.1984.5010193},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5010193},
    doi = {10.1109/tse.1984.5010193},
    institution = {Software Information Systems Division, TRW Defense Systems Group, Redondo Beach, CA 90278.},
    issn = {0098-5589},
    journal = {Software Engineering, IEEE Transactions on},
    keywords = {lcm, metrics},
    month = jan,
    number = {1},
    pages = {4--21},
    posted-at = {2010-10-30 02:24:57},
    priority = {2},
    publisher = {IEEE},
    title = {{Software Engineering Economics}},
    url = {http://dx.doi.org/10.1109/tse.1984.5010193},
    volume = {SE-10},
    year = {1984}
}

@article{Benestad2010Understanding,
    abstract = {{Making changes to software systems can prove costly and it remains a challenge to understand the factors that affect the costs of software evolution. This study sought to identify such factors by investigating the effort expended by developers to perform 336 change tasks in two different software organizations. We quantitatively analyzed data from version control systems and change trackers to identify factors that correlated with change effort. In-depth interviews with the developers about a subset of the change tasks further refined the analysis. Two central quantitative results found that dispersion of changed code and volatility of the requirements for the change task correlated with change effort. The analysis of the qualitative interviews pointed to two important, underlying cost drivers: Difficulties in comprehending dispersed code and difficulties in anticipating side effects of changes. This study demonstrates a novel method for combining qualitative and quantitative analysis to assess cost drivers of software evolution. Given our findings, we propose improvements to practices and development tools to manage and reduce the costs.}},
    address = {Hingham, MA, USA},
    author = {Benestad, Hans C. and Anda, Bente and Arisholm, Erik},
    citeulike-article-id = {5944121},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1743652},
    citeulike-linkout-1 = {http://dx.doi.org/10.1007/s10664-009-9118-8},
    citeulike-linkout-2 = {http://www.springerlink.com/content/a1p46l93824831q5},
    day = {1},
    doi = {10.1007/s10664-009-9118-8},
    issn = {1382-3256},
    journal = {Empirical Softw. Engg.},
    keywords = {lcm},
    month = apr,
    number = {2},
    pages = {166--203},
    posted-at = {2010-10-30 02:09:30},
    priority = {2},
    publisher = {Kluwer Academic Publishers},
    title = {{Understanding cost drivers of software evolution: a quantitative and qualitative investigation of change effort in two evolving software systems}},
    url = {http://dx.doi.org/10.1007/s10664-009-9118-8},
    volume = {15},
    year = {2010}
}

@article{Boehm2000Safe,
    abstract = {{Simple software cost-analysis methods are readily available, but they aren't always safe. The simplest method is to base your cost estimate on the typical costs or productivity rates of your previous projects. That approach will work well if your new project doesn't have any cost-critical differences from those previous projects, but it won't be safe if some critical cost driver has degraded. Simple history-based software cost-analysis methods would be safer if you could identify which cost driver factors were likely to cause critical cost differences and estimate how much cost difference would result if a critical cost driver changed by a given degree. In this article, I provide a safe and simple method for doing both of these by using some cost-estimating relationships. COCOMO II is an updated and re-calibrated version of COCOMO (COnstructive COst MOdel). I also show how the COCOMO II cost drivers let you perform cost sensitivity and tradeoff analyses, and discuss how you can use similar methods with other software cost-estimation models}},
    author = {Boehm, B.},
    citeulike-article-id = {441281},
    citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=877854},
    comment = {(private-note)Evaluates the leverage of factors included in the COCOMO II model on the person year effort for a given project.  Template of how to use COCOMO II to assist in trade-off decisions!  

Which parameters are related to each other such that they would change simultaneously?},
    journal = {Software, IEEE},
    keywords = {lcm},
    number = {5},
    pages = {14--17},
    posted-at = {2010-10-30 02:07:05},
    priority = {0},
    title = {{Safe and simple software cost analysis}},
    url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=877854},
    volume = {17},
    year = {2000}
}

@article{Boehm2000Software,
    abstract = {{This paper summarizes several classes of software cost estimation models and techniques: parametric models, expertise\^{a}based techniques, learning\^{a}oriented techniques, dynamics\^{a}based models, regression\^{a}based models, and composite\^{a}Bayesian techniques for integrating expertise\^{a}based and regression\^{a}based models. Experience to date indicates that neural\^{a}net and dynamics\^{a}based techniques are less mature than the other classes of techniques, but that all classes of techniques are challenged by the rapid pace of change in software technology. The primary conclusion is that no single technique is best for all situations, and that a careful comparison of the results of several approaches is most likely to produce realistic estimates.}},
    author = {Boehm, Barry and Abts, Chris and Chulani, Sunita},
    booktitle = {Annals of Software Engineering},
    citeulike-article-id = {7694706},
    citeulike-linkout-0 = {http://dx.doi.org/10.1023/a:1018991717352},
    citeulike-linkout-1 = {http://www.springerlink.com/content/vv7w63752176772j},
    citeulike-linkout-2 = {http://link.springer.com/article/10.1023/A:1018991717352},
    day = {1},
    doi = {10.1023/a:1018991717352},
    issn = {10227091},
    journal = {Annals of Software Engineering},
    keywords = {lcm},
    month = nov,
    number = {1-4},
    pages = {177--205},
    posted-at = {2010-10-30 02:03:11},
    priority = {3},
    publisher = {Kluwer Academic Publishers},
    title = {{Software development cost estimation approaches - A survey}},
    url = {http://dx.doi.org/10.1023/a:1018991717352},
    volume = {10},
    year = {2000}
}

@article{Boehm1995Cost,
    abstract = {{Current software cost estimation models, such as the 1981 Constructive Cost Model (COCOMO) for software cost estimation and its 1987 Ada COCOMO update, have been experiencing increasing difficulties in estimating the costs of software developed to new life cycle processes and capabilities. These include non-sequential and rapid-development process models; reuse-driven approaches involving commercial off-the-shelf (COTS) packages, re-engineering, applications composition, and applications generation capabilities; object-oriented approaches supported by distributed middleware; and software process maturity initiatives. This paper summarizes research in deriving a baseline COCOMO 2.0 model tailored to these new forms of software development, including rationale for the model decisions. The major new modeling capabilities of COCOMO 2.0 are a tailorable family of software sizing models, involving Object Points, Function Points, and Source Lines of Code; nonlinear models for software reuse and re-engineering; an exponentdriver approach for modeling relative software diseconomies of scale; and several additions, deletions and updates to previous COCOMO effort-multiplier cost drivers. This model is serving as a framework for an extensive current data collection and analysis effort to further refine and calibrate the model's estimation capabilities.}},
    author = {Boehm, Barry and Clark, Bradford and Horowitz, Ellis and Westland, Chris and Madachy, Ray and Selby, Richard},
    citeulike-article-id = {1389142},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/bf02249046},
    citeulike-linkout-1 = {http://www.springerlink.com/content/y2386315010g7113},
    day = {16},
    doi = {10.1007/bf02249046},
    issn = {1022-7091},
    journal = {Annals of Software Engineering},
    keywords = {lcm},
    month = dec,
    number = {1},
    pages = {57--94},
    posted-at = {2010-10-30 01:59:36},
    priority = {4},
    publisher = {Springer Netherlands},
    title = {{Cost models for future software life cycle processes: COCOMO 2.0}},
    url = {http://dx.doi.org/10.1007/bf02249046},
    volume = {1},
    year = {1995}
}

@article{Iannino1984Criteria,
    abstract = {{A set of criteria is proposed for the comparison of software reliability models. The intention is to provide a logically organized basis for determining the superior models and for the presentation of model characteristics. It is hoped that in the future, a software manager will be able to more easily select the model most suitable for his/her requirements from among the preferred ones.}},
    author = {Iannino, Anthony and Musa, John D. and Okumoto, Kazuhira and Littlewood, Bev},
    citeulike-article-id = {7956162},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/tse.1984.5010297},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5010297},
    doi = {10.1109/tse.1984.5010297},
    issn = {0098-5589},
    journal = {IEEE Transactions on Software Engineering},
    keywords = {prediction},
    month = nov,
    number = {6},
    pages = {687--691},
    posted-at = {2010-10-07 19:43:41},
    priority = {0},
    title = {{Criteria for Software Reliability Model Comparisons}},
    url = {http://dx.doi.org/10.1109/tse.1984.5010297},
    volume = {SE-10},
    year = {1984}
}

@article{Levendel1990Reliability,
    abstract = {{The author analyzes and models the software development process,
and presents field experience for large distributed systems. Defect
removal is shown to be the bottleneck in achieving the appropriate
quality level before system deployment in the field. The time to defect
detection, the defect repair time and a factor reflecting the
introduction of new defects due to imperfect defect repair are some of
the constants in the laws governing defect removal. Test coverage is a
measure of defect removal effectiveness. A birth-death mathematical
model based on these constants is developed and used to model field
failure report data. The birth-death model is contrasted with a more
classical decreasing exponential model. Both models indicate that defect
removal is not a cost-effective way to achieve quality. As a result of
the long latency of software defects in a system, defect prevention is
suggested to be a far more practical solution to quality than defect
removal}},
    address = {Piscataway, NJ, USA},
    author = {Levendel, Y.},
    citeulike-article-id = {1550095},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=78698.78702},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/32.44378},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=44378},
    doi = {10.1109/32.44378},
    issn = {00985589},
    journal = {IEEE Transactions on Software Engineering},
    keywords = {prediction},
    month = feb,
    number = {2},
    pages = {141--152},
    posted-at = {2010-10-07 19:20:33},
    priority = {0},
    publisher = {IEEE Press},
    title = {{Reliability analysis of large software systems: defect data modeling}},
    url = {http://dx.doi.org/10.1109/32.44378},
    volume = {16},
    year = {1990}
}

@article{Hudepohl1996Emerald,
    abstract = {{As software becomes more and more sophisticated, industry has
begun to place a premium on software reliability. The telecommunications
industry is no exception. Consequently software reliability is a
strategic business weapon in an increasingly competitive marketplace. In
response to these concerns, BNR, Nortel, and Bell Canada developed the
Enhanced Measurement for Early Risk Assessment of Latent Defects
(Emerald), a decision support system designed to improve
telecommunications software reliability. Emerald efficiently integrates
software measurements, quality models, and delivery of results to the
desktop of software developers. We have found that Emerald not only
improves software reliability, but also facilitates the accurate
correction of field problems. Our experiences developing Emerald have
also taught us some valuable lessons about the implementation and
adoption of this type of software tool}},
    author = {Hudepohl, J. P. and Aud, S. J. and Khoshgoftaar, T. M. and Allen, E. B. and Mayrand, J.},
    citeulike-article-id = {7955491},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/52.536459},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=536459},
    doi = {10.1109/52.536459},
    issn = {07407459},
    journal = {IEEE Software},
    keywords = {chs, chs-fse2012},
    month = sep,
    number = {5},
    pages = {56--60},
    posted-at = {2010-10-07 15:44:55},
    priority = {0},
    title = {{Emerald: software metrics and models on the desktop}},
    url = {http://dx.doi.org/10.1109/52.536459},
    volume = {13},
    year = {1996}
}

@inproceedings{Meneely2008Predicting,
    abstract = {{Software fails and fixing it is expensive. Research in failure prediction has been highly successful at modeling software failures. Few models, however, consider the key cause of failures in software: people. Understanding the structure of developer collaboration could explain a lot about the reliability of the final product. We examine this collaboration structure with the developer network derived from code churn information that can predict failures at the file level. We conducted a case study involving a mature Nortel networking product of over three million lines of code. Failure prediction models were developed using test and post-release failure data from two releases, then validated against a subsequent release. One model's prioritization revealed 58\% of the failures in 20\% of the files compared with the optimal prioritization that would have found 61\% in 20\% of the files, indicating that a significant correlation exists between file-based developer network metrics and failures.}},
    address = {New York, NY, USA},
    author = {Meneely, Andrew and Williams, Laurie and Snipes, Will and Osborne, Jason},
    booktitle = {Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of software engineering},
    citeulike-article-id = {6713927},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1453106},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1453101.1453106},
    doi = {10.1145/1453101.1453106},
    isbn = {978-1-59593-995-1},
    keywords = {chs, chs-fse2012},
    location = {Atlanta, Georgia},
    pages = {13--23},
    posted-at = {2010-10-06 13:55:47},
    priority = {0},
    publisher = {ACM},
    series = {SIGSOFT '08/FSE-16},
    title = {{Predicting failures with developer networks and social network analysis}},
    url = {http://dx.doi.org/10.1145/1453101.1453106},
    year = {2008}
}

@article{Yamada1983SSHAPED,
    author = {Yamada, Shigeru; O.},
    citeulike-article-id = {7908872},
    journal = {IEEE Transactions on Reliability},
    keywords = {prediction},
    month = dec,
    number = {5},
    pages = {475--478},
    posted-at = {2010-09-27 15:35:41},
    priority = {0},
    title = {S-SHAPED RELIABILITY GROWTH MODELING FOR SOFTWARE ERROR DETECTION.},
    volume = {R-32},
    year = {1983}
}

@book{RefWorks:118,
    author = {Hill, Thomas and Lewicki, Paul},
    citeulike-article-id = {7398614},
    citeulike-linkout-0 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&\#38;path=ASIN/1884233597},
    keywords = {prediction},
    posted-at = {2010-09-22 13:40:46},
    priority = {0},
    publisher = {StatSoft, Inc},
    title = {{Statistics: Methods and Applications}},
    url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/1884233597},
    year = {2007}
}

@inproceedings{Hudepohl1996Integrating,
    abstract = {{Enhanced Measurement for Early Risk Assessment of Latent Defects (EMERALD) is a decision support system for assessing reliability risk. It is used by software developers and managers to improve telecommunications software service quality as perceived by the customer and the end user. Risk models are based on static characteristics of source code. This paper shows how a system such as EMERALD can enhance software development, testing, and maintenance by integration of: a software quality improvement strategy; measurements and models; and delivery of results to the desktop of developers in a timely manner. This paper also summarizes empirical experiments with EMERALD's models using data from large industrial telecommunications software systems. EMERALD has been applied to a very large system with over 12 million lines of source code within procedures. Experience and lessons learned are also discussed.}},
    address = {Los Alamitos, CA, USA},
    author = {Hudepohl, J. P. and Aud, S. J. and Khoshgoftaar, T. M. and Allen, E. B. and Mayrand, J.},
    citeulike-article-id = {7820930},
    citeulike-linkout-0 = {http://doi.ieeecomputersociety.org/10.1109/ISSRE.1996.558707},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/issre.1996.558707},
    doi = {10.1109/issre.1996.558707},
    issn = {1071-9458},
    journal = {The Seventh International Symposium on Software Reliability Engineering (ISSRE '96)},
    keywords = {chs},
    pages = {93+},
    posted-at = {2010-09-14 03:16:45},
    priority = {0},
    publisher = {IEEE Computer Society},
    title = {{Integrating metrics and models for software risk assessment}},
    url = {http://dx.doi.org/10.1109/issre.1996.558707},
    volume = {0},
    year = {1996}
}

@article{Hudepohl1996EMERALD,
    abstract = {{BNR, NORTEL, and Bell Canada have jointly developed the Enhanced Measurement for Early Risk Assessment of Latent Defects (EMERALD) system for decision support, integrating software measurements, quality models, and delivery of results to the desktop of software developers in a timely manner (J.P. Hudepohl et al., 1996). It has been applied to a very large system with more than 12 million lines of code within procedures. The prerequisite infrastructure for reliability improvement includes a strong configuration management system, a flexible source code library system, a thorough problem reporting system, and widespread networking of the desktop systems of developers. Such systems are not part of EMERALD per se, but EMERALD would have been impossible without them.}},
    address = {Los Alamitos, CA, USA},
    author = {Hudepohl, J. P. and Aud, S. J. and Khoshgoftaar, T. M. and Allen, E. B. and Mayrand, J.},
    citeulike-article-id = {7820921},
    citeulike-linkout-0 = {http://doi.ieeecomputersociety.org/10.1109/AST.1996.506489},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/ast.1996.506489},
    doi = {10.1109/ast.1996.506489},
    isbn = {0-8186-7389-3},
    journal = {Proceedings of the Fourth International Symposium on Assessment of Software Tools (SAST '96)},
    pages = {111+},
    posted-at = {2010-09-14 03:14:42},
    priority = {0},
    publisher = {IEEE Computer Society},
    title = {{EMERALD: software metrics and models on the desktop}},
    url = {http://dx.doi.org/10.1109/ast.1996.506489},
    volume = {0},
    year = {1996}
}

@article{Khoshgoftaar1999Which,
    address = {New York, NY, USA},
    author = {Khoshgoftaar, Taghi M. and Allen, Edward B. and Jones, Wendell D. and Hudepohl, John P.},
    citeulike-article-id = {1468331},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=309578.309586},
    citeulike-linkout-1 = {http://dx.doi.org/10.1002/(sici)1096-908x(199901},
    doi = {10.1002/(sici)1096-908x(199901},
    journal = {Journal of Software Maintenance},
    keywords = {chs},
    number = {1},
    pages = {1--18},
    posted-at = {2010-09-14 03:11:59},
    priority = {0},
    publisher = {John Wiley \& Sons, Inc.},
    title = {{Which software modules have faults which will be discovered by customers?}},
    url = {http://dx.doi.org/10.1002/(sici)1096-908x(199901},
    volume = {11},
    year = {1999}
}

@inproceedings{Khomh2009Exploratory,
    abstract = {{Code smells are poor implementation choices, thought to make object-oriented systems hard to maintain. In this study, we investigate if classes with code smells are more change-prone than classes without smells. Specifically, we test the general hypothesis: classes with code smells are not more change prone than other classes. We detect 29 code smells in 9 releases of Azureus and in 13 releases of Eclipse, and study the relation between classes with these code smells and class change-proneness. We show that, in almost all releases of Azureus and Eclipse, classes with code smells are more change-prone than others, and that specific smells are more correlated than others to change-proneness. These results justify a posteriori previous work on the specification and detection ofcode smells and could help focusing quality assurance and testing activities.}},
    address = {Washington, DC, USA},
    author = {Khomh, Foutse and Di Penta, Massimiliano and Gueheneuc, Yann G.},
    booktitle = {Proceedings of the 2009 16th Working Conference on Reverse Engineering},
    citeulike-article-id = {7160838},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1686210},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/wcre.2009.28},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5328703},
    doi = {10.1109/wcre.2009.28},
    institution = {Dept. de Genie Inf. et Logiciel, Ecole Polytech. de Montreal, Montreal, QC, Canada},
    isbn = {978-0-7695-3867-9},
    issn = {1095-1350},
    keywords = {td},
    location = {Lille, France},
    month = oct,
    pages = {75--84},
    posted-at = {2010-09-08 22:15:51},
    priority = {4},
    publisher = {IEEE Computer Society},
    series = {WCRE '09},
    title = {{An Exploratory Study of the Impact of Code Smells on Software Change-proneness}},
    url = {http://dx.doi.org/10.1109/wcre.2009.28},
    year = {2009}
}

@article{Olbrich2009Evolution,
    abstract = {{Code smells are design flaws in object-oriented designs that may lead to maintainability issues in the further evolution of the software system. This study focuses on the evolution of code smells within a system and their impact on the change behavior (change frequency and size). The study investigates two code smells, God Class and Shotgun Surgery, by analyzing the historical data over several years of development of two large scale open source systems. The detection of code smells in the evolution of those systems was performed by the application of an automated approach using detection strategies. The results show that we can identify different phases in the evolution of code smells during the system development and that code smell infected components exhibit a different change behavior. This information is useful for the identification of risk areas within a software system that need refactoring to assure a future positive evolution.}},
    address = {Los Alamitos, CA, USA},
    author = {Olbrich, Steffen and Cruzes, Daniela S. and Basili, Victor and Zazworka, Nico},
    booktitle = {ESEM '09: Proceedings of the 2009 3rd International Symposium on Empirical Software Engineering and Measurement},
    citeulike-article-id = {7160848},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1671285},
    citeulike-linkout-1 = {http://doi.ieeecomputersociety.org/10.1109/ESEM.2009.5314231},
    citeulike-linkout-2 = {http://dx.doi.org/10.1109/esem.2009.5314231},
    doi = {10.1109/esem.2009.5314231},
    isbn = {978-1-4244-4842-5},
    journal = {Empirical Software Engineering and Measurement, International Symposium on},
    keywords = {chs},
    pages = {390--400},
    posted-at = {2010-09-08 22:11:19},
    priority = {2},
    publisher = {IEEE Computer Society},
    title = {{The evolution and impact of code smells: A case study of two open source systems}},
    url = {http://dx.doi.org/10.1109/esem.2009.5314231},
    volume = {0},
    year = {2009}
}

@article{Mantyla2006Subjective,
    abstract = {{This paper presents the results of an empirical study on the subjective evaluation of code smells that identify poorly evolvable structures in software. We propose use of the term software evolvability to describe the ease of further developing a piece of software and outline the research area based on four different viewpoints. Furthermore, we describe the differences between human evaluations and automatic program analysis based on software evolvability metrics. The empirical component is based on a case study in a Finnish software product company, in which we studied two topics. First, we looked at the effect of the evaluator when subjectively evaluating the existence of smells in code modules. We found that the use of smells for code evaluation purposes can be difficult due to conflicting perceptions of different evaluators. However, the demographics of the evaluators partly explain the variation. Second, we applied selected source code metrics for identifying four smells and compared these results to the subjective evaluations. The metrics based on automatic program analysis and the human-based smell evaluations did not fully correlate. Based upon our results, we suggest that organizations should make decisions regarding software evolvability improvement based on a combination of subjective evaluations and code metrics. Due to the limitations of the study we also recognize the need for conducting more refined studies and experiments in the area of software evolvability.}},
    author = {M\"{a}ntyl\"{a}, MikaV and Lassenius, Casper},
    booktitle = {Empirical Software Engineering},
    citeulike-article-id = {6335663},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/s10664-006-9002-8},
    citeulike-linkout-1 = {http://www.springerlink.com/content/j3l44j1532652378},
    citeulike-linkout-2 = {http://link.springer.com/article/10.1007/s10664-006-9002-8},
    day = {1},
    doi = {10.1007/s10664-006-9002-8},
    issn = {1382-3256},
    journal = {Empirical Software Engineering},
    keywords = {chs},
    month = sep,
    number = {3},
    pages = {395--431},
    posted-at = {2010-09-08 22:04:13},
    priority = {2},
    publisher = {Springer US},
    title = {{Subjective evaluation of software evolvability using code smells: An empirical study}},
    url = {http://dx.doi.org/10.1007/s10664-006-9002-8},
    volume = {11},
    year = {2006}
}

@article{Alon2009How,
    abstract = {{Choosing good problems is essential for being a good scientist. But what is a good problem, and how do you choose one? The subject is not usually discussed explicitly within our profession. Scientists are expected to be smart enough to figure it out on their own and through the observation of their teachers. This lack of explicit discussion leaves a vacuum that can lead to approaches such as choosing problems that can give results that merit publication in valued journals, resulting in a job and tenure.}},
    author = {Alon, Uri},
    citeulike-article-id = {5842862},
    citeulike-linkout-0 = {http://www.cell.com/molecular-cell/abstract/S1097-2765(09)00641-8},
    citeulike-linkout-1 = {http://dx.doi.org/10.1016/j.molcel.2009.09.013},
    citeulike-linkout-2 = {http://www.f1000.com/1165779},
    citeulike-linkout-3 = {http://view.ncbi.nlm.nih.gov/pubmed/19782018},
    citeulike-linkout-4 = {http://www.hubmed.org/display.cgi?uids=19782018},
    day = {24},
    doi = {10.1016/j.molcel.2009.09.013},
    issn = {10972765},
    journal = {Mol Cell},
    keywords = {scientist},
    month = sep,
    number = {6},
    pages = {726--728},
    pmid = {19782018},
    posted-at = {2010-09-08 21:54:01},
    priority = {0},
    publisher = {Cell Press},
    title = {{How To Choose a Good Scientific Problem}},
    url = {http://dx.doi.org/10.1016/j.molcel.2009.09.013},
    volume = {35},
    year = {2009}
}

@inproceedings{Fritz2007Does,
    abstract = {{The practice of software development can likely be improved if an externalized model of each programmer's knowledge of a particular code base is available. Some tools already assume a useful form of such a model can be created from data collected during development, such as expertise recommenders that use information about who has changed each file to suggest who might answer questions about particular parts of a system. In this paper, we report on an empirical study that investigates whether a programmer's activity can be used to build a model of what a programmer knows about a code base. In this study, nineteen professional Java programmers completed a series of questionnaires about the code on which they were working. These questionnaires were generated automatically and asked about program elements a programmer had worked with frequently and recently and ones that he had not. We found that a degree of interest model based on this frequency and recency of interaction can often indicate the parts of the code base for which the programmer has knowledge. We also determined a number of factors that may be used to improve the model, such as authorship of program elements, the role of elements, and the task being performed.}},
    address = {New York, NY, USA},
    author = {Fritz, Thomas and Murphy, Gail C. and Hill, Emily},
    booktitle = {Proceedings of the the 6th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on The foundations of software engineering},
    citeulike-article-id = {2823097},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1287673},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1287624.1287673},
    comment = {(private-note)Defines the Degree of Interest metric.},
    doi = {10.1145/1287624.1287673},
    isbn = {978-1-59593-811-4},
    keywords = {chs},
    location = {Dubrovnik, Croatia},
    pages = {341--350},
    posted-at = {2010-09-08 21:24:36},
    priority = {3},
    publisher = {ACM},
    series = {ESEC-FSE '07},
    title = {{Does a programmer's activity indicate knowledge of code?}},
    url = {http://dx.doi.org/10.1145/1287624.1287673},
    year = {2007}
}

@inproceedings{Anvik2007Determining,
    abstract = {{As developers work on a software product they accumulate expertise, including expertise about the code base of the software product. We call this type of expertise 'implementation expertise'. Knowing the set of developers who have implementation expertise for a software product has many important uses. This paper presents an empirical evaluation of two approaches to determining implementation expertise from the data in source and bug repositories. The expertise sets created by the approaches are compared to those provided by experts and evaluated using the measures of precision and recall. We found that both approaches are good at finding all of the appropriate developers, although they vary in how many false positives are returned.}},
    address = {Washington, DC, USA},
    author = {Anvik, John and Murphy, Gail C.},
    booktitle = {Fourth International Workshop on Mining Software Repositories (MSR'07:ICSE Workshops 2007)},
    citeulike-article-id = {3064726},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1268983.1269018},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/msr.2007.7},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4228639},
    doi = {10.1109/msr.2007.7},
    isbn = {0-7695-2950-X},
    journal = {Mining Software Repositories, 2007. ICSE Workshops MSR '07. Fourth International Workshop on},
    keywords = {chs},
    location = {Minneapolis, MN, USA},
    month = may,
    pages = {2},
    posted-at = {2010-09-08 21:22:30},
    priority = {2},
    publisher = {IEEE},
    title = {{Determining Implementation Expertise from Bug Reports}},
    url = {http://dx.doi.org/10.1109/msr.2007.7},
    year = {2007}
}

@inproceedings{Zimmermann2006Finegrained,
    abstract = {{In this paper, we present the APFEL plug-in that collects fine-grained changes from version archives in a database. APFEL is built upon the Eclipse infrastructure for CVS and Java. In order to describe changes, APFEL uses tokens such as method calls, exceptions, and variable usages. We demonstrate the usefulness of APFEL's database with several case studies.}},
    address = {New York, NY, USA},
    author = {Zimmermann, Thomas},
    booktitle = {Proceedings of the 2006 OOPSLA workshop on eclipse technology eXchange},
    citeulike-article-id = {7801478},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1188839},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1188835.1188839},
    doi = {10.1145/1188835.1188839},
    isbn = {1-59593-621-1},
    keywords = {chs, chs-fse2012},
    location = {Portland, Oregon},
    pages = {16--20},
    posted-at = {2010-09-08 21:18:16},
    priority = {2},
    publisher = {ACM},
    series = {eclipse '06},
    title = {{Fine-grained processing of CVS archives with APFEL}},
    url = {http://dx.doi.org/10.1145/1188835.1188839},
    year = {2006}
}

@inproceedings{Schuler2008Mining,
    abstract = {{In software development, there is an increasing need to find and connect developers with relevant expertise. Existing expertise recommendation systems are mostly based on variations of the Line 10 Rule: developers who changed a file most often have the most implementation expertise. In this paper, we introduce the concept of usage expertise, which manifests itself whenever developers are using functionality, e.g., by calling API methods. We present preliminary results for the ECLIPSE project that demonstrate that our technique allows to recommend experts for files with no or little history, identify developers with similar expertise, and measure the usage of API methods.}},
    address = {New York, NY, USA},
    author = {Schuler, David and Zimmermann, Thomas},
    booktitle = {Proceedings of the 2008 international working conference on Mining software repositories},
    citeulike-article-id = {6662019},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1370779},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1370750.1370779},
    comment = {(private-note)Related to "A Degree of Knowledge Model to Capture Source Code Familiarity"},
    doi = {10.1145/1370750.1370779},
    isbn = {978-1-60558-024-1},
    keywords = {chs},
    location = {Leipzig, Germany},
    pages = {121--124},
    posted-at = {2010-09-08 21:14:50},
    priority = {2},
    publisher = {ACM},
    series = {MSR '08},
    title = {{Mining usage expertise from version archives}},
    url = {http://dx.doi.org/10.1145/1370750.1370779},
    year = {2008}
}

@inproceedings{Fritz2010Degreeofknowledge,
    abstract = {{The size and high rate of change of source code comprising a software system make it difficult for software developers to keep up with who on the team knows about particular parts of the code. Existing approaches to this problem are based solely on authorship of code. In this paper, we present data from two professional software development teams to show that both authorship and interaction information about how a developer interacts with the code are important in characterizing a developer's knowledge of code. We introduce the degree-of-knowledge model that computes automatically a real value for each source code element based on both authorship and interaction information. We show that the degree-of-knowledge model can provide better results than an existing expertise finding approach and also report on case studies of the use of the model to support knowledge transfer and to identify changes of interest.}},
    address = {New York, NY, USA},
    author = {Fritz, Thomas and Ou, Jingwen and Murphy, Gail C. and Murphy-Hill, Emerson},
    booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1},
    citeulike-article-id = {7262111},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1806799.1806856},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1806799.1806856},
    comment = {(private-note)Describes a degree of knowledge model that expresses a developer's experience with a code element.  The DOK model includes both authorship information and interaction information.  Interaction information is gleaned by monitoring the developer's activity in the development environment (IDE).  A Degree of Interest records events such as a developer opening a file to edit and the developer actually making an edit in the file.  See the Eclipse Mylyn project. Being the first author of a code element was the most significant impact to the DOK value as rated by the developers themselves.  However, other factors combined with authorship were better match for expertise than authorship alone.  Previous approaches used only authorship (Expertise Recommender and Expertise Browser).  DOK was found to compare favorably with developer's personal opinion of who are experts in a particular code element.  It was useful for identifying recent changes due to defects that would be of interest to particular developers.  It was less useful for on-boarding because the ratings were mostly of lower level code elements rather than high level APIs which new people would need to become familiar with quickly.
---=note-separator=---
(private-note)Idea: Use the DOK information to determine who to inspect.  Give the present author a list of experts to invite to an inspection
Idea: Rate a dev's knowledge based on how much the file/element changed since they last viewed/edited it.
Idea: Notify dev's of overlapping edits that change code they once wrote},
    doi = {10.1145/1806799.1806856},
    isbn = {978-1-60558-719-6},
    keywords = {chs, chs-fse2012, *ideas},
    location = {Cape Town, South Africa},
    pages = {385--394},
    posted-at = {2010-09-08 17:12:44},
    priority = {0},
    publisher = {ACM},
    series = {ICSE '10},
    title = {{A degree-of-knowledge model to capture source code familiarity}},
    url = {http://dx.doi.org/10.1145/1806799.1806856},
    year = {2010}
}

@inproceedings{Buse2008Metric,
    abstract = {{In this paper, we explore the concept of code readability and investigate its relation to software quality. With data collected from human annotators, we derive associations between a simple set of local code features and human notions of readability. Using those features, we construct an automated readability measure and show that it can be 80\% effective, and better than a human on average, at predicting readability judgments. Furthermore, we show that this metric correlates strongly with two traditional measures of software quality, code changes and defect reports. Finally, we discuss the implications of this study on programming language design and engineering practice. For example, our data suggests that comments, in of themselves, are less important than simple blank lines to local judgments of readability.}},
    address = {New York, NY, USA},
    author = {Buse, Raymond P. L. and Weimer, Westley R.},
    booktitle = {Proceedings of the 2008 International Symposium on Software Testing and Analysis},
    citeulike-article-id = {7796237},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1390630.1390647},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1390630.1390647},
    comment = {Defines a series of measures that evaluate readability of programs compared with human judgments of readability.  Readability was correlated with static defects and change frequency.},
    doi = {10.1145/1390630.1390647},
    isbn = {978-1-60558-050-0},
    keywords = {chs},
    location = {Seattle, WA, USA},
    pages = {121--130},
    posted-at = {2010-09-07 22:50:14},
    priority = {0},
    publisher = {ACM},
    series = {ISSTA '08},
    title = {{A Metric for Software Readability}},
    url = {http://dx.doi.org/10.1145/1390630.1390647},
    year = {2008}
}

@article{Yu2010Component,
    abstract = {{Software component interaction is essential for realising proper software system functions. Such interactions between software components induce interdependencies between multiple components. One effect of such a dependency is co-evolution, wherein changes made to one component also requires corresponding changes to other component(s). This study presents a mathematical framework for representing component co-evolution. Two types of co-evolution, internal co-evolution and external co-evolution are defined for an evolving software component. The component dependency metrics that are related with component co-evolutions are analysed and the correlations between component dependency and component co-evolution are hypothesised. Further, in a quasi-experiment of nine open-source Java projects, component dependencies are measured and component revision histories are mined to verify the speculated correlations.}},
    author = {Yu, L. and Mishra, A. and Ramaswamy, S.},
    citeulike-article-id = {7796231},
    citeulike-linkout-0 = {http://dx.doi.org/10.1049/iet-sen.2008.0084},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5523693},
    comment = {Provides a detailed definition of a software component this is useful for background information.  Defines a component as a collection of software files and modules.  Describes analysis of how components that change together establish evidence of a dependency whether or not there is a observable dependency from static analysis.},
    doi = {10.1049/iet-sen.2008.0084},
    issn = {17518806},
    journal = {IET Software},
    keywords = {chs, component, reliability},
    number = {4},
    pages = {252+},
    posted-at = {2010-09-07 22:46:02},
    priority = {0},
    title = {{Component co-evolution and component dependency: speculations and verifications}},
    url = {http://dx.doi.org/10.1049/iet-sen.2008.0084},
    volume = {4},
    year = {2010}
}

@article{Pai2007Empirical,
    abstract = {{We present a methodology for Bayesian analysis of software quality. We cast our research in the broader context of constructing a causal framework that can include process, product, and other diverse sources of information regarding fault introduction during the software development process. In this paper, we discuss the aspect of relating internal product metrics to external quality metrics. Specifically, we build a Bayesian network (BN) model to relate object-oriented software metrics to software fault content and fault proneness. Assuming that the relationship can be described as a generalized linear model, we derive parametric functional forms for the target node conditional distributions in the BN. These functional forms are shown to be able to represent linear, Poisson, and binomial logistic regression. The models are empirically evaluated using a public domain data set from a software subsystem. The results show that our approach produces statistically significant estimations and that our overall modeling method performs no worse than existing techniques.}},
    address = {Los Alamitos, CA, USA},
    author = {Pai and Bechta Dugan, Joanne},
    citeulike-article-id = {7418290},
    citeulike-linkout-0 = {http://doi.ieeecomputersociety.org/10.1109/TSE.2007.70722},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/tse.2007.70722},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4302779},
    comment = {(private-note)Defined a method to apply pure Bayesian analysis to prediction of fault-prone modules and to predict fault content in modules.  Used the C-K Suite of metrics.  Mostly a paper on how to apply Bayesian methods to this domain.  All relationships have been found before.},
    doi = {10.1109/tse.2007.70722},
    issn = {0098-5589},
    journal = {IEEE Transactions on Software Engineering},
    keywords = {chs},
    month = oct,
    number = {10},
    pages = {675--686},
    posted-at = {2010-09-07 22:38:40},
    priority = {0},
    publisher = {IEEE Computer Society},
    title = {{Empirical Analysis of Software Fault Content and Fault Proneness Using Bayesian Methods}},
    url = {http://dx.doi.org/10.1109/tse.2007.70722},
    volume = {33},
    year = {2007}
}

@article{Kim2008Classifying,
    abstract = {{This paper introduces a new technique for finding latent software bugs called change classification. Change classification uses a machine learning classifier to determine whether a new software change is more similar to prior buggy changes, or clean changes. In this manner, change classification predicts the existence of bugs in software changes. The classifier is trained using features (in the machine learning sense) extracted from the revision history of a software project, as stored in its software configuration management repository. The trained classifier can classify changes as buggy or clean with 78\&\#x025; accuracy and 65\&\#x025; buggy change recall (on average). Change classification has several desirable qualities: (1) the prediction granularity is small (a change to a single file), (2) predictions do not require semantic information about the source code, (3) the technique works for a broad array of project types and programming languages, and (4) predictions can be made immediately upon completion of a change. Contributions of the paper include a description of the change classification approach, techniques for extracting features from source code and change histories, a characterization of the performance of change classification across 12 open source projects, and evaluation of the predictive power of different groups of features.}},
    author = {Kim, Sunghun and Whitehead, Jr and Zhang, Yi},
    booktitle = {Software Engineering, IEEE Transactions on},
    citeulike-article-id = {2719220},
    citeulike-linkout-0 = {http://doi.ieeecomputersociety.org/10.1109/TSE.2007.70773},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/tse.2007.70773},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4408585},
    comment = {(private-note)This paper performs a machine learning analysis of code deltas and change history.  It uses the code itself in delta form , complexity, change records, and defect classification.  They use Baysean methods to separate prior information like complexity from the change itself.  S77 is used to determine the bug injection point.  Results vary, it is not repeatable across OS applications.  They have not investigated why it works in some situations and not others.  },
    doi = {10.1109/tse.2007.70773},
    journal = {Software Engineering, IEEE Transactions on},
    keywords = {chs, chs-fse2012},
    number = {2},
    pages = {181--196},
    posted-at = {2010-09-07 22:29:40},
    priority = {0},
    title = {{Classifying Software Changes: Clean or Buggy?}},
    url = {http://dx.doi.org/10.1109/tse.2007.70773},
    volume = {34},
    year = {2008}
}

@article{Ying2004Predicting,
    abstract = {{Software developers are often faced with modification tasks that involve source which is spread across a code base. Some dependencies between source code, such as those between source code written in different languages, are difficult to determine using existing static and dynamic analyses. To augment existing analyses and to help developers identify relevant source code during a modification task, we have developed an approach that applies data mining techniques to determine change patterns - sets of files that were changed together frequently in the past - from the change history of the code base. Our hypothesis is that the change patterns can be used to recommend potentially relevant source code to a developer performing a modification task. We show that this approach can reveal valuable dependencies by applying the approach to the Eclipse and Mozilla open source projects and by evaluating the predictability and interestingness of the recommendations produced for actual modification tasks on these systems.}},
    address = {Los Alamitos, CA, USA},
    author = {Ying, A. T. T. and Murphy, G. C. and Ng, R. and Chu-Carroll, M. C.},
    booktitle = {Software Engineering, IEEE Transactions on},
    citeulike-article-id = {983796},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1018388},
    citeulike-linkout-1 = {http://doi.ieeecomputersociety.org/10.1109/TSE.2004.52},
    citeulike-linkout-2 = {http://dx.doi.org/10.1109/tse.2004.52},
    citeulike-linkout-3 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1324645},
    doi = {10.1109/tse.2004.52},
    issn = {0098-5589},
    journal = {Software Engineering, IEEE Transactions on},
    keywords = {chs, chs-fse2012, impact\_analysis},
    month = sep,
    number = {9},
    pages = {574--586},
    posted-at = {2010-09-07 22:07:47},
    priority = {3},
    publisher = {IEEE},
    title = {{Predicting source code changes by mining change history}},
    url = {http://dx.doi.org/10.1109/tse.2004.52},
    volume = {30},
    year = {2004}
}

@article{Mockus2000Predicting,
    abstract = {{Abstract 10.1002/bltj.2229.abs Reducing the number of software failures is one of the most challenging problems of software production. We assume that software development proceeds as a series of changes and model the probability that a change to software will cause a failure. We use predictors based on the properties of a change itself. Such predictors include size in lines of code added, deleted, and unmodified; diffusion of the change and its component subchanges, as reflected in the number of files, modules, and subsystems touched, or changed; several measures of developer experience; and the type of change and its subchanges (fault fixes or new code). The model is built on historic information and is used to predict the risk of new changes. In this paper we apply the model to 5ESS® software updates and find that change diffusion and developer experience are essential to predicting failures. The predictive model is implemented as a Web-based tool to allow timely prediction of change quality. The ability to predict the quality of change enables us to make appropriate decisions regarding inspection, testing, and delivery. Historic information on software changes is recorded in many commercial software projects, suggesting that our results can be easily and widely applied in practice.}},
    address = {Software Production Research Department, Bell Labs, Naperville, Illinois},
    author = {Mockus, A. and Weiss, D. M.},
    citeulike-article-id = {2719241},
    citeulike-linkout-0 = {http://dx.doi.org/10.1002/bltj.2229},
    citeulike-linkout-1 = {http://www3.interscience.wiley.com/cgi-bin/abstract/97518991/ABSTRACT},
    doi = {10.1002/bltj.2229},
    issn = {1538-7305},
    journal = {Bell Labs Tech. J.},
    keywords = {chs, chs-fse2012},
    number = {2},
    pages = {169--180},
    posted-at = {2010-09-07 22:05:39},
    priority = {3},
    publisher = {Wiley Subscription Services, Inc., A Wiley Company},
    title = {{Predicting risk of software changes}},
    url = {http://dx.doi.org/10.1002/bltj.2229},
    volume = {5},
    year = {2000}
}

@article{Zimmermann2005Mining,
    abstract = {{We apply data mining to version histories in order to guide programmers along related changes: "Programmers who changed these functions also changed...." Given a set of existing changes, the mined association rules 1) suggest and predict likely further changes, 2) show up item coupling that is undetectable by program analysis, and 3) can prevent errors due to incomplete changes. After an initial change, our ROSE prototype can correctly predict further locations to be changed; the best predictive power is obtained for changes to existing software. In our evaluation based on the history of eight popular open source projects, ROSE's topmost three suggestions contained a correct location with a likelihood of more than 70 percent.}},
    address = {Los Alamitos, CA, USA},
    author = {Zimmermann, T. and Zeller, A. and Weissgerber, P. and Diehl, S.},
    booktitle = {Software Engineering, IEEE Transactions on},
    citeulike-article-id = {277045},
    citeulike-linkout-0 = {http://doi.ieeecomputersociety.org/10.1109/TSE.2005.72},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/tse.2005.72},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1463228},
    comment = {This paper describes a methodology and tool (ROSE) for mining version histories to identify which entities may need to change simultaneously with a specific source change.  An entity is an element of the file identified by the file, the class, method, and identifier of the changed text.  The entities may not be connected programmatically or may not even be programs.  The entities are identified as common change partners with the subject change by analyzing which entities in the CM system have been changed together.  The ROSE tool applied to eclipse had 26\% precision at 15\% recall for correctly identifying the entities that change together given a specific change.  The idea was to present a to 10 list of suggested entities that should change with a given change. Therefore between 2 and 3 of the suggestions would be correct out of the 10.},
    day = {11},
    doi = {10.1109/tse.2005.72},
    institution = {Dept. of Comput. Sci., Saarlandes Univ., Saarbrucken, Germany},
    issn = {0098-5589},
    journal = {Software Engineering, IEEE Transactions on},
    keywords = {chs, chs-fse2012, impact\_analysis},
    month = jun,
    number = {6},
    pages = {429--445},
    posted-at = {2010-09-07 22:02:34},
    priority = {0},
    publisher = {IEEE},
    title = {{Mining version histories to guide software changes}},
    url = {http://dx.doi.org/10.1109/tse.2005.72},
    volume = {31},
    year = {2005}
}

@inproceedings{kim_analysis_2005,
    abstract = {Software continually changes due to performance improvements, new requirements, bug fixes, and adaptation to a changing operational environment. Common changes include modifications to data definitions, control flow, method/function signatures, and class/file relationships. Signature changes are notable because they require changes at all sites calling the modified function, and hence as a class they have more impact than other change {kinds.We} performed signature change analysis over software project histories to reveal multiple properties of signature changes, including their kind, frequency, and evolution patterns. These signature properties can be used to alleviate the impact of signature changes. In this paper we introduce a taxonomy of signature change kinds to categorize observed changes. We report multiple properties of signature changes based on an analysis of eight prominent open source projects including the Apache {HTTP} server, {GCC,} and Linux 2.5 kernel.},
    address = {St. Louis, Missouri},
    author = {Kim, Sunghun and Whitehead, E. James and Bevan, Jennifer},
    booktitle = {Proceedings of the 2005 international workshop on Mining software repositories},
    citeulike-article-id = {7796198},
    citeulike-linkout-0 = {http://portal.acm.org.www.lib.ncsu.edu:2048/citation.cfm?id=1083154},
    pages = {1--5},
    posted-at = {2010-09-07 21:55:46},
    priority = {3},
    publisher = {{ACM}},
    title = {{Analysis of signature change patterns}},
    url = {http://portal.acm.org.www.lib.ncsu.edu:2048/citation.cfm?id=1083154},
    year = {2005}
}

@proceedings{30,
    abstract = {{We analyze the version history of 7 software systems to predict the most fault prone entities and files. The basic assumption is that faults do not occur in isolation, but rather in bursts of several related faults. Therefore, we cache locations that are likely to have faults: starting from the location of a known (fixed) fault, we cache the location itself, any locations changed together with the fault, recently added locations, and recently changed locations. By consulting the cache at the moment a fault is fixed, a developer can detect likely fault-prone locations. This is useful for prioritizing verification and validation resources on the most fault prone files or entities. In our evaluation of seven open source projects with more than 200,000 revisions, the cache selects 10\% of the source code files; these files account for 73\%-95\% of faults-- a significant advance beyond the state of the art.}},
    author = {Kim, Sunghun and Zimmermann, Thomas and Jr and Zeller, Andreas},
    citeulike-article-id = {7796184},
    citeulike-linkout-0 = {http://portal.acm.org.www.lib.ncsu.edu:2048/citation.cfm?id=1248881},
    comment = {<p>Note</p>},
    isbn = {0-7695-2828-7},
    journal = {Proceedings of the 29th international conference on Software Engineering},
    keywords = {chs, chs-fse2012},
    pages = {489--498},
    posted-at = {2010-09-07 21:50:52},
    priority = {0},
    publisher = {IEEE Computer Society},
    title = {{Predicting Faults from Cached History}},
    url = {http://portal.acm.org.www.lib.ncsu.edu:2048/citation.cfm?id=1248881},
    year = {2007}
}

@article{14,
    abstract = {{Diversification of software assets through changing requirements impose a constant challenge on the developers and maintainers of large software systems. Recent research has addressed the mining for data in software repositories of single products ranging from fine- to coarse grained analyses. But so far, little attention has been payed to mining data about the evolution of product families. In this work, we study the evolution and commonalities of three variants of the BSD (Berkeley Software Distribution), a large open source operating system. The research questions we tackle are concerned with how to generate high level views of the system discovering and indicating evolutionary highlights. To process the large amount of data, we extended our previously developed approach for storing release history information to support the analysis of product families. In a case study we apply our approach on data from three different code repositories representing about 8.5GB of data and 10 years of active development.}},
    author = {Fischer, Michael and Oberleitner, Johann and Ratzinger, Jacek and Gall, Harald},
    citeulike-article-id = {7796182},
    citeulike-linkout-0 = {http://portal.acm.org.www.lib.ncsu.edu:2048/citation.cfm?id=1082983.1083145},
    journal = {SIGSOFT Softw. Eng. Notes},
    keywords = {chs},
    number = {4},
    pages = {1--5},
    posted-at = {2010-09-07 21:50:51},
    priority = {3},
    title = {{Mining evolution data of a product family}},
    url = {http://portal.acm.org.www.lib.ncsu.edu:2048/citation.cfm?id=1082983.1083145},
    volume = {30},
    year = {2005}
}

@techreport{40,
    abstract = {{In software engineering, the importance of measurement is well understood, and many efficient software development metrics have been developed to help measurement. However, as the number of metrics increases, the effort required to collect data, analyze them and interpret the results quickly becomes overwhelming. This problem is even more critical in educational approaches regarding empirical software engineering. The Software Intensive Care Unit is a new approach to facilitating software measurement and control with multiple software development metrics. It uses the Hackystat system to achieve automated data collection and analysis, then uses the collected analysis data to create a monitoring interface for multiple ``vital signs''. A vital sign is a wrapper of a software metric with an easy to use presentation. It consists of a historical trend and a newest state value, both of which are colored according to the ``health'' state. My research deployed and evaluated the Software ICU in a senior-level software engineering course. Students' usage was logged in the system, and a survey was conducted. The results provide supporting evidence that Software ICU does help students in course project development and project team organization. In addition, the results of the study also discover some limitations of the system, including inappropriate vital sign presentation and measurement dysfunction.}},
    author = {Zhang, Shaoxuan},
    citeulike-article-id = {7796181},
    citeulike-linkout-0 = {http://csdl.ics.hawaii.edu/techreports/09-10/09-10.pdf},
    keywords = {hackystat, softwareicu, thesis-ms},
    posted-at = {2010-09-07 21:50:51},
    priority = {2},
    title = {{Learning Empirical Software Engineering Using Software Intensive Care Unit}},
    url = {http://csdl.ics.hawaii.edu/techreports/09-10/09-10.pdf},
    year = {2009}
}

@proceedings{25,
    abstract = {{As a software system evolves, programmers make changes which sometimes lead to problems. The risk of later problems significantly depends on the location of the change. Which are the locations where changes impose the greatest risk? Our HATARI prototype relates a version history (such as CVS) to a bug database (such as BUGZILLA) to detect those locations where changes have been risky in the past. HATARI makes this risk visible for developers by annotating source code with color bars. Furthermore, HATARI provides views to browse through the most risky locations and to analyze the risk history of a particular location.}},
    author = {\'{S}liwerski, Jacek and Zimmermann, Thomas and Zeller, Andreas},
    citeulike-article-id = {7796178},
    citeulike-linkout-0 = {http://portal.acm.org.www.lib.ncsu.edu:2048/citation.cfm?id=1081706.1081725},
    comment = {<p style="margin: 0in 0in 0in 0.375in; font-family: Tahoma; font-size: 8pt; color: \#666666;">Paper describes a new tool HATARI for assessing the risk of making  changes to a slice of a program.<span> </span>The tool determines that  the change is risky based on defect change history for each slice of  code.<span> </span>It determines this by investigating the change  history of the program slice to determine what was the change that  preceded the fix in the same code slice.<span> </span>The tool  integrates with eclipse and highlights using green and shades of red to  indicate the degree of risk of changing a particular line of code based  on the change history.<span> </span>Static data are not used in making  this assessment.<span> </span>It works with CVS and Bugzilla.</p>},
    isbn = {1-59593-014-0},
    journal = {Proceedings of the 10th European software engineering conference held jointly with 13th ACM SIGSOFT international symposium on Foundations of software engineering},
    keywords = {chs, chs-fse2012},
    location = {Lisbon, Portugal},
    pages = {107--110},
    posted-at = {2010-09-07 21:50:51},
    priority = {0},
    publisher = {ACM},
    title = {{HATARI: raising risk awareness}},
    url = {http://portal.acm.org.www.lib.ncsu.edu:2048/citation.cfm?id=1081706.1081725},
    year = {2005}
}

@inproceedings{Zhou2008Bayesian,
    abstract = {{Source code coupling and change history are two important data sources for change coupling analysis. The popularity of public open source projects in recent years makes both sources available. Based on our previous research, in this paper, we inspect different dimensions of software changes including change significance or source code dependency levels, extract a set of features from the two sources and propose a Bayesian network-based approach for change coupling prediction. By combining the features from the co-changed entities and their dependency relation, the approach can model the underlying uncertainty. The empirical case study on two medium-sized open source projects demonstrates the feasibility and effectiveness of our approach compared to previous work.}},
    author = {Zhou, Yu and W\"{u}rsch, Michael and Giger, Emanuel and Gall, Harald C. and L\"{u}, Jian},
    booktitle = {2008 15th Working Conference on Reverse Engineering},
    citeulike-article-id = {5971324},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/wcre.2008.39},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4656390},
    day = {24},
    doi = {10.1109/wcre.2008.39},
    isbn = {978-0-7695-3429-9},
    journal = {Reverse Engineering, 2008. WCRE '08. 15th Working Conference on},
    keywords = {chs},
    location = {Antwerp, Belgium},
    month = oct,
    pages = {27--36},
    posted-at = {2010-09-07 21:39:02},
    priority = {3},
    publisher = {IEEE},
    title = {{A Bayesian Network Based Approach for Change Coupling Prediction}},
    url = {http://dx.doi.org/10.1109/wcre.2008.39},
    year = {2008}
}

@inproceedings{Breu2006Mining,
    abstract = {{Aspect mining identifies cross-cutting concerns in a program to help migrating it to an aspect-oriented design. Such concerns may not exist from the beginning, but emerge over time. By analysing where developers add code to a program, our history-based aspect mining (HAM) identifies and ranks cross-cutting concerns. We evaluated the effectiveness of our approach with the history of three open-source projects. HAM scales up to industrial-sized projects: for example, we were able to identify a locking concern that cross-cuts 1 284 methods in Eclipse. Additionally, the precision of HAM increases with project size and history; for Eclipse, it reaches 90\% for the top-10 candidates.}},
    address = {Washington, DC, USA},
    author = {Breu, Silvia and Zimmermann, Thomas},
    booktitle = {Proceedings of the 21st IEEE/ACM International Conference on Automated Software Engineering},
    citeulike-article-id = {1232688},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1169321},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/ase.2006.50},
    citeulike-linkout-2 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4019577},
    doi = {10.1109/ase.2006.50},
    isbn = {0-7695-2579-2},
    keywords = {chs},
    location = {Tokyo},
    month = nov,
    pages = {221--230},
    posted-at = {2010-09-07 21:35:53},
    priority = {3},
    publisher = {IEEE Computer Society},
    title = {{Mining Aspects from Version History}},
    url = {http://dx.doi.org/10.1109/ase.2006.50},
    year = {2006}
}

